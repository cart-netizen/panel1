#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Reinforcement Learning –∞–≥–µ–Ω—Ç–æ–≤
–û–±—É—á–∞–µ—Ç Q-Learning –∏ DQN –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ª–æ—Ç–µ—Ä–µ–π

–ú–µ—Ö–∞–Ω–∏–∑–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ RL –º–æ–¥–µ–ª–µ–π:
üìÅ –ì–¥–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –º–æ–¥–µ–ª–∏:
backend/models/rl/
‚îú‚îÄ‚îÄ 4x20/
‚îÇ   ‚îú‚îÄ‚îÄ q_agent.pkl           # Q-Learning —Ç–∞–±–ª–∏—Ü–∞
‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.pth         # DQN –Ω–µ–π—Ä–æ—Å–µ—Ç—å
‚îÇ   ‚îî‚îÄ‚îÄ training_stats.json   # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
‚îî‚îÄ‚îÄ 5x36plus/
    ‚îú‚îÄ‚îÄ q_agent.pkl
    ‚îú‚îÄ‚îÄ dqn_agent.pth
    ‚îî‚îÄ‚îÄ training_stats.json
üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞:

–ü—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞ - –º–æ–¥–µ–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏–∑ —Ñ–∞–π–ª–æ–≤
–ü—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ —Å–µ—Ä–≤–µ—Ä–∞ - –º–æ–¥–µ–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è
–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è - –º–æ–¥–µ–ª–∏ —Å—Ä–∞–∑—É —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è

üöÄ –í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è:
–í–∞—Ä–∏–∞–Ω—Ç 1: –ü—Ä–æ—Å—Ç–æ–π (–∫–∞–∫ –≤ –ø—Ä–æ—à–ª—ã—Ö —á–∞—Ç–∞—Ö)
bashcd backend
python -c "from backend.app.core.rl.rl_generator import GLOBAL_RL_MANAGER; GLOBAL_RL_MANAGER.train_all(verbose=True)"
–í–∞—Ä–∏–∞–Ω—Ç 2: –ë—ã—Å—Ç—Ä—ã–π —Å–∫—Ä–∏–ø—Ç
bashpython quick_train.py
–í–∞—Ä–∏–∞–Ω—Ç 3: –ü–æ–ª–Ω—ã–π —Å–∫—Ä–∏–ø—Ç —Å –æ–ø—Ü–∏—è–º–∏
bash# –û–±—É—á–∏—Ç—å –≤—Å–µ –ª–æ—Ç–µ—Ä–µ–∏ (–ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)
python train_rl_agents.py

# –û–±—É—á–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ª–æ—Ç–µ—Ä–µ—é
python train_rl_agents.py --lottery 4x20

# –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ (–º–µ–Ω—å—à–µ —ç–ø–∏–∑–æ–¥–æ–≤)
python train_rl_agents.py --quick

# –¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
python train_rl_agents.py --check-only

# –ö–∞—Å—Ç–æ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
python train_rl_agents.py --q-episodes 2000 --dqn-episodes 1000
‚úÖ –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:

–ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ —Ñ–∞–π–ª—ã - –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å
–ü—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞ - –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
–í –ª–æ–≥–∞—Ö –≤–∏–¥–Ω–æ: ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ RL –º–æ–¥–µ–ª–∏ –¥–ª—è 4x20
RL –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —á–µ—Ä–µ–∑ API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã

üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –º–æ–¥–µ–ª–µ–π:
–ß–µ—Ä–µ–∑ —Å–∫—Ä–∏–ø—Ç:
bashpython train_rl_agents.py --check-only
–ß–µ—Ä–µ–∑ API:
bashcurl http://127.0.0.1:8002/api/rl/status/4x20
–í –ª–æ–≥–∞—Ö –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞:
[RL STATS] –ò—Ç–æ–≥–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RL:
   –í—Å–µ–≥–æ –∞–≥–µ–Ω—Ç–æ–≤: 4
   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: 4  ‚Üê –í–º–µ—Å—Ç–æ 0 –±—É–¥–µ—Ç 4 –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π:

–ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–∏–º –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ –≤—ã—à–µ
–î–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 10-30 –º–∏–Ω—É—Ç)
–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ - –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä - –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∑—è—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ RL —á–µ—Ä–µ–∑ API –∏–ª–∏ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥

‚ö° –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:

Q-Learning: ~5-10 –º–∏–Ω—É—Ç –Ω–∞ –ª–æ—Ç–µ—Ä–µ—é
DQN: ~10-20 –º–∏–Ω—É—Ç –Ω–∞ –ª–æ—Ç–µ—Ä–µ—é
–û–±—â–µ–µ –≤—Ä–µ–º—è: ~30-60 –º–∏–Ω—É—Ç –¥–ª—è –≤—Å–µ—Ö –ª–æ—Ç–µ—Ä–µ–π

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç—ã –±—É–¥—É—Ç –≥–æ—Ç–æ–≤—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —á–µ—Ä–µ–∑:

API: /api/rl/generate/4x20
Frontend: —Ä–∞–∑–¥–µ–ª "–ê–Ω–∞–ª–∏–∑" ‚Üí –≤–∫–ª–∞–¥–∫–∞ "RL Analysis"

üéâ –ì–ª–∞–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ: –û–±—É—á–µ–Ω–∏–µ –¥–µ–ª–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑, –ø–æ—Ç–æ–º –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω–æ!

"""

import sys
import os
import time
import argparse
from datetime import datetime
from typing import Dict

import numpy as np
import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ PYTHONPATH
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from backend.app.core.rl.rl_generator import GLOBAL_RL_MANAGER
from backend.app.core import data_manager


def print_header():
  """–í—ã–≤–æ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
  print("=" * 70)
  print("ü§ñ –û–ë–£–ß–ï–ù–ò–ï REINFORCEMENT LEARNING –ê–ì–ï–ù–¢–û–í")
  print("=" * 70)
  print(f"‚è∞ –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
  print()


def check_data_availability():
  """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –ª–æ—Ç–µ—Ä–µ–π"""
  print("üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö...")

  data_status = {}
  for lottery_type in data_manager.LOTTERY_CONFIGS.keys():
    # with data_manager.LotteryContext(lottery_type):
    df = data_manager.fetch_draws_from_db()
    data_status[lottery_type] = len(df)
    print(f"   üìà {lottery_type}: {len(df)} —Ç–∏—Ä–∞–∂–µ–π")

  print()
  return data_status


def train_specific_lottery(lottery_type: str, q_episodes: int = 1000, dqn_episodes: int = 500):
  """–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ª–æ—Ç–µ—Ä–µ–∏"""
  print(f"üéØ –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –ª–æ—Ç–µ—Ä–µ–∏: {lottery_type}")
  print("-" * 50)

  if lottery_type not in data_manager.LOTTERY_CONFIGS:
    print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –ª–æ—Ç–µ—Ä–µ–∏: {lottery_type}")
    print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(data_manager.LOTTERY_CONFIGS.keys())}")
    return False

  config = data_manager.LOTTERY_CONFIGS[lottery_type]
  generator = GLOBAL_RL_MANAGER.get_generator(lottery_type, config)

  # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
  # with data_manager.LotteryContext(lottery_type):
  #   df = data_manager.fetch_draws_from_db()
  df = data_manager.fetch_draws_from_db()

  if len(df) < 60:
    print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(df)} < 60")
    return False

  print(f"   üìä –î–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(df)} —Ç–∏—Ä–∞–∂–µ–π")
  print(f"   ‚öôÔ∏è Q-Learning —ç–ø–∏–∑–æ–¥–æ–≤: {q_episodes}")
  print(f"   üß† DQN —ç–ø–∏–∑–æ–¥–æ–≤: {dqn_episodes}")
  print()

  start_time = time.time()

  try:
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    stats = generator.train(
      df_history=df,
      q_episodes=q_episodes,
      dqn_episodes=dqn_episodes,
      verbose=True
    )

    training_time = time.time() - start_time

    print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time:.1f} —Å–µ–∫—É–Ω–¥")
    print()

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    if 'q_learning' in stats:
      q_stats = stats['q_learning']
      print("üìà Q-Learning —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
      print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {q_stats.get('average_reward', 0):.3f}")
      print(f"   ‚Ä¢ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {q_stats.get('best_reward', 0):.3f}")
      print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä Q-—Ç–∞–±–ª–∏—Ü—ã: {q_stats.get('q_table_size', 0)}")
      print(f"   ‚Ä¢ –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤—ã–∏–≥—Ä—ã—à–µ–π: {q_stats.get('win_rate', 0):.1%}")

    if 'dqn' in stats:
      dqn_stats = stats['dqn']
      print("üß† DQN —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
      print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {dqn_stats.get('average_reward', 0):.3f}")
      print(f"   ‚Ä¢ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {dqn_stats.get('best_reward', 0):.3f}")
      print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä –ø–∞–º—è—Ç–∏: {dqn_stats.get('memory_size', 0)}")
      print(f"   ‚Ä¢ –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤—ã–∏–≥—Ä—ã—à–µ–π: {dqn_stats.get('win_rate', 0):.1%}")

    print()
    return True

  except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
    return False

def train_with_validation(lottery_type: str, df_full: pd.DataFrame, config: Dict):
    """–û–±—É—á–µ–Ω–∏–µ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    print(f"üéØ –û–±—É—á–µ–Ω–∏–µ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –¥–ª—è –ª–æ—Ç–µ—Ä–µ–∏: {lottery_type}")
    print("-" * 50)

    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/val/test
    n = len(df_full)
    train_end = int(n * 0.7)
    val_end = int(n * 0.85)

    train_df = df_full.iloc[:train_end]
    val_df = df_full.iloc[train_end:val_end]
    test_df = df_full.iloc[val_end:]

    print(f"   üìä –î–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã:")
    print(f"      –û–±—É—á–µ–Ω–∏–µ: {len(train_df)} —Ç–∏—Ä–∞–∂–µ–π (70%)")
    print(f"      –í–∞–ª–∏–¥–∞—Ü–∏—è: {len(val_df)} —Ç–∏—Ä–∞–∂–µ–π (15%)")
    print(f"      –¢–µ—Å—Ç: {len(test_df)} —Ç–∏—Ä–∞–∂–µ–π (15%)")

    # –ü–æ–ª—É—á–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    complexity = config['field1_size'] * config['field1_max']

    if complexity <= 80:  # –ü—Ä–æ—Å—Ç—ã–µ –ª–æ—Ç–µ—Ä–µ–∏
      q_episodes = 1500
      dqn_episodes = 800
      print(f"   ‚öôÔ∏è –ü—Ä–æ—Å—Ç–∞—è –ª–æ—Ç–µ—Ä–µ—è (—Å–ª–æ–∂–Ω–æ—Å—Ç—å: {complexity})")
    elif complexity <= 180:  # –°—Ä–µ–¥–Ω–∏–µ –ª–æ—Ç–µ—Ä–µ–∏
      q_episodes = 2000
      dqn_episodes = 1200
      print(f"   ‚öôÔ∏è –°—Ä–µ–¥–Ω—è—è –ª–æ—Ç–µ—Ä–µ—è (—Å–ª–æ–∂–Ω–æ—Å—Ç—å: {complexity})")
    else:  # –°–ª–æ–∂–Ω—ã–µ –ª–æ—Ç–µ—Ä–µ–∏
      q_episodes = 3000
      dqn_episodes = 2000
      print(f"   ‚öôÔ∏è –°–ª–æ–∂–Ω–∞—è –ª–æ—Ç–µ—Ä–µ—è (—Å–ª–æ–∂–Ω–æ—Å—Ç—å: {complexity})")

    print(f"   üìà –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: Q={q_episodes}, DQN={dqn_episodes}")
    print()

    # –û–±—É—á–∞–µ–º –Ω–∞ train_df
    generator = GLOBAL_RL_MANAGER.get_generator(lottery_type, config)

    start_time = time.time()
    stats = generator.train(
      df_history=train_df,
      q_episodes=q_episodes,
      dqn_episodes=dqn_episodes,
      verbose=True
    )
    training_time = time.time() - start_time

    print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time:.1f} —Å–µ–∫—É–Ω–¥")
    print()

    # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ val_df
    print("üîç –í–ê–õ–ò–î–ê–¶–ò–Ø –ù–ê –û–¢–î–ï–õ–¨–ù–´–• –î–ê–ù–ù–´–•")
    print("-" * 50)

    try:
      from backend.app.core.rl.environment import LotteryEnvironment

      val_env = LotteryEnvironment(val_df, config)

      if generator.q_trained:
        q_rewards = []
        q_wins = 0
        total_plays = 0

        # 20 —ç–ø–∏–∑–æ–¥–æ–≤ –ø–æ 30 —Ö–æ–¥–æ–≤ = 600 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–≥—Ä
        for episode in range(20):
          state = val_env.reset()

          for step in range(30):
            if state is None:
              break

            action = generator.q_agent.choose_action(state, training=False)
            next_state, reward, done, info = val_env.step(action)

            q_rewards.append(reward)
            total_plays += 1

            # –°—á–∏—Ç–∞–µ–º –≤—ã–∏–≥—Ä—ã—à–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑ (–Ω–µ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—Ç –±–∏–ª–µ—Ç–∞)
            if reward > 0:
              q_wins += 1

            if done:
              break
            state = next_state

        q_avg_reward = sum(q_rewards) / len(q_rewards) if q_rewards else 0
        q_win_rate = (q_wins / total_plays) * 100 if total_plays > 0 else 0

        print(f"üìà Q-Learning –≤–∞–ª–∏–¥–∞—Ü–∏—è:")
        print(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {q_avg_reward:.2f}")
        print(f"   Win rate: {q_win_rate:.2f}%")
        print(f"   –í—Å–µ–≥–æ –∏–≥—Ä: {total_plays}")
        print(f"   –í—ã–∏–≥—Ä—ã—à–Ω—ã—Ö: {q_wins}")
        print(f"   –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {max(q_rewards) if q_rewards else 0:.2f}")
        print(f"   –•—É–¥—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {min(q_rewards) if q_rewards else 0:.2f}")

        if q_win_rate > 10:
          print("   ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π win rate!")
        elif q_win_rate > 0.1:
          print("   ‚úÖ –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π win rate –¥–ª—è –ª–æ—Ç–µ—Ä–µ–∏")
        else:
          print("   üìä –û—á–µ–Ω—å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")

      # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è DQN
      if generator.dqn_trained:
        dqn_rewards = []
        dqn_wins = 0
        total_plays_dqn = 0

        for episode in range(20):
          state = val_env.reset()

          for step in range(30):
            if state is None:
              break

            action = generator.dqn_agent.choose_action(state, training=False)
            next_state, reward, done, info = val_env.step(action)

            dqn_rewards.append(reward)
            total_plays_dqn += 1

            if reward > 0:
              dqn_wins += 1

            if done:
              break
            state = next_state

        dqn_avg_reward = sum(dqn_rewards) / len(dqn_rewards) if dqn_rewards else 0
        dqn_win_rate = (dqn_wins / total_plays_dqn) * 100 if total_plays_dqn > 0 else 0

        print(f"üß† DQN –≤–∞–ª–∏–¥–∞—Ü–∏—è:")
        print(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {dqn_avg_reward:.2f}")
        print(f"   Win rate: {dqn_win_rate:.2f}%")
        print(f"   –í—Å–µ–≥–æ –∏–≥—Ä: {total_plays_dqn}")
        print(f"   –í—ã–∏–≥—Ä—ã—à–Ω—ã—Ö: {dqn_wins}")
        print(f"   –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {max(dqn_rewards) if dqn_rewards else 0:.2f}")
        print(f"   –•—É–¥—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {min(dqn_rewards) if dqn_rewards else 0:.2f}")

        if dqn_win_rate > 10:
          print("   ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π win rate!")
        elif dqn_win_rate > 0.1:
          print("   ‚úÖ –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π win rate –¥–ª—è –ª–æ—Ç–µ—Ä–µ–∏")
        else:
          print("   üìä –û—á–µ–Ω—å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")

    except Exception as e:
      print(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
      print()

    return True

def train_all_lotteries(q_episodes: int = 1000, dqn_episodes: int = 500):
  """–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö –ª–æ—Ç–µ—Ä–µ–π"""
  print("üåü –ú–ê–°–°–û–í–û–ï –û–ë–£–ß–ï–ù–ò–ï –í–°–ï–• –õ–û–¢–ï–†–ï–ô")
  print("=" * 50)

  data_status = check_data_availability()

  success_count = 0
  total_count = 0

  for lottery_type in data_manager.LOTTERY_CONFIGS.keys():
    total_count += 1

    if data_status[lottery_type] < 60:
      print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ {lottery_type}: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({data_status[lottery_type]} < 60)")
      continue

    if train_with_validation(lottery_type, data_manager.fetch_draws_from_db(),
                             data_manager.LOTTERY_CONFIGS[lottery_type]):
      success_count += 1

  print("=" * 70)
  print("üìä –ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø:")
  print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–æ: {success_count}/{total_count} –ª–æ—Ç–µ—Ä–µ–π")
  print(f"   üìÅ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: backend/models/rl/")
  print("=" * 70)


def check_trained_models():
  """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
  print("üîç –ü–†–û–í–ï–†–ö–ê –û–ë–£–ß–ï–ù–ù–´–• –ú–û–î–ï–õ–ï–ô")
  print("-" * 50)

  for lottery_type in data_manager.LOTTERY_CONFIGS.keys():
    config = data_manager.LOTTERY_CONFIGS[lottery_type]
    generator = GLOBAL_RL_MANAGER.get_generator(lottery_type, config)

    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏
    loaded = generator.load_models()

    status = "üü¢ –û–±—É—á–µ–Ω—ã" if (generator.q_trained or generator.dqn_trained) else "üî¥ –ù–µ –æ–±—É—á–µ–Ω—ã"
    details = []

    if generator.q_trained:
      details.append("Q-Learning ‚úì")
    if generator.dqn_trained:
      details.append("DQN ‚úì")

    details_str = " | ".join(details) if details else "–ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤"

    print(f"   {lottery_type}: {status} ({details_str})")

  print()


def main():
  """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
  parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ RL –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –ª–æ—Ç–µ—Ä–µ–π")

  parser.add_argument(
    "--lottery",
    type=str,
    help="–û–±—É—á–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ª–æ—Ç–µ—Ä–µ—é (4x20, 5x36plus)",
    choices=list(data_manager.LOTTERY_CONFIGS.keys())
  )

  parser.add_argument(
    "--q-episodes",
    type=int,
    default=1000,
    help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è Q-Learning (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1000)"
  )

  parser.add_argument(
    "--dqn-episodes",
    type=int,
    default=500,
    help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è DQN (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 500)"
  )

  parser.add_argument(
    "--quick",
    action="store_true",
    help="–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ (Q=200, DQN=100 —ç–ø–∏–∑–æ–¥–æ–≤)"
  )

  parser.add_argument(
    "--check-only",
    action="store_true",
    help="–¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
  )

  args = parser.parse_args()

  print_header()

  # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
  if args.quick:
    args.q_episodes = 200
    args.dqn_episodes = 100
    print("‚ö° –†–µ–∂–∏–º –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
    print()

  # –¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞
  if args.check_only:
    check_trained_models()
    return

  # –û–±—É—á–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ª–æ—Ç–µ—Ä–µ–∏
  if args.lottery:
    train_specific_lottery(args.lottery, args.q_episodes, args.dqn_episodes)
  else:
    # –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –ª–æ—Ç–µ—Ä–µ–π
    train_all_lotteries(args.q_episodes, args.dqn_episodes)

  print()
  check_trained_models()

  print("üéâ –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
  print(f"‚è∞ –í—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


if __name__ == "__main__":
  main()