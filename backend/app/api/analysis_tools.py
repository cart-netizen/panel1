"""
API –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
"""
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List, Dict, Optional
from pydantic import BaseModel
import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
import json

from backend.app.core.auth import get_current_user
from backend.app.core.lottery_context import LotteryContext
from backend.app.core import data_manager, pattern_analyzer, ai_model
from backend.app.core.database import User, UserPreferences, get_db

router = APIRouter()


class PatternAnalysisRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    lottery_type: str = "4x20"
    depth: int = 100
    include_favorites: bool = True


class ClusterAnalysisRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
    lottery_type: str = "4x20"
    n_clusters: int = 5
    method: str = "kmeans"  # kmeans, dbscan, hierarchical


class CombinationEvaluationRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –æ—Ü–µ–Ω–∫—É –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏"""
    lottery_type: str = "4x20"
    field1: List[int]
    field2: List[int]
    use_favorites: bool = False


@router.post("/patterns")
async def analyze_patterns(
    request: PatternAnalysisRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤—ã–ø–∞–¥–µ–Ω–∏—è"""
    try:
        print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è {request.lottery_type}")

        with LotteryContext(request.lottery_type):
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
            df_history = data_manager.fetch_draws_from_db()
            print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_history)} —Ç–∏—Ä–∞–∂–µ–π")

            if df_history.empty:
                raise HTTPException(status_code=404, detail="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–º–µ—Å—Ç–æ GLOBAL_PATTERN_ANALYZER
            df_analysis = df_history.tail(request.depth)
            print(f"üî¨ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ {len(df_analysis)} —Ç–∏—Ä–∞–∂–µ–π")

            hot_cold_analysis = _analyze_hot_cold_simple(df_analysis)
            print("‚úÖ –ê–Ω–∞–ª–∏–∑ –≥–æ—Ä—è—á–∏—Ö/—Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª –∑–∞–≤–µ—Ä—à–µ–Ω")

            # –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            correlations = _analyze_correlations_simple(df_analysis)
            print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω")

            # –ê–Ω–∞–ª–∏–∑ –∏–∑–±—Ä–∞–Ω–Ω—ã—Ö —á–∏—Å–µ–ª (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
            favorites_analysis = None
            if request.include_favorites:
                prefs = db.query(UserPreferences).filter_by(user_id=current_user.id).first()
                if prefs and prefs.favorite_numbers:
                    all_favorites = json.loads(prefs.favorite_numbers)
                    lottery_favorites = all_favorites.get(request.lottery_type, {"field1": [], "field2": []})
                    if lottery_favorites['field1'] or lottery_favorites['field2']:
                        favorites_analysis = _analyze_favorite_numbers(
                            df_analysis,
                            lottery_favorites['field1'],
                            lottery_favorites['field2']
                        )
                        print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∏–∑–±—Ä–∞–Ω–Ω—ã—Ö —á–∏—Å–µ–ª –∑–∞–≤–µ—Ä—à–µ–Ω")

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                "status": "success",
                "hot_cold": hot_cold_analysis,
                "correlations": correlations,
                "favorites_analysis": favorites_analysis,
                "data_stats": {
                    "total_draws": len(df_history),
                    "analyzed_period": request.depth,
                    "lottery_type": request.lottery_type
                },
                "date_range": {
                    "from": df_history['–î–∞—Ç–∞'].min().isoformat() if not df_history.empty else None,
                    "to": df_history['–î–∞—Ç–∞'].max().isoformat() if not df_history.empty else None
                },
                "timestamp": datetime.utcnow().isoformat()
            }

            print("üîç –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:", result)
            return result

    except Exception as e:
        import traceback
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")


def _analyze_correlations_simple(df_history, top_n=5):
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É —á–∏—Å–ª–∞–º–∏"""

    correlations = {
        "field1": [],
        "field2": []
    }

    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä –≤ –ø–æ–ª–µ 1
    from collections import Counter
    field1_pairs = Counter()

    for _, row in df_history.iterrows():
        numbers = row['–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list']
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ –ø–∞—Ä—ã
        for i in range(len(numbers)):
            for j in range(i + 1, len(numbers)):
                pair = tuple(sorted([numbers[i], numbers[j]]))
                field1_pairs[pair] += 1

    # –¢–æ–ø –ø–∞—Ä—ã –¥–ª—è –ø–æ–ª—è 1
    total_draws = len(df_history)
    for pair, count in field1_pairs.most_common(top_n):
        frequency = (count / total_draws) * 100
        correlations["field1"].append({
            "pair": f"{pair[0]}-{pair[1]}",
            "frequency_percent": round(frequency, 1),
            "count": count
        })

    # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä –≤ –ø–æ–ª–µ 2
    field2_pairs = Counter()

    for _, row in df_history.iterrows():
        numbers = row['–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list']
        if len(numbers) >= 2:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –º–∏–Ω–∏–º—É–º 2 —á–∏—Å–ª–∞
            for i in range(len(numbers)):
                for j in range(i + 1, len(numbers)):
                    pair = tuple(sorted([numbers[i], numbers[j]]))
                    field2_pairs[pair] += 1

    # –¢–æ–ø –ø–∞—Ä—ã –¥–ª—è –ø–æ–ª—è 2
    for pair, count in field2_pairs.most_common(top_n):
        frequency = (count / total_draws) * 100
        correlations["field2"].append({
            "pair": f"{pair[0]}-{pair[1]}",
            "frequency_percent": round(frequency, 1),
            "count": count
        })

    return correlations


@router.post("/clusters")
async def analyze_clusters(
    request: ClusterAnalysisRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–∏—Å–µ–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    try:
        print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è {request.lottery_type}")

        with LotteryContext(request.lottery_type):
            df_history = data_manager.fetch_draws_from_db()
            print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_history)} —Ç–∏—Ä–∞–∂–µ–π")

            if df_history.empty:
                raise HTTPException(status_code=404, detail="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            config = data_manager.get_current_config()

            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–µ—Ä–µ–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–ª–µ–π
            features_f1 = _prepare_clustering_features_fixed(
                df_history, 'field1', config['field1_max']
            )
            features_f2 = _prepare_clustering_features_fixed(
                df_history, 'field2', config['field2_max']
            )

            print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏: –ø–æ–ª–µ1={len(features_f1)}, –ø–æ–ª–µ2={len(features_f2)}")

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é
            if request.method == "kmeans":
                clusters_f1 = _kmeans_clustering(features_f1, request.n_clusters)
                clusters_f2 = _kmeans_clustering(features_f2, min(request.n_clusters, config['field2_max']))
            elif request.method == "dbscan":
                clusters_f1 = _dbscan_clustering(features_f1)
                clusters_f2 = _dbscan_clustering(features_f2)
            else:
                raise HTTPException(status_code=400, detail="–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –º–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")

            print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –º–µ—Ç–æ–¥–æ–º {request.method}")

            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            cluster_interpretation = _interpret_clusters_fixed(
                clusters_f1, clusters_f2, df_history
            )

            result = {
                "status": "success",
                "clusters": {
                    "field1": clusters_f1,
                    "field2": clusters_f2
                },
                "interpretation": cluster_interpretation,
                "method": request.method,
                "n_clusters_found": {
                    "field1": len(set(clusters_f1.values())),
                    "field2": len(set(clusters_f2.values()))
                },
                "timestamp": datetime.utcnow().isoformat()
            }

            print("üîç –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
            return result

    except Exception as e:
        import traceback
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}")


@router.post("/evaluate")
async def evaluate_combination(
    request: CombinationEvaluationRequest,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å AI-—Å–∫–æ—Ä–∏–Ω–≥–æ–º"""
    try:
        with LotteryContext(request.lottery_type):
            df_history = data_manager.fetch_draws_from_db()

            if df_history.empty:
                raise HTTPException(status_code=404, detail="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

            config = data_manager.get_current_config()
            score = 0.0
            factors = []

            # 1. –ê–Ω–∞–ª–∏–∑ –≥–æ—Ä—è—á–∏—Ö/—Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –±–µ–∑ GLOBAL_PATTERN_ANALYZER)
            hot_cold_analysis = _analyze_hot_cold_simple(df_history)

            hot_f1 = hot_cold_analysis.get('field1', {}).get('hot_numbers', [])
            hot_f2 = hot_cold_analysis.get('field2', {}).get('hot_numbers', [])
            cold_f1 = hot_cold_analysis.get('field1', {}).get('cold_numbers', [])
            cold_f2 = hot_cold_analysis.get('field2', {}).get('cold_numbers', [])

            hot_count = len(set(request.field1) & set(hot_f1)) + len(set(request.field2) & set(hot_f2))
            cold_count = len(set(request.field1) & set(cold_f1)) + len(set(request.field2) & set(cold_f2))

            if hot_count >= 3:
                score += 25
                factors.append(f"‚úÖ –°–æ–¥–µ—Ä–∂–∏—Ç {hot_count} –≥–æ—Ä—è—á–∏—Ö —á–∏—Å–µ–ª (+25)")
            elif hot_count >= 1:
                score += 15
                factors.append(f"‚ûï –°–æ–¥–µ—Ä–∂–∏—Ç {hot_count} –≥–æ—Ä—è—á–∏—Ö —á–∏—Å–µ–ª (+15)")

            if cold_count >= 2:
                score += 20
                factors.append(f"üîÑ –°–æ–¥–µ—Ä–∂–∏—Ç {cold_count} —Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª, –≥–æ—Ç–æ–≤—ã—Ö –∫ –≤—ã—Ö–æ–¥—É (+20)")

            # 2. –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            sequence_score = _evaluate_sequences(request.field1, request.field2)
            score += sequence_score
            if sequence_score > 0:
                factors.append(f"üìä –•–æ—Ä–æ—à–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–µ–ª (+{sequence_score})")

            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
            uniqueness = _check_uniqueness(request.field1, request.field2, df_history)
            score += uniqueness
            if uniqueness > 10:
                factors.append(f"üíé –£–Ω–∏–∫–∞–ª—å–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è (+{uniqueness})")

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫–æ—Ä–∞
            score = min(100, max(0, score))

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            if score >= 80:
                recommendation = "üèÜ –û—Ç–ª–∏—á–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è! –í—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª"
            elif score >= 60:
                recommendation = "‚úÖ –•–æ—Ä–æ—à–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º–∏"
            elif score >= 40:
                recommendation = "‚ö° –°—Ä–µ–¥–Ω—è—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è, –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å"
            else:
                recommendation = "‚ö†Ô∏è –°–ª–∞–±–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å"

            return {
                "status": "success",
                "score": round(score, 1),
                "max_score": 100,
                "factors": factors,
                "recommendation": recommendation,
                "combination": {
                    "field1": request.field1,
                    "field2": request.field2
                },
                "suggestions": _generate_improvement_suggestions(
                    score, hot_count, cold_count, request.field1, request.field2
                ),
                "timestamp": datetime.utcnow().isoformat()
            }

    except Exception as e:
        import traceback
        print(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏: {str(e)}")


def _analyze_hot_cold_simple(df_history, window=20, top_n=10):
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≥–æ—Ä—è—á–∏—Ö/—Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª –±–µ–∑ GLOBAL_PATTERN_ANALYZER"""

    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ window —Ç–∏—Ä–∞–∂–µ–π
    recent_df = df_history.tail(window)

    analysis = {
        "field1": {"hot_numbers": [], "cold_numbers": []},
        "field2": {"hot_numbers": [], "cold_numbers": []}
    }

    # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª—è 1
    field1_counter = {}
    for _, row in recent_df.iterrows():
        for num in row['–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list']:
            field1_counter[num] = field1_counter.get(num, 0) + 1

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
    sorted_f1 = sorted(field1_counter.items(), key=lambda x: x[1], reverse=True)
    analysis["field1"]["hot_numbers"] = [num for num, _ in sorted_f1[:top_n]]
    analysis["field1"]["cold_numbers"] = [num for num, _ in sorted_f1[-top_n:]]

    # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª—è 2
    field2_counter = {}
    for _, row in recent_df.iterrows():
        for num in row['–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list']:
            field2_counter[num] = field2_counter.get(num, 0) + 1

    sorted_f2 = sorted(field2_counter.items(), key=lambda x: x[1], reverse=True)
    analysis["field2"]["hot_numbers"] = [num for num, _ in sorted_f2[:top_n]]
    analysis["field2"]["cold_numbers"] = [num for num, _ in sorted_f2[-top_n:]]

    return analysis

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
def _analyze_favorite_numbers(df, favorites_f1, favorites_f2):
    """–ê–Ω–∞–ª–∏–∑ –∏–∑–±—Ä–∞–Ω–Ω—ã—Ö —á–∏—Å–µ–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    analysis = {
        "field1": {},
        "field2": {}
    }

    for num in favorites_f1:
        count = sum(num in row for row in df['–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list'])  # ‚Üê –ò—Å–ø—Ä–∞–≤–∏—Ç—å
        analysis["field1"][num] = {
            "frequency": count,
            "percentage": (count / len(df)) * 100 if len(df) > 0 else 0
        }

    for num in favorites_f2:
        count = sum(num in row for row in df['–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'])  # ‚Üê –ò—Å–ø—Ä–∞–≤–∏—Ç—å
        analysis["field2"][num] = {
            "frequency": count,
            "percentage": (count / len(df)) * 100 if len(df) > 0 else 0
        }

    return analysis


def _prepare_clustering_features(df, field_name, max_num):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    features = {}

    # –ò—Å–ø—Ä–∞–≤–∏—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
    if 'field1' in field_name:
        column_name = '–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list'
    else:
        column_name = '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'


    for num in range(1, max_num + 1):
        # –ß–∞—Å—Ç–æ—Ç–∞ –ø–æ—è–≤–ª–µ–Ω–∏—è
        frequency = sum(num in row for row in df[column_name])

        # –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –ø–æ—è–≤–ª–µ–Ω–∏—è–º–∏
        appearances = [i for i, row in enumerate(df[column_name]) if num in row]
        avg_gap = np.mean(np.diff(appearances)) if len(appearances) > 1 else len(df)

        # –¢—Ä–µ–Ω–¥ (–ø–æ—è–≤–ª–µ–Ω–∏—è –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 20% —Ç–∏—Ä–∞–∂–µ–π)
        recent_df = df.tail(max(1, len(df) // 5))
        recent_freq = sum(num in row for row in recent_df[column_name])

        features[num] = [frequency, avg_gap, recent_freq]

    return features


def _kmeans_clustering(features, n_clusters):
    """KMeans –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"""
    if not features:
        return {}

    X = np.array(list(features.values()))
    numbers = list(features.keys())

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    kmeans = KMeans(n_clusters=min(n_clusters, len(numbers)), random_state=42)
    labels = kmeans.fit_predict(X_scaled)

    return {num: int(label) for num, label in zip(numbers, labels)}


def _dbscan_clustering(features):
    """DBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"""
    if not features:
        return {}

    X = np.array(list(features.values()))
    numbers = list(features.keys())

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    dbscan = DBSCAN(eps=0.5, min_samples=2)
    labels = dbscan.fit_predict(X_scaled)

    return {num: int(label) for num, label in zip(numbers, labels)}


def _interpret_clusters(clusters_f1, clusters_f2, df_history):
    """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
    interpretation = {}

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º –¥–ª—è –æ–±–æ–∏—Ö –ø–æ–ª–µ–π
    for field_name, clusters in [("field1", clusters_f1), ("field2", clusters_f2)]:
        cluster_groups = {}
        for num, cluster_id in clusters.items():
            if cluster_id not in cluster_groups:
                cluster_groups[cluster_id] = []
            cluster_groups[cluster_id].append(num)

        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        column_name = '–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list' if field_name == 'field1' else '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        for cluster_id, numbers in cluster_groups.items():
            if cluster_id == -1:  # DBSCAN outliers
                interpretation[f"{field_name}_outliers"] = {
                    "numbers": numbers,
                    "description": "–ê–Ω–æ–º–∞–ª—å–Ω—ã–µ —á–∏—Å–ª–∞ —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏"
                }
            else:
                # –ê–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–∞
                avg_frequency = np.mean([
                    sum(num in row for row in df_history[column_name])
                    for num in numbers
                ])

                if avg_frequency > len(df_history) * 0.15:
                    desc = "–ì–æ—Ä—è—á–∏–π –∫–ª–∞—Å—Ç–µ—Ä - —á–∏—Å–ª–∞ —Å –≤—ã—Å–æ–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π"
                elif avg_frequency < len(df_history) * 0.05:
                    desc = "–•–æ–ª–æ–¥–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä - —Ä–µ–¥–∫–æ –≤—ã–ø–∞–¥–∞—é—â–∏–µ —á–∏—Å–ª–∞"
                else:
                    desc = "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä - —Å—Ä–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"

                interpretation[f"{field_name}_cluster_{cluster_id}"] = {
                    "numbers": numbers,
                    "description": desc,
                    "avg_frequency": round(avg_frequency, 2)
                }

    return interpretation


def _evaluate_sequences(field1, field2):
    """–û—Ü–µ–Ω–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤"""
    score = 0

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —á–∏—Å–ª–∞
    for field in [field1, field2]:
        sorted_field = sorted(field)
        sequences = []
        current_seq = [sorted_field[0]]

        for i in range(1, len(sorted_field)):
            if sorted_field[i] == sorted_field[i-1] + 1:
                current_seq.append(sorted_field[i])
            else:
                if len(current_seq) > 1:
                    sequences.append(current_seq)
                current_seq = [sorted_field[i]]

        if len(current_seq) > 1:
            sequences.append(current_seq)

        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –∏–º–µ—Ç—å 1-2 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if len(sequences) == 1 and len(sequences[0]) <= 3:
            score += 5
        elif len(sequences) == 0:
            score += 3  # –•–æ—Ä–æ—à–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–µ–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
    for field in [field1, field2]:
        sorted_field = sorted(field)
        gaps = [sorted_field[i+1] - sorted_field[i] for i in range(len(sorted_field)-1)]

        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –∏–º–µ—Ç—å —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
        if gaps and np.std(gaps) < 3:
            score += 5

    return min(15, score)


def _check_uniqueness(field1, field2, df_history):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏"""
    combo_str = f"{sorted(field1)}-{sorted(field2)}"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Å—Ç—Ä–µ—á–∞–ª–∞—Å—å –ª–∏ —Ç–∞–∫–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
    for _, row in df_history.iterrows():
        if (sorted(row['–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list']) == sorted(field1) and
            sorted(row['–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list']) == sorted(field2)):
            return 0  # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —É–∂–µ –≤—ã–ø–∞–¥–∞–ª–∞

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ö–æ–∂–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ (—Å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ–º > 80%)
    for _, row in df_history.iterrows():
        f1_intersection = len(set(field1) & set(row['–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list']))
        f2_intersection = len(set(field2) & set(row['–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list']))

        total_numbers = len(field1) + len(field2)
        total_intersections = f1_intersection + f2_intersection

        similarity = total_intersections / total_numbers

        if similarity > 0.8:
            return 5  # –û—á–µ–Ω—å –ø–æ—Ö–æ–∂–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
        elif similarity > 0.6:
            return 10  # –ß–∞—Å—Ç–∏—á–Ω–æ –ø–æ—Ö–æ–∂–∞—è

    return 15  # –£–Ω–∏–∫–∞–ª—å–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è


def _generate_improvement_suggestions(score, hot_count, cold_count, field1, field2):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è"""
    suggestions = []

    if score < 40:
        if hot_count < 2:
            suggestions.append("–î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –≥–æ—Ä—è—á–∏—Ö —á–∏—Å–µ–ª (2-3 —à—Ç)")
        if cold_count < 1:
            suggestions.append("–í–∫–ª—é—á–∏—Ç–µ 1-2 —Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–ª–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        sequences_f1 = _count_sequences(field1)
        sequences_f2 = _count_sequences(field2)

        if sequences_f1 > 2:
            suggestions.append("–£–º–µ–Ω—å—à–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —á–∏—Å–µ–ª –≤ –ø–æ–ª–µ 1")
        if sequences_f2 > 1:
            suggestions.append("–ò–∑–±–µ–≥–∞–π—Ç–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —á–∏—Å–µ–ª –≤ –ø–æ–ª–µ 2")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        if len(field1) > 2:
            spread_f1 = max(field1) - min(field1)
            if spread_f1 < len(field1) * 2:
                suggestions.append("–£–≤–µ–ª–∏—á—å—Ç–µ —Ä–∞–∑–±—Ä–æ—Å —á–∏—Å–µ–ª –≤ –ø–æ–ª–µ 1")

    elif score < 70:
        suggestions.append("–•–æ—Ä–æ—à–∞—è –±–∞–∑–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–º–µ–Ω–∏—Ç—å 1-2 —á–∏—Å–ª–∞ –Ω–∞ –±–æ–ª–µ–µ –≥–æ—Ä—è—á–∏–µ")
        suggestions.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ - –≤–æ–∑–º–æ–∂–Ω–æ –µ—Å—Ç—å —É–¥–∞—á–Ω—ã–µ –ø–∞—Ä—ã")

    if not suggestions:
        suggestions.append("–û—Ç–ª–∏—á–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è! –ú–æ–∂–Ω–æ –∏–≥—Ä–∞—Ç—å –∫–∞–∫ –µ—Å—Ç—å")

    return suggestions



def _count_sequences(numbers):
    """–ü–æ–¥—Å—á–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —á–∏—Å–µ–ª"""
    if len(numbers) < 2:
        return 0

    sorted_nums = sorted(numbers)
    sequences = 0

    for i in range(1, len(sorted_nums)):
        if sorted_nums[i] == sorted_nums[i - 1] + 1:
            sequences += 1

    return sequences


def _prepare_clustering_features_fixed(df, field_name, max_num):
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    features = {}

    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
    column_name = '–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list' if field_name == 'field1' else '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'

    print(f"üîß –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è {field_name}, –∫–æ–ª–æ–Ω–∫–∞: {column_name}")

    for num in range(1, max_num + 1):
        try:
            # –ß–∞—Å—Ç–æ—Ç–∞ –ø–æ—è–≤–ª–µ–Ω–∏—è
            frequency = sum(num in row for row in df[column_name])

            # –°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –ø–æ—è–≤–ª–µ–Ω–∏—è–º–∏
            appearances = [i for i, row in enumerate(df[column_name]) if num in row]
            avg_gap = np.mean(np.diff(appearances)) if len(appearances) > 1 else len(df)

            # –¢—Ä–µ–Ω–¥ (–ø–æ—è–≤–ª–µ–Ω–∏—è –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 20% —Ç–∏—Ä–∞–∂–µ–π)
            recent_df = df.tail(max(1, len(df) // 5))
            recent_freq = sum(num in row for row in recent_df[column_name])

            features[num] = [frequency, avg_gap, recent_freq]
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∏—Å–ª–∞ {num}: {e}")
            features[num] = [0, len(df), 0]  # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

    return features


def _interpret_clusters_fixed(clusters_f1, clusters_f2, df_history):
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
    interpretation = {}

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º –¥–ª—è –æ–±–æ–∏—Ö –ø–æ–ª–µ–π
    for field_name, clusters in [("field1", clusters_f1), ("field2", clusters_f2)]:
        cluster_groups = {}
        for num, cluster_id in clusters.items():
            if cluster_id not in cluster_groups:
                cluster_groups[cluster_id] = []
            cluster_groups[cluster_id].append(num)

        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        column_name = '–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list' if field_name == 'field1' else '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        for cluster_id, numbers in cluster_groups.items():
            try:
                if cluster_id == -1:  # DBSCAN outliers
                    interpretation[f"{field_name}_outliers"] = {
                        "numbers": numbers,
                        "description": "–ê–Ω–æ–º–∞–ª—å–Ω—ã–µ —á–∏—Å–ª–∞ —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏"
                    }
                else:
                    # –ê–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–∞
                    avg_frequency = np.mean([
                        sum(num in row for row in df_history[column_name])
                        for num in numbers
                    ])

                    if avg_frequency > len(df_history) * 0.15:
                        desc = "–ì–æ—Ä—è—á–∏–π –∫–ª–∞—Å—Ç–µ—Ä - —á–∏—Å–ª–∞ —Å –≤—ã—Å–æ–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π"
                    elif avg_frequency < len(df_history) * 0.05:
                        desc = "–•–æ–ª–æ–¥–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä - —Ä–µ–¥–∫–æ –≤—ã–ø–∞–¥–∞—é—â–∏–µ —á–∏—Å–ª–∞"
                    else:
                        desc = "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä - —Å—Ä–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"

                    interpretation[f"{field_name}_cluster_{cluster_id}"] = {
                        "numbers": numbers,
                        "description": desc,
                        "avg_frequency": round(avg_frequency, 2)
                    }
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id}: {e}")
                interpretation[f"{field_name}_cluster_{cluster_id}"] = {
                    "numbers": numbers,
                    "description": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞",
                    "avg_frequency": 0
                }

    return interpretation