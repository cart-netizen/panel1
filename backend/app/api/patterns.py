from datetime import datetime

from fastapi import APIRouter, Depends, Path, Query, HTTPException
from typing import List

from backend.app.core import pattern_analyzer, data_manager
from backend.app.models.schemas import (
  FullPatternAnalysis, PatternAnalysisResponse, HotColdNumbers,
  CorrelationPair, CycleStat
)
from .analysis import set_lottery_context
from backend.app.core.subscription_protection import require_premium

from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
import numpy as np
from typing import List, Dict, Any, Optional
from pydantic import BaseModel



router = APIRouter()


class ClusterAnalysisRequest(BaseModel):
  """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
  method: str = "kmeans"  # kmeans, dbscan
  n_clusters: int = 5


class CombinationEvaluationRequest(BaseModel):
  """–ó–∞–ø—Ä–æ—Å –Ω–∞ –æ—Ü–µ–Ω–∫—É –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏"""
  field1: List[int]
  field2: List[int]

class PatternVisualizationResponse(BaseModel):
  """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
  hot_cold: Dict[str, Any]
  correlations: Dict[str, Any]
  cycles_field1: List[Dict[str, Any]]
  cycles_field2: List[Dict[str, Any]]
  frequency_data: Dict[str, Any]
  trend_data: List[Dict[str, Any]]
  metadata: Dict[str, Any]


class FrequencyDataRequest(BaseModel):
  """–ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç –¥–ª—è —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã"""
  window_size: Optional[int] = 30
  include_trends: Optional[bool] = True


@router.get("/visualization", response_model=PatternVisualizationResponse,
            summary="üé® –î–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
def get_pattern_visualization_data(
    window: int = Query(30, description="–†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞"),
    top_n: int = Query(10, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —ç–ª–µ–º–µ–Ω—Ç–æ–≤"),
    include_frequencies: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ—Ç"),
    include_trends: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ"),
    context: None = Depends(set_lottery_context),
    current_user=Depends(require_premium)
):
  """
  üîí –ü–†–ï–ú–ò–£–ú: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤

  **–í–∫–ª—é—á–∞–µ—Ç:**
  - üî• –ì–æ—Ä—è—á–∏–µ/—Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞ —Å —á–∞—Å—Ç–æ—Ç–∞–º–∏
  - üîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É —á–∏—Å–ª–∞–º–∏
  - üìä –î–∞–Ω–Ω—ã–µ —Ü–∏–∫–ª–æ–≤ –∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
  - üìà –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
  - üéØ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
  """

  df_history = data_manager.fetch_draws_from_db()

  if df_history.empty:
    raise HTTPException(status_code=404, detail="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

  try:
    # 1. –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    hot_cold_raw = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.analyze_hot_cold_numbers(
      df_history, window_sizes=[window], top_n=top_n
    )

    # 2. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    correlations_raw = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.find_number_correlations(df_history)

    # 3. –¶–∏–∫–ª—ã
    cycles_raw = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.analyze_draw_cycles(df_history)

    # 4. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
    hot_cold = {}
    for key, data in hot_cold_raw.items():
      field_key = f"field{data['field']}"
      hot_cold[field_key] = {
        'hot': [item[0] for item in data.get('hot_numbers', [])],
        'cold': [item[0] for item in data.get('cold_numbers', [])]
      }

    # 5. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    correlations = {
      'field1': [
        {
          'pair': f"{p[0]}-{p[1]}",
          'frequency_percent': freq,
          'count': count
        }
        for p, count, freq in correlations_raw.get('field1', {}).get('frequent_pairs', [])
      ],
      'field2': [
        {
          'pair': f"{p[0]}-{p[1]}",
          'frequency_percent': freq,
          'count': count
        }
        for p, count, freq in correlations_raw.get('field2', {}).get('frequent_pairs', [])
      ]
    }

    # 6. –¶–∏–∫–ª—ã
    cycles_field1 = [
      {
        'number': num,
        'last_seen_ago': stats['last_seen'],
        'avg_cycle': stats['avg_cycle'],
        'is_overdue': stats['overdue']
      }
      for num, stats in cycles_raw.get('field1', {}).items()
    ]

    cycles_field2 = [
      {
        'number': num,
        'last_seen_ago': stats['last_seen'],
        'avg_cycle': stats['avg_cycle'],
        'is_overdue': stats['overdue']
      }
      for num, stats in cycles_raw.get('field2', {}).items()
    ]

    # 7. –î–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ—Ç (–µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω—ã)
    frequency_data = {}
    if include_frequencies:
      frequency_data = _generate_frequency_data(df_history, window)

    # 8. –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω—ã)
    trend_data = []
    if include_trends:
      trend_data = _generate_trend_data(df_history, hot_cold, window)

    # 9. –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {
      'total_draws': len(df_history),
      'analyzed_period_days': (df_history.index[0] - df_history.index[-1]).days if len(df_history) > 1 else 0,
      'confidence_score': _calculate_confidence_score(df_history),
      'last_updated': datetime.utcnow().isoformat(),
      'window_size': window,
      'algorithm_version': '2.1.0'
    }

    return PatternVisualizationResponse(
      hot_cold=hot_cold,
      correlations=correlations,
      cycles_field1=cycles_field1,
      cycles_field2=cycles_field2,
      frequency_data=frequency_data,
      trend_data=trend_data,
      metadata=metadata
    )

  except Exception as e:
    import traceback
    print(f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {e}")
    print(traceback.format_exc())
    raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")


@router.get("/patterns", response_model=FullPatternAnalysis, summary="–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
def get_full_pattern_analysis(
    window: int = Query(20, description="–û–∫–Ω–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≥–æ—Ä—è—á–∏—Ö/—Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª (–≤ —Ç–∏—Ä–∞–∂–∞—Ö)"),
    top_n: int = Query(5, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —á–∏—Å–µ–ª –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"),
    context: None = Depends(set_lottery_context), current_user = Depends(require_premium)
):
  """
  –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, –≤–∫–ª—é—á–∞—è –≥–æ—Ä—è—á–∏–µ/—Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞,
  –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –ø–∞—Ä—ã –∏ –∞–Ω–∞–ª–∏–∑ —Ü–∏–∫–ª–æ–≤ –≤—ã–ø–∞–¥–µ–Ω–∏—è —á–∏—Å–µ–ª.

  üîí –ü–†–ï–ú–ò–£–ú –§–£–ù–ö–¶–ò–Ø: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.

  **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:** –ü—Ä–µ–º–∏—É–º –ø–æ–¥–ø–∏—Å–∫–∞ –∏–ª–∏ –≤—ã—à–µ

  **–í–∫–ª—é—á–∞–µ—Ç:**
  - üî• –ì–æ—Ä—è—á–∏–µ –∏ —Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞
  - üîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É —á–∏—Å–ª–∞–º–∏
  - üìä –ê–Ω–∞–ª–∏–∑ —Ü–∏–∫–ª–æ–≤ –≤—ã–ø–∞–¥–µ–Ω–∏—è
  - üìà –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏

  """
  df_history = data_manager.fetch_draws_from_db()

  # 1. –ì–æ—Ä—è—á–∏–µ/—Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞
  hot_cold_raw = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.analyze_hot_cold_numbers(
    df_history, window_sizes=[window], top_n=top_n
  )
  hot_cold_resp = {}
  for key, data in hot_cold_raw.items():
    field_key = f"field{data['field']}"
    hot_cold_resp[field_key] = HotColdNumbers(
      hot=data.get('hot_numbers', []),
      cold=data.get('cold_numbers', [])
    )

  # 2. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
  correlations_raw = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.find_number_correlations(df_history)
  correlations_f1 = [
    CorrelationPair(pair=f"{p[0]}-{p[1]}", frequency_percent=freq, count=count)
    for p, count, freq in correlations_raw.get('field1', {}).get('frequent_pairs', [])
  ]
  correlations_f2 = [
    CorrelationPair(pair=f"{p[0]}-{p[1]}", frequency_percent=freq, count=count)
    for p, count, freq in correlations_raw.get('field2', {}).get('frequent_pairs', [])
  ]

  # 3. –¶–∏–∫–ª—ã
  cycles_raw = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.analyze_draw_cycles(df_history)
  cycles_f1 = [
    CycleStat(number=num, last_seen_ago=stats['last_seen'], avg_cycle=stats['avg_cycle'], is_overdue=stats['overdue'])
    for num, stats in cycles_raw.get('field1', {}).items()
  ]
  cycles_f2 = [
    CycleStat(number=num, last_seen_ago=stats['last_seen'], avg_cycle=stats['avg_cycle'], is_overdue=stats['overdue'])
    for num, stats in cycles_raw.get('field2', {}).items()
  ]

  return FullPatternAnalysis(
    hot_cold=PatternAnalysisResponse(**hot_cold_resp),
    correlations_field1=correlations_f1,
    correlations_field2=correlations_f2,
    cycles_field1=sorted(cycles_f1, key=lambda x: x.last_seen_ago, reverse=True),
    cycles_field2=sorted(cycles_f2, key=lambda x: x.last_seen_ago, reverse=True)
  )

@router.post("/cluster-analysis", summary="üéØ –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
def cluster_analysis_simple(
    request: ClusterAnalysisRequest,
    context: None = Depends(set_lottery_context),
    current_user=Depends(require_premium)
):
  """
  üîí –ü–†–ï–ú–ò–£–ú: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —á–∏—Å–µ–ª –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –≤—ã–ø–∞–¥–µ–Ω–∏—è
  """
  df_history = data_manager.fetch_draws_from_db()

  if df_history.empty:
    from fastapi import HTTPException
    raise HTTPException(status_code=404, detail="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

  config = data_manager.get_current_config()

  # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
  def prepare_features(field_col, max_num):
    features = {}
    for num in range(1, max_num + 1):
      frequency = sum(num in row for row in df_history[field_col])
      appearances = [i for i, row in enumerate(df_history[field_col]) if num in row]
      avg_gap = np.mean(np.diff(appearances)) if len(appearances) > 1 else len(df_history)
      recent_freq = sum(num in row for row in df_history[field_col].head(20))
      features[num] = [frequency, avg_gap, recent_freq]
    return features

  # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±–æ–∏—Ö –ø–æ–ª–µ–π
  features_f1 = prepare_features('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', config['field1_max'])
  features_f2 = prepare_features('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', config['field2_max'])

  def perform_clustering(features, n_clusters, method):
    if not features:
      return {}

    X = np.array(list(features.values()))
    numbers = list(features.keys())

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    if method == "kmeans":
      clusterer = KMeans(n_clusters=min(n_clusters, len(numbers)), random_state=42)
    else:  # dbscan
      clusterer = DBSCAN(eps=0.5, min_samples=2)

    labels = clusterer.fit_predict(X_scaled)
    return {num: int(label) for num, label in zip(numbers, labels)}

  clusters_f1 = perform_clustering(features_f1, request.n_clusters, request.method)
  clusters_f2 = perform_clustering(features_f2, request.n_clusters, request.method)

  # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
  def group_clusters(clusters):
    groups = {}
    for num, cluster_id in clusters.items():
      if cluster_id not in groups:
        groups[cluster_id] = []
      groups[cluster_id].append(num)
    return groups

  result = {
    "status": "success",
    "method": request.method,
    "clusters": {
      "field1": group_clusters(clusters_f1),
      "field2": group_clusters(clusters_f2)
    },
    "n_clusters_found": {
      "field1": len(set(clusters_f1.values())),
      "field2": len(set(clusters_f2.values()))
    },
    "interpretation": {
      "field1": "–ß–∏—Å–ª–∞ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —á–∞—Å—Ç–æ—Ç–µ –≤—ã–ø–∞–¥–µ–Ω–∏—è",
      "field2": "–ù–∞–π–¥–µ–Ω—ã –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —á–∏—Å–µ–ª"
    }
  }

  return result


@router.post("/evaluate-combination", summary="‚≠ê –û—Ü–µ–Ω–∫–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏")
def evaluate_combination_simple(
    request: CombinationEvaluationRequest,
    context: None = Depends(set_lottery_context),
    current_user=Depends(require_premium)
):
  """
  üîí –ü–†–ï–ú–ò–£–ú: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
  """
  df_history = data_manager.fetch_draws_from_db()

  if df_history.empty:
    from fastapi import HTTPException
    raise HTTPException(status_code=404, detail="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

  # –ü—Ä–æ—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏
  score = 50  # –ë–∞–∑–æ–≤—ã–π —Å—á–µ—Ç
  factors = []

  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω –ø–æ–ª–µ–π
  config = data_manager.get_current_config()
  if len(request.field1) != config['field1_count'] or len(request.field2) != config['field2_count']:
    return {
      "score": 0,
      "recommendation": "–ù–µ–≤–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∏—Å–µ–ª",
      "combination": {
        "field1": request.field1,
        "field2": request.field2
      },
      "factors": ["–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–ª–µ–π"]
    }

  # –ê–Ω–∞–ª–∏–∑ –≥–æ—Ä—è—á–∏—Ö/—Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª
  hot_cold = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.analyze_hot_cold_numbers(
    df_history, window_sizes=[20], top_n=5
  )

  # –ü–æ–¥—Å—á–µ—Ç –≥–æ—Ä—è—á–∏—Ö –∏ —Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª –≤ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
  hot_count = 0
  cold_count = 0

  for field_data in hot_cold.values():
    field_num = field_data['field']
    hot_nums = [num for num, _ in field_data.get('hot_numbers', [])]
    cold_nums = [num for num, _ in field_data.get('cold_numbers', [])]

    field_combo = request.field1 if field_num == 1 else request.field2

    hot_count += len(set(field_combo) & set(hot_nums))
    cold_count += len(set(field_combo) & set(cold_nums))

  # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Å—á–µ—Ç–∞
  if hot_count > 0:
    score += hot_count * 10
    factors.append(f"‚úÖ –°–æ–¥–µ—Ä–∂–∏—Ç {hot_count} –≥–æ—Ä—è—á–∏—Ö —á–∏—Å–µ–ª (+{hot_count * 10} –æ—á–∫–æ–≤)")

  if cold_count > 2:
    score -= cold_count * 5
    factors.append(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª (-{cold_count * 5} –æ—á–∫–æ–≤)")

  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
  def count_sequences(numbers):
    sorted_nums = sorted(numbers)
    sequences = 0
    for i in range(1, len(sorted_nums)):
      if sorted_nums[i] == sorted_nums[i - 1] + 1:
        sequences += 1
    return sequences

  seq1 = count_sequences(request.field1)
  seq2 = count_sequences(request.field2)

  if seq1 + seq2 > 3:
    score -= 15
    factors.append("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —á–∏—Å–µ–ª (-15 –æ—á–∫–æ–≤)")
  elif seq1 + seq2 <= 1:
    score += 10
    factors.append("‚úÖ –•–æ—Ä–æ—à–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–µ–ª (+10 –æ—á–∫–æ–≤)")

  # –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞
  score = max(0, min(100, score))

  # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
  if score >= 80:
    recommendation = "–û—Ç–ª–∏—á–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è! –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∫ –∏–≥—Ä–µ"
  elif score >= 60:
    recommendation = "–•–æ—Ä–æ—à–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º"
  elif score >= 40:
    recommendation = "–°—Ä–µ–¥–Ω—è—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è, –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å"
  else:
    recommendation = "–°–ª–∞–±–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å"

  return {
    "score": score,
    "recommendation": recommendation,
    "combination": {
      "field1": request.field1,
      "field2": request.field2
    },
    "factors": factors
  }

def _generate_frequency_data(df_history, window_size: int) -> Dict[str, Any]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ—Ç –¥–ª—è —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã"""
    try:
      recent_df = df_history.tail(window_size)

      # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç –¥–ª—è –ø–æ–ª—è 1
      field1_frequencies = {}
      for _, row in recent_df.iterrows():
        for num in row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', []):
          field1_frequencies[num] = field1_frequencies.get(num, 0) + 1

      # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç –¥–ª—è –ø–æ–ª—è 2
      field2_frequencies = {}
      for _, row in recent_df.iterrows():
        for num in row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', []):
          field2_frequencies[num] = field2_frequencies.get(num, 0) + 1

      return {
        'field1_frequencies': field1_frequencies,
        'field2_frequencies': field2_frequencies,
        'window_analyzed': len(recent_df),
        'frequency_stats': {
          'field1_avg': np.mean(list(field1_frequencies.values())) if field1_frequencies else 0,
          'field1_max': max(field1_frequencies.values()) if field1_frequencies else 0,
          'field1_min': min(field1_frequencies.values()) if field1_frequencies else 0,
          'field2_avg': np.mean(list(field2_frequencies.values())) if field2_frequencies else 0,
          'field2_max': max(field2_frequencies.values()) if field2_frequencies else 0,
          'field2_min': min(field2_frequencies.values()) if field2_frequencies else 0,
        }
      }

    except Exception as e:
      print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç: {e}")
      return {'field1_frequencies': {}, 'field2_frequencies': {}}


def _generate_trend_data(df_history, hot_cold_data: Dict, window_size: int) -> List[Dict[str, Any]]:
  """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
  try:
    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ window_size —Ç–∏—Ä–∞–∂–µ–π
    recent_df = df_history.tail(window_size)

    # –ü–æ–ª—É—á–∞–µ–º –≥–æ—Ä—è—á–∏–µ —á–∏—Å–ª–∞
    hot_field1 = hot_cold_data.get('field1', {}).get('hot', [])[:5]  # —Ç–æ–ø-5
    hot_field2 = hot_cold_data.get('field2', {}).get('hot', [])[:3]  # —Ç–æ–ø-3

    trend_data = []

    for idx, (_, row) in enumerate(recent_df.iterrows()):
      draw_data = {'draw': idx + 1}

      # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≥–æ—Ä—è—á–∏—Ö —á–∏—Å–µ–ª –ø–æ–ª—è 1
      for num in hot_field1:
        field1_nums = row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', [])
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º "–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å" –∫–∞–∫ –ø–æ—è–≤–ª–µ–Ω–∏–µ —á–∏—Å–ª–∞ + –µ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
        activity = 0
        if num in field1_nums:
          activity = 5.0  # –±–∞–∑–æ–≤–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∑–∞ –ø–æ—è–≤–ª–µ–Ω–∏–µ
        else:
          # –ë–ª–∏–∑–æ—Å—Ç—å –∫ —á–∏—Å–ª–∞–º –≤ —Ç–∏—Ä–∞–∂–µ (—á–µ–º –±–ª–∏–∂–µ, —Ç–µ–º –≤—ã—à–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å)
          if field1_nums:
            min_distance = min(abs(num - n) for n in field1_nums)
            activity = max(0, 3 - min_distance * 0.5)

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
        activity += (np.random.random() - 0.5) * 1.0
        draw_data[f'num{num}'] = max(0, activity)

      # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≥–æ—Ä—è—á–∏—Ö —á–∏—Å–µ–ª –ø–æ–ª—è 2
      for num in hot_field2:
        field2_nums = row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', [])
        activity = 0
        if num in field2_nums:
          activity = 6.0
        else:
          if field2_nums:
            min_distance = min(abs(num - n) for n in field2_nums)
            activity = max(0, 4 - min_distance * 0.8)

        activity += (np.random.random() - 0.5) * 1.2
        draw_data[f'field2_num{num}'] = max(0, activity)

      trend_data.append(draw_data)

    return trend_data

  except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
    return []


def _calculate_confidence_score(df_history) -> float:
  """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∞–Ω–∞–ª–∏–∑–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
  data_count = len(df_history)

  if data_count < 50:
    return 0.3  # –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
  elif data_count < 100:
    return 0.6  # —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
  elif data_count < 500:
    return 0.8  # –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
  else:
    return 0.95  # –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å


@router.post("/favorites/analyze", summary="üéØ –ê–Ω–∞–ª–∏–∑ –∏–∑–±—Ä–∞–Ω–Ω—ã—Ö —á–∏—Å–µ–ª")
def analyze_favorite_numbers(
    favorites: Dict[str, List[int]],
    context: None = Depends(set_lottery_context),
    current_user=Depends(require_premium)
):
  """
  –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑–±—Ä–∞–Ω–Ω—ã—Ö —á–∏—Å–µ–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

  Body:
  {
      "field1": [7, 12, 19],
      "field2": [2, 4]
  }
  """
  df_history = data_manager.fetch_draws_from_db()

  try:
    field1_favorites = favorites.get('field1', [])
    field2_favorites = favorites.get('field2', [])

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ –∏–∑–±—Ä–∞–Ω–Ω–æ–µ —á–∏—Å–ª–æ
    field1_analysis = {}
    for num in field1_favorites:
      # –ü–æ–¥—Å—á–µ—Ç –ø–æ—è–≤–ª–µ–Ω–∏–π
      appearances = sum(1 for _, row in df_history.iterrows()
                        if num in row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', []))

      field1_analysis[str(num)] = {
        'frequency': appearances,
        'percentage': (appearances / len(df_history) * 100) if len(df_history) > 0 else 0,
        'last_seen': _get_last_seen(df_history, num, 'field1'),
        'avg_interval': _get_avg_interval(df_history, num, 'field1')
      }

    field2_analysis = {}
    for num in field2_favorites:
      appearances = sum(1 for _, row in df_history.iterrows()
                        if num in row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', []))

      field2_analysis[str(num)] = {
        'frequency': appearances,
        'percentage': (appearances / len(df_history) * 100) if len(df_history) > 0 else 0,
        'last_seen': _get_last_seen(df_history, num, 'field2'),
        'avg_interval': _get_avg_interval(df_history, num, 'field2')
      }

    return {
      'favorites_analysis': {
        'field1': field1_analysis,
        'field2': field2_analysis
      },
      'summary': {
        'total_favorites': len(field1_favorites) + len(field2_favorites),
        'analysis_period': len(df_history),
        'avg_performance_field1': np.mean(
          [data['percentage'] for data in field1_analysis.values()]) if field1_analysis else 0,
        'avg_performance_field2': np.mean(
          [data['percentage'] for data in field2_analysis.values()]) if field2_analysis else 0
      }
    }

  except Exception as e:
    raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–±—Ä–∞–Ω–Ω—ã—Ö: {str(e)}")


def _get_last_seen(df_history, number: int, field: str) -> int:
  """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏—Ä–∞–∂–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è —á–∏—Å–ª–∞"""
  field_column = '–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list' if field == 'field1' else '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'

  for idx, (_, row) in enumerate(df_history.iterrows()):
    if number in row.get(field_column, []):
      return idx

  return len(df_history)  # –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –ø–æ—è–≤–ª—è–ª–æ—Å—å


def _get_avg_interval(df_history, number: int, field: str) -> float:
  """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –ø–æ—è–≤–ª–µ–Ω–∏—è–º–∏ —á–∏—Å–ª–∞"""
  field_column = '–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list' if field == 'field1' else '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'

  appearances = []
  for idx, (_, row) in enumerate(df_history.iterrows()):
    if number in row.get(field_column, []):
      appearances.append(idx)

  if len(appearances) < 2:
    return len(df_history)  # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö

  intervals = [appearances[i + 1] - appearances[i] for i in range(len(appearances) - 1)]
  return np.mean(intervals)