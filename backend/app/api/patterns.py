from fastapi import APIRouter, Depends, Path, Query
from typing import List

from backend.app.core import pattern_analyzer, data_manager
from backend.app.models.schemas import (
  FullPatternAnalysis, PatternAnalysisResponse, HotColdNumbers,
  CorrelationPair, CycleStat
)
from .analysis import set_lottery_context
from backend.app.core.subscription_protection import require_premium

from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
import numpy as np
from typing import List, Dict, Any
from pydantic import BaseModel

router = APIRouter()


class ClusterAnalysisRequest(BaseModel):
  """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
  method: str = "kmeans"  # kmeans, dbscan
  n_clusters: int = 5


class CombinationEvaluationRequest(BaseModel):
  """–ó–∞–ø—Ä–æ—Å –Ω–∞ –æ—Ü–µ–Ω–∫—É –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏"""
  field1: List[int]
  field2: List[int]


@router.get("/patterns", response_model=FullPatternAnalysis, summary="–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
def get_full_pattern_analysis(
    window: int = Query(20, description="–û–∫–Ω–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≥–æ—Ä—è—á–∏—Ö/—Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª (–≤ —Ç–∏—Ä–∞–∂–∞—Ö)"),
    top_n: int = Query(5, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —á–∏—Å–µ–ª –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"),
    context: None = Depends(set_lottery_context), current_user = Depends(require_premium)
):
  """
  –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, –≤–∫–ª—é—á–∞—è –≥–æ—Ä—è—á–∏–µ/—Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞,
  –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –ø–∞—Ä—ã –∏ –∞–Ω–∞–ª–∏–∑ —Ü–∏–∫–ª–æ–≤ –≤—ã–ø–∞–¥–µ–Ω–∏—è —á–∏—Å–µ–ª.

  üîí –ü–†–ï–ú–ò–£–ú –§–£–ù–ö–¶–ò–Ø: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.

  **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:** –ü—Ä–µ–º–∏—É–º –ø–æ–¥–ø–∏—Å–∫–∞ –∏–ª–∏ –≤—ã—à–µ

  **–í–∫–ª—é—á–∞–µ—Ç:**
  - üî• –ì–æ—Ä—è—á–∏–µ –∏ —Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞
  - üîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É —á–∏—Å–ª–∞–º–∏
  - üìä –ê–Ω–∞–ª–∏–∑ —Ü–∏–∫–ª–æ–≤ –≤—ã–ø–∞–¥–µ–Ω–∏—è
  - üìà –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏

  """
  df_history = data_manager.fetch_draws_from_db()

  # 1. –ì–æ—Ä—è—á–∏–µ/—Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞
  hot_cold_raw = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.analyze_hot_cold_numbers(
    df_history, window_sizes=[window], top_n=top_n
  )
  hot_cold_resp = {}
  for key, data in hot_cold_raw.items():
    field_key = f"field{data['field']}"
    hot_cold_resp[field_key] = HotColdNumbers(
      hot=data.get('hot_numbers', []),
      cold=data.get('cold_numbers', [])
    )

  # 2. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
  correlations_raw = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.find_number_correlations(df_history)
  correlations_f1 = [
    CorrelationPair(pair=f"{p[0]}-{p[1]}", frequency_percent=freq, count=count)
    for p, count, freq in correlations_raw.get('field1', {}).get('frequent_pairs', [])
  ]
  correlations_f2 = [
    CorrelationPair(pair=f"{p[0]}-{p[1]}", frequency_percent=freq, count=count)
    for p, count, freq in correlations_raw.get('field2', {}).get('frequent_pairs', [])
  ]

  # 3. –¶–∏–∫–ª—ã
  cycles_raw = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.analyze_draw_cycles(df_history)
  cycles_f1 = [
    CycleStat(number=num, last_seen_ago=stats['last_seen'], avg_cycle=stats['avg_cycle'], is_overdue=stats['overdue'])
    for num, stats in cycles_raw.get('field1', {}).items()
  ]
  cycles_f2 = [
    CycleStat(number=num, last_seen_ago=stats['last_seen'], avg_cycle=stats['avg_cycle'], is_overdue=stats['overdue'])
    for num, stats in cycles_raw.get('field2', {}).items()
  ]

  return FullPatternAnalysis(
    hot_cold=PatternAnalysisResponse(**hot_cold_resp),
    correlations_field1=correlations_f1,
    correlations_field2=correlations_f2,
    cycles_field1=sorted(cycles_f1, key=lambda x: x.last_seen_ago, reverse=True),
    cycles_field2=sorted(cycles_f2, key=lambda x: x.last_seen_ago, reverse=True)
  )

#
# @router.get("/analyze-patterns", summary="üî• –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
# def analyze_patterns_simple(
#     context: None = Depends(set_lottery_context),
#     current_user=Depends(require_premium)
# ):
#   """
#   üîí –ü–†–ï–ú–ò–£–ú: –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
#
#   –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –æ–∂–∏–¥–∞–µ–º–æ–º AnalysisPage.tsx
#   """
#   df_history = data_manager.fetch_draws_from_db()
#
#   if df_history.empty:
#     from fastapi import HTTPException
#     raise HTTPException(status_code=404, detail="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
#
#   # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
#   hot_cold_raw = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.analyze_hot_cold_numbers(
#     df_history, window_sizes=[20], top_n=5
#   )
#
#   # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
#   result = {
#     "total_draws": len(df_history),
#     "analyzed_period": (df_history.index[0] - df_history.index[-1]).days if len(df_history) > 1 else 0,
#     "patterns_found": len(hot_cold_raw),
#     "confidence_score": 0.85,  # –ü—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏—è
#     "hot_numbers": {},
#     "cold_numbers": {},
#     "timestamp": df_history.index[0].isoformat() if not df_history.empty else None
#   }
#
#   # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ—Ä—è—á–∏—Ö –∏ —Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª
#   for key, data in hot_cold_raw.items():
#     field_key = f"field{data['field']}"
#     result["hot_numbers"][field_key] = [num for num, _ in data.get('hot_numbers', [])]
#     result["cold_numbers"][field_key] = [num for num, _ in data.get('cold_numbers', [])]
#
#   return result


@router.post("/cluster-analysis", summary="üéØ –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
def cluster_analysis_simple(
    request: ClusterAnalysisRequest,
    context: None = Depends(set_lottery_context),
    current_user=Depends(require_premium)
):
  """
  üîí –ü–†–ï–ú–ò–£–ú: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —á–∏—Å–µ–ª –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –≤—ã–ø–∞–¥–µ–Ω–∏—è
  """
  df_history = data_manager.fetch_draws_from_db()

  if df_history.empty:
    from fastapi import HTTPException
    raise HTTPException(status_code=404, detail="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

  config = data_manager.get_current_config()

  # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
  def prepare_features(field_col, max_num):
    features = {}
    for num in range(1, max_num + 1):
      frequency = sum(num in row for row in df_history[field_col])
      appearances = [i for i, row in enumerate(df_history[field_col]) if num in row]
      avg_gap = np.mean(np.diff(appearances)) if len(appearances) > 1 else len(df_history)
      recent_freq = sum(num in row for row in df_history[field_col].head(20))
      features[num] = [frequency, avg_gap, recent_freq]
    return features

  # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±–æ–∏—Ö –ø–æ–ª–µ–π
  features_f1 = prepare_features('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', config['field1_max'])
  features_f2 = prepare_features('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', config['field2_max'])

  def perform_clustering(features, n_clusters, method):
    if not features:
      return {}

    X = np.array(list(features.values()))
    numbers = list(features.keys())

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    if method == "kmeans":
      clusterer = KMeans(n_clusters=min(n_clusters, len(numbers)), random_state=42)
    else:  # dbscan
      clusterer = DBSCAN(eps=0.5, min_samples=2)

    labels = clusterer.fit_predict(X_scaled)
    return {num: int(label) for num, label in zip(numbers, labels)}

  clusters_f1 = perform_clustering(features_f1, request.n_clusters, request.method)
  clusters_f2 = perform_clustering(features_f2, request.n_clusters, request.method)

  # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
  def group_clusters(clusters):
    groups = {}
    for num, cluster_id in clusters.items():
      if cluster_id not in groups:
        groups[cluster_id] = []
      groups[cluster_id].append(num)
    return groups

  result = {
    "status": "success",
    "method": request.method,
    "clusters": {
      "field1": group_clusters(clusters_f1),
      "field2": group_clusters(clusters_f2)
    },
    "n_clusters_found": {
      "field1": len(set(clusters_f1.values())),
      "field2": len(set(clusters_f2.values()))
    },
    "interpretation": {
      "field1": "–ß–∏—Å–ª–∞ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —á–∞—Å—Ç–æ—Ç–µ –≤—ã–ø–∞–¥–µ–Ω–∏—è",
      "field2": "–ù–∞–π–¥–µ–Ω—ã –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —á–∏—Å–µ–ª"
    }
  }

  return result


@router.post("/evaluate-combination", summary="‚≠ê –û—Ü–µ–Ω–∫–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏")
def evaluate_combination_simple(
    request: CombinationEvaluationRequest,
    context: None = Depends(set_lottery_context),
    current_user=Depends(require_premium)
):
  """
  üîí –ü–†–ï–ú–ò–£–ú: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
  """
  df_history = data_manager.fetch_draws_from_db()

  if df_history.empty:
    from fastapi import HTTPException
    raise HTTPException(status_code=404, detail="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

  # –ü—Ä–æ—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏
  score = 50  # –ë–∞–∑–æ–≤—ã–π —Å—á–µ—Ç
  factors = []

  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω –ø–æ–ª–µ–π
  config = data_manager.get_current_config()
  if len(request.field1) != config['field1_count'] or len(request.field2) != config['field2_count']:
    return {
      "score": 0,
      "recommendation": "–ù–µ–≤–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∏—Å–µ–ª",
      "combination": {
        "field1": request.field1,
        "field2": request.field2
      },
      "factors": ["–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–ª–µ–π"]
    }

  # –ê–Ω–∞–ª–∏–∑ –≥–æ—Ä—è—á–∏—Ö/—Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª
  hot_cold = pattern_analyzer.GLOBAL_PATTERN_ANALYZER.analyze_hot_cold_numbers(
    df_history, window_sizes=[20], top_n=5
  )

  # –ü–æ–¥—Å—á–µ—Ç –≥–æ—Ä—è—á–∏—Ö –∏ —Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª –≤ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
  hot_count = 0
  cold_count = 0

  for field_data in hot_cold.values():
    field_num = field_data['field']
    hot_nums = [num for num, _ in field_data.get('hot_numbers', [])]
    cold_nums = [num for num, _ in field_data.get('cold_numbers', [])]

    field_combo = request.field1 if field_num == 1 else request.field2

    hot_count += len(set(field_combo) & set(hot_nums))
    cold_count += len(set(field_combo) & set(cold_nums))

  # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Å—á–µ—Ç–∞
  if hot_count > 0:
    score += hot_count * 10
    factors.append(f"‚úÖ –°–æ–¥–µ—Ä–∂–∏—Ç {hot_count} –≥–æ—Ä—è—á–∏—Ö —á–∏—Å–µ–ª (+{hot_count * 10} –æ—á–∫–æ–≤)")

  if cold_count > 2:
    score -= cold_count * 5
    factors.append(f"‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª (-{cold_count * 5} –æ—á–∫–æ–≤)")

  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
  def count_sequences(numbers):
    sorted_nums = sorted(numbers)
    sequences = 0
    for i in range(1, len(sorted_nums)):
      if sorted_nums[i] == sorted_nums[i - 1] + 1:
        sequences += 1
    return sequences

  seq1 = count_sequences(request.field1)
  seq2 = count_sequences(request.field2)

  if seq1 + seq2 > 3:
    score -= 15
    factors.append("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —á–∏—Å–µ–ª (-15 –æ—á–∫–æ–≤)")
  elif seq1 + seq2 <= 1:
    score += 10
    factors.append("‚úÖ –•–æ—Ä–æ—à–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–µ–ª (+10 –æ—á–∫–æ–≤)")

  # –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞
  score = max(0, min(100, score))

  # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
  if score >= 80:
    recommendation = "–û—Ç–ª–∏—á–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è! –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∫ –∏–≥—Ä–µ"
  elif score >= 60:
    recommendation = "–•–æ—Ä–æ—à–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º"
  elif score >= 40:
    recommendation = "–°—Ä–µ–¥–Ω—è—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è, –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å"
  else:
    recommendation = "–°–ª–∞–±–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å"

  return {
    "score": score,
    "recommendation": recommendation,
    "combination": {
      "field1": request.field1,
      "field2": request.field2
    },
    "factors": factors
  }