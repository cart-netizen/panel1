"""
Q-Learning –∞–≥–µ–Ω—Ç –¥–ª—è –ª–æ—Ç–µ—Ä–µ–∏
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Q-—Ç–∞–±–ª–∏—Ü—É —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict, deque
import pickle
import json
import logging
from datetime import datetime
import random

from backend.app.core.rl.environment import LotteryEnvironment, LotteryState
from backend.app.core.rl.state_encoder import StateEncoder, ActionEncoder
from backend.app.core.rl.reward_calculator import RewardCalculator

logger = logging.getLogger(__name__)


class QLearningAgent:
    """
    Q-Learning –∞–≥–µ–Ω—Ç –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤—ã–±–æ—Ä–∞ –ª–æ—Ç–µ—Ä–µ–π–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
    """
    
    def __init__(self, 
                 lottery_config: Dict,
                 learning_rate: float = 0.1,
                 discount_factor: float = 0.95,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 memory_limit: int = 100000):
        """
        Args:
            lottery_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ—Ç–µ—Ä–µ–∏
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (alpha)
            discount_factor: –§–∞–∫—Ç–æ—Ä –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (gamma)
            epsilon: –ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è epsilon-greedy
            epsilon_decay: –°–∫–æ—Ä–æ—Å—Ç—å –∑–∞—Ç—É—Ö–∞–Ω–∏—è epsilon
            epsilon_min: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ epsilon
            memory_limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä Q-—Ç–∞–±–ª–∏—Ü—ã
        """
        self.lottery_config = lottery_config
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.memory_limit = memory_limit
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤
        feature_dims = {
            'universe_length': 100,
            'parity_ratio': 1,
            'mean_gap': 50,
            'mean_frequency': 10,
            'hot_numbers_count': 20,
            'cold_numbers_count': 20,
            'sum_trend': 100,
            'diversity_index': 1,
            'days_since_jackpot': 365,
            'draw_number': 1000
        }
        self.state_encoder = StateEncoder(feature_dims)
        self.action_encoder = ActionEncoder(lottery_config)
        
        # Q-—Ç–∞–±–ª–∏—Ü–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.state_visits = defaultdict(int)
        self.action_visits = defaultdict(lambda: defaultdict(int))
        
        # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.episode_rewards = []
        self.learning_history = []
        self.best_actions = deque(maxlen=100)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_episodes = 0
        self.total_steps = 0
        self.total_reward = 0
        self.wins = 0
        
        # –ö—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        self.action_cache = {}
        self.state_cache = {}
        
        logger.info(f"‚úÖ Q-Learning –∞–≥–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: Œ±={learning_rate}, Œ≥={discount_factor}, Œµ={epsilon}")
    
    def get_state_key(self, state: LotteryState) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è Q-—Ç–∞–±–ª–∏—Ü—ã"""
        state_dict = state.to_dict()
        return self.state_encoder.encode_discrete(state_dict)
    
    def choose_action(self, state: LotteryState, training: bool = True) -> Tuple[List[int], List[int]]:
        """
        –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è –ø–æ epsilon-greedy —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        
        Args:
            state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            training: –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
        
        Returns:
            –í—ã–±—Ä–∞–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (field1, field2)
        """
        state_key = self.get_state_key(state)
        
        # Epsilon-greedy —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
        if training and random.random() < self.epsilon:
            # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            action = self.action_encoder.sample_random_action()
            logger.debug(f"üé≤ –°–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (Œµ={self.epsilon:.3f})")
        else:
            # –≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è: –ª—É—á—à–µ–µ –∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            action = self._get_best_action(state_key)
            if action is None:
                # –ï—Å–ª–∏ –Ω–µ—Ç –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π, –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ
                action = self.action_encoder.sample_random_action()
                logger.debug("üÜï –ù–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ")
            else:
                logger.debug(f"üéØ –õ—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–µ—â–µ–Ω–∏–π
        self.state_visits[state_key] += 1
        action_key = self.action_encoder.encode(*action)
        self.action_visits[state_key][action_key] += 1
        
        return action
    
    def _get_best_action(self, state_key: str) -> Optional[Tuple[List[int], List[int]]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
        
        Args:
            state_key: –ö–ª—é—á —Å–æ—Å—Ç–æ—è–Ω–∏—è
        
        Returns:
            –õ—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –∏–ª–∏ None
        """
        if state_key not in self.q_table or not self.q_table[state_key]:
            return None
        
        # –ù–∞—Ö–æ–¥–∏–º –¥–µ–π—Å—Ç–≤–∏–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º Q-–∑–Ω–∞—á–µ–Ω–∏–µ–º
        best_action_key = max(self.q_table[state_key], 
                              key=lambda a: self.q_table[state_key][a])
        
        return self.action_encoder.decode(best_action_key)
    
    def update_q_value(self, 
                      state: LotteryState,
                      action: Tuple[List[int], List[int]],
                      reward: float,
                      next_state: Optional[LotteryState],
                      done: bool):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —Ñ–æ—Ä–º—É–ª–µ –ë–µ–ª–ª–º–∞–Ω–∞
        
        Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max_a'Q(s',a') - Q(s,a)]
        
        Args:
            state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            action: –í—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            reward: –ü–æ–ª—É—á–µ–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞
            next_state: –°–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            done: –ü—Ä–∏–∑–Ω–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç–ø–∏–∑–æ–¥–∞
        """
        state_key = self.get_state_key(state)
        action_key = self.action_encoder.encode(*action)
        
        # –¢–µ–∫—É—â–µ–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ
        current_q = self.q_table[state_key][action_key]
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        if done or next_state is None:
            target_q = reward
        else:
            next_state_key = self.get_state_key(next_state)
            max_next_q = max(self.q_table[next_state_key].values()) if self.q_table[next_state_key] else 0
            target_q = reward + self.discount_factor * max_next_q
        
        # –û–±–Ω–æ–≤–ª—è–µ–º Q-–∑–Ω–∞—á–µ–Ω–∏–µ
        new_q = current_q + self.learning_rate * (target_q - current_q)
        self.q_table[state_key][action_key] = new_q
        
        logger.debug(f"üìä Q –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {current_q:.3f} ‚Üí {new_q:.3f} (–Ω–∞–≥—Ä–∞–¥–∞={reward:.2f})")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö–æ—Ä–æ—à–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
        if reward > 0:
            self.best_actions.append((state_key, action_key, reward))
    
    def train_episode(self, env: LotteryEnvironment, max_steps: int = 100) -> Dict:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º —ç–ø–∏–∑–æ–¥–µ
        
        Args:
            env: –°—Ä–µ–¥–∞ –ª–æ—Ç–µ—Ä–µ–∏
            max_steps: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–∏–∑–æ–¥–∞
        """
        state = env.reset()
        episode_reward = 0
        steps = 0
        wins_episode = 0
        
        for step in range(max_steps):
            # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            action = self.choose_action(state, training=True)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            next_state, reward, done, info = env.step(action)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º Q-–∑–Ω–∞—á–µ–Ω–∏–µ
            self.update_q_value(state, action, reward, next_state, done)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            episode_reward += reward
            if reward > 0:
                wins_episode += 1
            
            steps += 1
            state = next_state
            
            if done:
                break
        
        # –ó–∞—Ç—É—Ö–∞–Ω–∏–µ epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.total_episodes += 1
        self.total_steps += steps
        self.total_reward += episode_reward
        # if wins_episode > 0:
        #     self.wins += 1
        self.wins += wins_episode

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        episode_stats = {
            'episode': self.total_episodes,
            'steps': steps,
            'reward': episode_reward,
            'wins': wins_episode,
            'epsilon': self.epsilon,
            'q_table_size': self._get_q_table_size()
        }
        
        self.episode_rewards.append(episode_reward)
        self.learning_history.append(episode_stats)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
        if self._get_q_table_size() > self.memory_limit:
            self._optimize_memory()
        
        return episode_stats
    
    def train(self, 
              df_history: pd.DataFrame,
              num_episodes: int = 1000,
              window_size: int = 50,
              verbose: bool = True) -> Dict:
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            df_history: –ò—Å—Ç–æ—Ä–∏—è —Ç–∏—Ä–∞–∂–µ–π
            num_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤
            window_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            verbose: –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
        Returns:
            –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        """
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è Q-–∞–≥–µ–Ω—Ç–∞: {num_episodes} —ç–ø–∏–∑–æ–¥–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É
        env = LotteryEnvironment(df_history, self.lottery_config, window_size)
        
        # –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –Ω–∞–≥—Ä–∞–¥
        # reward_calc = RewardCalculator(self.lottery_config)
        
        best_episode_reward = -float('inf')
        best_episode = None
        
        for episode in range(num_episodes):
            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —ç–ø–∏–∑–æ–¥–µ
            stats = self.train_episode(env)
            
            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –ª—É—á—à–∏–π —ç–ø–∏–∑–æ–¥
            if stats['reward'] > best_episode_reward:
                best_episode_reward = stats['reward']
                best_episode = stats['episode']
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if verbose and (episode + 1) % 100 == 0:
                avg_reward = np.mean(self.episode_rewards[-100:])
                win_rate = (self.wins / max(self.total_steps, 1)) * 100
                # Win rate —Å—á–∏—Ç–∞–µ–º –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤, –∞ –Ω–µ —ç–ø–∏–∑–æ–¥–æ–≤
                # total_games = sum(len(episode) for episode in episode_history) if hasattr(self,
                #                                                                           'episode_history') else self.total_steps
                # win_rate = (self.wins / max(total_games, 1)) * 100 if total_games > 0 else 0
                logger.info(f"üìà –≠–ø–∏–∑–æ–¥ {episode + 1}/{num_episodes}: "
                          f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞={avg_reward:.2f}, "
                          f"Win rate={win_rate:.1f}%, "
                          f"Œµ={self.epsilon:.3f}, "
                          f"Q-—Ä–∞–∑–º–µ—Ä={self._get_q_table_size()}")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        final_stats = {
            'total_episodes': self.total_episodes,
            'total_steps': self.total_steps,
            'total_reward': self.total_reward,
            'average_reward': self.total_reward / self.total_episodes,
            'win_rate': (self.wins / max(self.total_steps, 1)) * 100,
            'best_episode': best_episode,
            'best_reward': best_episode_reward,
            'final_epsilon': self.epsilon,
            'q_table_size': self._get_q_table_size(),
            'unique_states': len(self.q_table),
            'unique_actions': sum(len(actions) for actions in self.q_table.values())
        }
        
        logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {final_stats['average_reward']:.2f}")
        logger.info(f"   Win rate: {final_stats['win_rate']:.1f}%")
        logger.info(f"   –õ—É—á—à–∏–π —ç–ø–∏–∑–æ–¥: {final_stats['best_episode']} (–Ω–∞–≥—Ä–∞–¥–∞={final_stats['best_reward']:.2f})")
        logger.info(f"   –†–∞–∑–º–µ—Ä Q-—Ç–∞–±–ª–∏—Ü—ã: {final_stats['q_table_size']} –∑–∞–ø–∏—Å–µ–π")
        
        return final_stats
    
    def predict(self, state: LotteryState) -> Tuple[List[int], List[int]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ª—É—á—à–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è (—Ä–µ–∂–∏–º —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏)
        
        Args:
            state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        
        Returns:
            –õ—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        """
        return self.choose_action(state, training=False)
    
    def _get_q_table_size(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ Q-—Ç–∞–±–ª–∏—Ü—ã"""
        return sum(len(actions) for actions in self.q_table.values())
    
    def _optimize_memory(self):
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –ø—É—Ç–µ–º —É–¥–∞–ª–µ–Ω–∏—è —Ä–µ–¥–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        logger.info(f"üßπ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ Q-—Ç–∞–±–ª–∏—Ü—ã (—Ç–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä: {self._get_q_table_size()})")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ —á–∞—Å—Ç–æ—Ç–µ –ø–æ—Å–µ—â–µ–Ω–∏–π
        states_by_visits = sorted(self.state_visits.items(), key=lambda x: x[1])
        
        # –£–¥–∞–ª—è–µ–º 20% –Ω–∞–∏–º–µ–Ω–µ–µ –ø–æ—Å–µ—â–∞–µ–º—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
        num_to_remove = len(states_by_visits) // 5
        
        for state_key, _ in states_by_visits[:num_to_remove]:
            if state_key in self.q_table:
                del self.q_table[state_key]
            if state_key in self.state_visits:
                del self.state_visits[state_key]
            if state_key in self.action_visits:
                del self.action_visits[state_key]
        
        logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {num_to_remove} —Å–æ—Å—Ç–æ—è–Ω–∏–π, –Ω–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {self._get_q_table_size()}")
    
    def save(self, filepath: str):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –≤ —Ñ–∞–π–ª
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        """
        save_data = {
            'q_table': dict(self.q_table),
            'state_visits': dict(self.state_visits),
            'action_visits': dict(self.action_visits),
            'epsilon': self.epsilon,
            'total_episodes': self.total_episodes,
            'total_steps': self.total_steps,
            'total_reward': self.total_reward,
            'wins': self.wins,
            'learning_history': self.learning_history,
            'best_actions': list(self.best_actions),
            'config': {
                'learning_rate': self.learning_rate,
                'discount_factor': self.discount_factor,
                'epsilon_decay': self.epsilon_decay,
                'epsilon_min': self.epsilon_min
            }
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(save_data, f)
        
        logger.info(f"üíæ –ê–≥–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filepath}")
    
    def load(self, filepath: str):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∞–≥–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        """
        with open(filepath, 'rb') as f:
            save_data = pickle.load(f)
        
        self.q_table = defaultdict(lambda: defaultdict(float), save_data['q_table'])
        self.state_visits = defaultdict(int, save_data['state_visits'])
        self.action_visits = defaultdict(lambda: defaultdict(int), save_data['action_visits'])
        self.epsilon = save_data['epsilon']
        self.total_episodes = save_data['total_episodes']
        self.total_steps = save_data['total_steps']
        self.total_reward = save_data['total_reward']
        self.wins = save_data['wins']
        self.learning_history = save_data['learning_history']
        self.best_actions = deque(save_data['best_actions'], maxlen=100)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = save_data['config']
        self.learning_rate = config['learning_rate']
        self.discount_factor = config['discount_factor']
        self.epsilon_decay = config['epsilon_decay']
        self.epsilon_min = config['epsilon_min']
        
        logger.info(f"‚úÖ –ê–≥–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {filepath}")
        logger.info(f"   –≠–ø–∏–∑–æ–¥–æ–≤: {self.total_episodes}, Q-—Ä–∞–∑–º–µ—Ä: {self._get_q_table_size()}")
    
    def get_best_combinations(self, state: LotteryState, top_k: int = 5) -> List[Tuple[Tuple[List[int], List[int]], float]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø-K –ª—É—á—à–∏—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
        
        Args:
            state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è, Q-–∑–Ω–∞—á–µ–Ω–∏–µ)
        """
        state_key = self.get_state_key(state)
        
        if state_key not in self.q_table:
            # –ï—Å–ª–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–æ–≤–æ–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
            return [(self.action_encoder.sample_random_action(), 0.0) for _ in range(top_k)]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏—è –ø–æ Q-–∑–Ω–∞—á–µ–Ω–∏—é
        sorted_actions = sorted(self.q_table[state_key].items(), 
                              key=lambda x: x[1], 
                              reverse=True)
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K
        result = []
        for action_key, q_value in sorted_actions[:top_k]:
            action = self.action_encoder.decode(action_key)
            result.append((action, q_value))
        
        # –ï—Å–ª–∏ –º–µ–Ω—å—à–µ top_k, –¥–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ
        while len(result) < top_k:
            result.append((self.action_encoder.sample_random_action(), 0.0))
        
        return result