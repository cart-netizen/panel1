"""
–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≥—Ä–∞–¥ –¥–ª—è RL –∞–≥–µ–Ω—Ç–æ–≤
–í–∫–ª—é—á–∞–µ—Ç:
1. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
2. –°–∏—Å—Ç–µ–º—É –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö
3. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ª–æ—Ç–µ—Ä–µ–π
4. –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""

import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
from math import comb
import json
import os

logger = logging.getLogger(__name__)


@dataclass
class ValidationMetrics:
  """–ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
  win_rate: float
  average_reward: float
  roi: float  # Return on Investment
  sharpe_ratio: float  # Risk-adjusted return
  max_drawdown: float
  total_games: int
  profitable_games: int
  biggest_win: float
  biggest_loss: float


class RealisticRewardCalculator:
  """
  –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞—Å—á–µ—Ç–∞ –Ω–∞–≥—Ä–∞–¥
  –û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–æ–≤—ã—Ö —Å—Ö–µ–º–∞—Ö –ª–æ—Ç–µ—Ä–µ–π
  """

  def __init__(self, lottery_config: Dict):
    self.field1_size = lottery_config['field1_size']
    self.field2_size = lottery_config['field2_size']
    self.field1_max = lottery_config['field1_max']
    self.field2_max = lottery_config['field2_max']

    # –°—Ç–æ–∏–º–æ—Å—Ç—å –±–∏–ª–µ—Ç–∞
    self.ticket_cost = 10.0  # –ë–∞–∑–æ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤ —É—Å–ª–æ–≤–Ω—ã—Ö –µ–¥–∏–Ω–∏—Ü–∞—Ö

    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥
    self._compute_probabilities()
    self._setup_prize_structure()

    logger.info(f"‚úÖ RealisticRewardCalculator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    logger.info(f"   –õ–æ—Ç–µ—Ä–µ—è: {self.field1_size}x{self.field1_max} + {self.field2_size}x{self.field2_max}")
    logger.info(f"   –°—Ç–æ–∏–º–æ—Å—Ç—å –±–∏–ª–µ—Ç–∞: {self.ticket_cost}")

  def _compute_probabilities(self):
    """–í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª—è 1
    self.field1_probabilities = {}
    total_combinations = comb(self.field1_max, self.field1_size)

    for matches in range(self.field1_size + 1):
      if matches <= self.field1_size:
        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É–≥–∞–¥–∞—Ç—å exactly 'matches' —á–∏—Å–µ–ª
        prob = (comb(self.field1_size, matches) *
                comb(self.field1_max - self.field1_size, self.field1_size - matches)) / total_combinations
        self.field1_probabilities[matches] = prob

    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª—è 2 (–ø—Ä–æ—â–µ, —Ç–∞–∫ –∫–∞–∫ –æ–±—ã—á–Ω–æ –º–µ–Ω—å—à–µ —á–∏—Å–µ–ª)
    self.field2_probabilities = {}
    if self.field2_size == 1:
      # –î–ª—è –æ–¥–Ω–æ–≥–æ —á–∏—Å–ª–∞
      self.field2_probabilities[0] = (self.field2_max - 1) / self.field2_max
      self.field2_probabilities[1] = 1 / self.field2_max
    else:
      # –î–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
      total_combinations_f2 = comb(self.field2_max, self.field2_size)
      for matches in range(self.field2_size + 1):
        prob = (comb(self.field2_size, matches) *
                comb(self.field2_max - self.field2_size, self.field2_size - matches)) / total_combinations_f2
        self.field2_probabilities[matches] = prob

  def _setup_prize_structure(self):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –ø—Ä–∏–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É"""
    # –û—Å–Ω–æ–≤—ã–≤–∞–µ–º –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –ª–æ—Ç–µ—Ä–µ—è—Ö —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –º–∞—Ç–æ–∂–∏–¥–∞–Ω–∏–µ–º

    if self.field1_size == 4:  # 4x20 –ª–æ—Ç–µ—Ä–µ—è
      self.prize_structure = {
        (4, 4): 3333333,  # –î–∂–µ–∫–ø–æ—Ç: –≤—Å–µ 4 + –±–æ–Ω—É—Å
        (4, 3): 2300,  # –î–∂–µ–∫–ø–æ—Ç: –≤—Å–µ 4 + –±–æ–Ω—É—Å
        (4, 2): 650,  # –î–∂–µ–∫–ø–æ—Ç: –≤—Å–µ 4 + –±–æ–Ω—É—Å
        (4, 1): 330,  # –î–∂–µ–∫–ø–æ—Ç: –≤—Å–µ 4 + –±–æ–Ω—É—Å
        (4, 0): 1400,  # –í—Å–µ 4 –±–µ–∑ –±–æ–Ω—É—Å–∞
        (3, 3): 700,  # 3 + –±–æ–Ω—É—Å
        (3, 2): 70,  # 3 + –±–æ–Ω—É—Å
        (3, 1): 30,  # 3 + –±–æ–Ω—É—Å
        (3, 0): 60,  # 3 –±–µ–∑ –±–æ–Ω—É—Å–∞
        (2, 2): 20,  # 3 –±–µ–∑ –±–æ–Ω—É—Å–∞

        (2, 1): 10,  # 2 + –±–æ–Ω—É—Å
        (2, 0): 10,  # 2 –±–µ–∑ –±–æ–Ω—É—Å–∞ (–Ω–µ –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç)
      }
    elif self.field1_size == 5:  # 5x36 –ª–æ—Ç–µ—Ä–µ—è (—Å–ª–æ–∂–Ω–µ–µ)
      self.prize_structure = {
        (5, 1): 290000,  # –°—É–ø–µ—Ä–¥–∂–µ–∫–ø–æ—Ç
        (5, 0): 145000,  # –î–∂–µ–∫–ø–æ—Ç
        # (4, 1): 10000,  # 4 + –±–æ–Ω—É—Å
        (4, 0): 1000,  # 4 –±–µ–∑ –±–æ–Ω—É—Å–∞
        # (3, 1): 100,  # 3 + –±–æ–Ω—É—Å
        (3, 0): 100,  # 3 –±–µ–∑ –±–æ–Ω—É—Å–∞
        # (2, 1): 5,  # 2 + –±–æ–Ω—É—Å
        (2, 0): 10,  # –ù–µ –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç
      }
    else:
      # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å—Ö–µ–º–∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
      max_prize = 100000
      self.prize_structure = {}
      for f1_matches in range(self.field1_size + 1):
        for f2_matches in range(self.field2_size + 1):
          if f1_matches >= 2:  # –ú–∏–Ω–∏–º—É–º –¥–ª—è –≤—ã–∏–≥—Ä—ã—à–∞
            multiplier = (f1_matches ** 3) * (1 + f2_matches)
            prize = min(max_prize, int(self.ticket_cost * multiplier))
            self.prize_structure[(f1_matches, f2_matches)] = prize
          else:
            self.prize_structure[(f1_matches, f2_matches)] = 0

  def calculate_reward(self, pred_field1: List[int], pred_field2: List[int],
                       actual_field1: List[int], actual_field2: List[int]) -> float:
    """
    –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —Ä–∞—Å—á–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã

    Returns:
        –ù–∞–≥—Ä–∞–¥–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π)
    """
    # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—É—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if not actual_field1 or not actual_field2:
      return -self.ticket_cost

    # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    matches_f1 = len(set(pred_field1) & set(actual_field1))
    matches_f2 = len(set(pred_field2) & set(actual_field2))

    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–∑ –ø–æ —Ç–∞–±–ª–∏—Ü–µ
    prize = self.prize_structure.get((matches_f1, matches_f2), 0)

    # –ò—Ç–æ–≥–æ–≤–∞—è –Ω–∞–≥—Ä–∞–¥–∞ = –ø—Ä–∏–∑ - —Å—Ç–æ–∏–º–æ—Å—Ç—å –±–∏–ª–µ—Ç–∞
    reward = prize - self.ticket_cost

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π
    if prize > self.ticket_cost * 10:
      logger.debug(f"üéâ –ö—Ä—É–ø–Ω—ã–π –≤—ã–∏–≥—Ä—ã—à: {matches_f1}+{matches_f2} = {prize} (–Ω–∞–≥—Ä–∞–¥–∞: {reward})")

    return reward

  def get_expected_value(self) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º)"""
    expected_value = -self.ticket_cost  # –ù–∞—á–∏–Ω–∞–µ–º —Å —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –±–∏–ª–µ—Ç–∞

    for (f1_matches, f2_matches), prize in self.prize_structure.items():
      prob_f1 = self.field1_probabilities.get(f1_matches, 0)
      prob_f2 = self.field2_probabilities.get(f2_matches, 0)
      combined_prob = prob_f1 * prob_f2

      expected_value += prize * combined_prob

    return expected_value

  def get_statistics(self) -> Dict[str, Any]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø—Ä–∏–∑–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ"""
    ev = self.get_expected_value()
    rtp = (ev + self.ticket_cost) / self.ticket_cost  # Return to Player

    return {
      'expected_value': ev,
      'return_to_player': rtp,
      'house_edge': 1 - rtp,
      'ticket_cost': self.ticket_cost,
      'max_prize': max(self.prize_structure.values()),
      'prize_levels': len([p for p in self.prize_structure.values() if p > 0])
    }


class TrainValidationSplitter:
  """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ/–≤–∞–ª–∏–¥–∞—Ü–∏—é/—Ç–µ—Å—Ç"""

  @staticmethod
  def split_data(df: pd.DataFrame, train_ratio: float = 0.7,
                 val_ratio: float = 0.15, test_ratio: float = 0.15) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    –†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞

    Args:
        df: –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏)
        train_ratio: –î–æ–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        val_ratio: –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_ratio: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

    Returns:
        (train_df, val_df, test_df)
    """
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "–°—É–º–º–∞ –¥–æ–ª–µ–π –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1.0"

    n = len(df)
    train_end = int(n * train_ratio)
    val_end = int(n * (train_ratio + val_ratio))

    # –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ (—Å–≤–µ–∂–∏–µ - –¥–ª—è —Ç–µ—Å—Ç–∞)
    train_df = df.iloc[:train_end].copy()
    val_df = df.iloc[train_end:val_end].copy()
    test_df = df.iloc[val_end:].copy()

    logger.info(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
    logger.info(f"   –û–±—É—á–µ–Ω–∏–µ: {len(train_df)} —Ç–∏—Ä–∞–∂–µ–π ({train_ratio:.1%})")
    logger.info(f"   –í–∞–ª–∏–¥–∞—Ü–∏—è: {len(val_df)} —Ç–∏—Ä–∞–∂–µ–π ({val_ratio:.1%})")
    logger.info(f"   –¢–µ—Å—Ç: {len(test_df)} —Ç–∏—Ä–∞–∂–µ–π ({test_ratio:.1%})")

    return train_df, val_df, test_df


class PerformanceValidator:
  """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤"""

  def __init__(self, reward_calculator: RealisticRewardCalculator):
    self.reward_calculator = reward_calculator

  def validate_agent(self, agent, val_df: pd.DataFrame, lottery_config: Dict,
                     num_episodes: int = 100) -> ValidationMetrics:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

    Args:
        agent: –û–±—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç (Q-Learning –∏–ª–∏ DQN)
        val_df: –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        lottery_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ—Ç–µ—Ä–µ–∏
        num_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏

    Returns:
        –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    from backend.app.core.rl.environment import LotteryEnvironment

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    env = LotteryEnvironment(val_df, lottery_config)

    rewards = []
    wins = 0
    total_invested = 0
    total_won = 0

    for episode in range(num_episodes):
      state = env.reset()
      episode_reward = 0
      episode_invested = 0
      episode_won = 0

      done = False
      while not done:
        # –ü–æ–ª—É—á–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –æ—Ç –∞–≥–µ–Ω—Ç–∞ (–±–µ–∑ –æ–±—É—á–µ–Ω–∏—è)
        if hasattr(agent, 'choose_action'):
          action = agent.choose_action(state, training=False)
        else:
          action = agent.predict(state)

        next_state, reward, done, info = env.step(action)

        episode_reward += reward
        episode_invested += self.reward_calculator.ticket_cost

        if reward > 0:
          episode_won += reward + self.reward_calculator.ticket_cost
          wins += 1

        state = next_state

      rewards.append(episode_reward)
      total_invested += episode_invested
      total_won += episode_won

    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    avg_reward = np.mean(rewards)
    roi = ((total_won - total_invested) / total_invested * 100) if total_invested > 0 else -100

    # Sharpe ratio (—Ä–∏—Å–∫-—Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å)
    reward_std = np.std(rewards) if len(rewards) > 1 else 1
    sharpe_ratio = avg_reward / reward_std if reward_std > 0 else 0

    # Maximum drawdown
    cumulative_rewards = np.cumsum(rewards)
    running_max = np.maximum.accumulate(cumulative_rewards)
    drawdown = (cumulative_rewards - running_max)
    max_drawdown = np.min(drawdown)

    return ValidationMetrics(
      win_rate=wins / (num_episodes * len(val_df)) * 100 if num_episodes > 0 else 0,
      average_reward=avg_reward,
      roi=roi,
      sharpe_ratio=sharpe_ratio,
      max_drawdown=max_drawdown,
      total_games=num_episodes * len(val_df),
      profitable_games=wins,
      biggest_win=np.max(rewards) if rewards else 0,
      biggest_loss=np.min(rewards) if rewards else 0
    )


class AdaptiveHyperparameters:
  """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ª–æ—Ç–µ—Ä–µ–π"""

  @staticmethod
  def get_config(lottery_type: str, lottery_config: Dict) -> Dict[str, Any]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ª–æ—Ç–µ—Ä–µ–∏

    Args:
        lottery_type: –¢–∏–ø –ª–æ—Ç–µ—Ä–µ–∏ (4x20, 5x36plus, etc.)
        lottery_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ—Ç–µ—Ä–µ–∏

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    field1_size = lottery_config['field1_size']
    field1_max = lottery_config['field1_max']
    complexity = field1_size * field1_max  # –ú–µ—Ä–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏

    if complexity <= 80:  # –ü—Ä–æ—Å—Ç—ã–µ –ª–æ—Ç–µ—Ä–µ–∏ (—Ç–∏–ø–∞ 4x20)
      return {
        'q_learning': {
          'learning_rate': 0.1,
          'discount_factor': 0.95,
          'epsilon_start': 1.0,
          'epsilon_end': 0.01,
          'epsilon_decay': 0.995,
          'episodes': 1500,
          'memory_limit': 50000
        },
        'dqn': {
          'learning_rate': 0.001,
          'discount_factor': 0.99,
          'epsilon_start': 1.0,
          'epsilon_end': 0.05,
          'epsilon_decay': 0.995,
          'episodes': 800,
          'batch_size': 32,
          'memory_size': 10000,
          'target_update_freq': 100,
          'hidden_size': 256
        }
      }

    elif complexity <= 180:  # –°—Ä–µ–¥–Ω–∏–µ –ª–æ—Ç–µ—Ä–µ–∏ (—Ç–∏–ø–∞ 5x36)
      return {
        'q_learning': {
          'learning_rate': 0.05,  # –ú–µ–Ω—å—à–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
          'discount_factor': 0.99,
          'epsilon_start': 1.0,
          'epsilon_end': 0.02,
          'epsilon_decay': 0.998,
          'episodes': 2000,
          'memory_limit': 100000
        },
        'dqn': {
          'learning_rate': 0.0005,  # –ú–µ–Ω—å—à–µ learning rate
          'discount_factor': 0.99,
          'epsilon_start': 1.0,
          'epsilon_end': 0.08,
          'epsilon_decay': 0.998,
          'episodes': 1200,  # –ë–æ–ª—å—à–µ —ç–ø–∏–∑–æ–¥–æ–≤
          'batch_size': 64,  # –ë–æ–ª—å—à–∏–π –±–∞—Ç—á
          'memory_size': 15000,
          'target_update_freq': 200,
          'hidden_size': 512  # –ë–æ–ª—å—à–µ –Ω–µ–π—Ä–æ–Ω–æ–≤
        }
      }

    else:  # –°–ª–æ–∂–Ω—ã–µ –ª–æ—Ç–µ—Ä–µ–∏
      return {
        'q_learning': {
          'learning_rate': 0.01,
          'discount_factor': 0.99,
          'epsilon_start': 1.0,
          'epsilon_end': 0.05,
          'epsilon_decay': 0.999,
          'episodes': 3000,
          'memory_limit': 200000
        },
        'dqn': {
          'learning_rate': 0.0001,
          'discount_factor': 0.99,
          'epsilon_start': 1.0,
          'epsilon_end': 0.1,
          'epsilon_decay': 0.999,
          'episodes': 2000,
          'batch_size': 128,
          'memory_size': 25000,
          'target_update_freq': 300,
          'hidden_size': 1024
        }
      }

  @staticmethod
  def save_config(lottery_type: str, config: Dict, filepath: str):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª"""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)

    with open(filepath, 'w') as f:
      json.dump({
        'lottery_type': lottery_type,
        'config': config,
        'created_at': datetime.now().isoformat()
      }, f, indent=2)

    logger.info(f"üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")


# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ LotteryEnvironment
def _calculate_reward_realistic(self, pred_field1: List[int], pred_field2: List[int],
                                actual_field1: List[int], actual_field2: List[int]) -> float:
  """
  –ó–ê–ú–ï–ù–ê –¥–ª—è –º–µ—Ç–æ–¥–∞ _calculate_reward –≤ LotteryEnvironment

  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞–≥—Ä–∞–¥
  """
  if not hasattr(self, '_reward_calculator'):
    self._reward_calculator = RealisticRewardCalculator(self.lottery_config)

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    stats = self._reward_calculator.get_statistics()
    logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞–≥—Ä–∞–¥:")
    logger.info(f"   –ú–∞—Ç–æ–∂–∏–¥–∞–Ω–∏–µ: {stats['expected_value']:.2f}")
    logger.info(f"   RTP: {stats['return_to_player']:.1%}")
    logger.info(f"   House Edge: {stats['house_edge']:.1%}")
    logger.info(f"   –ú–∞–∫—Å. –ø—Ä–∏–∑: {stats['max_prize']:,.0f}")

  return self._reward_calculator.calculate_reward(pred_field1, pred_field2, actual_field1, actual_field2)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
def validate_trained_models(lottery_type: str, lottery_config: Dict, df_full: pd.DataFrame):
  """
  –ü—Ä–∏–º–µ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
  """
  # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
  train_df, val_df, test_df = TrainValidationSplitter.split_data(df_full)

  # –°–æ–∑–¥–∞–µ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä
  reward_calc = RealisticRewardCalculator(lottery_config)
  validator = PerformanceValidator(reward_calc)

  # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
  from backend.app.core.rl.rl_generator import GLOBAL_RL_MANAGER
  generator = GLOBAL_RL_MANAGER.get_generator(lottery_type, lottery_config)

  if generator.q_trained:
    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º Q-Learning
    q_metrics = validator.validate_agent(generator.q_agent, val_df, lottery_config)

    logger.info(f"üìà Q-Learning –≤–∞–ª–∏–¥–∞—Ü–∏—è:")
    logger.info(f"   Win rate: {q_metrics.win_rate:.2f}%")
    logger.info(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {q_metrics.average_reward:.2f}")
    logger.info(f"   ROI: {q_metrics.roi:.2f}%")
    logger.info(f"   Sharpe ratio: {q_metrics.sharpe_ratio:.3f}")

  if generator.dqn_trained:
    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º DQN
    dqn_metrics = validator.validate_agent(generator.dqn_agent, val_df, lottery_config)

    logger.info(f"üß† DQN –≤–∞–ª–∏–¥–∞—Ü–∏—è:")
    logger.info(f"   Win rate: {dqn_metrics.win_rate:.2f}%")
    logger.info(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {dqn_metrics.average_reward:.2f}")
    logger.info(f"   ROI: {dqn_metrics.roi:.2f}%")
    logger.info(f"   Sharpe ratio: {dqn_metrics.sharpe_ratio:.3f}")