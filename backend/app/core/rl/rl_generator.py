"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è RL –∞–≥–µ–Ω—Ç–æ–≤ —Å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç Q-Learning –∏ DQN –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
from datetime import datetime
import os
import json

import torch

from backend.app.core.rl.environment import LotteryEnvironment, LotteryState
from backend.app.core.rl.q_agent import QLearningAgent
from backend.app.core.rl.dqn_agent import DQNAgent
from backend.app.core.rl.reward_calculator import RewardCalculator, ShapedRewardCalculator
from backend.app.core import data_manager
from backend.app.core.combination_generator import _analyze_hot_cold_numbers_for_generator
from backend.app.core.lottery_context import LotteryContext
logger = logging.getLogger(__name__)


class RLGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Reinforcement Learning
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ–º –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RL –∞–≥–µ–Ω—Ç–æ–≤
    """
    
    def __init__(self, lottery_config: Dict, use_gpu: bool = True):
        """
        Args:
            lottery_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ—Ç–µ—Ä–µ–∏
            use_gpu: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è DQN
        """
        self.lottery_config = lottery_config
        self.lottery_type = lottery_config.get('db_table', '').replace('draws_', '')
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤
        self.q_agent = QLearningAgent(lottery_config)
        
        # device = "cuda" if use_gpu else "cpu"
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        device = 'cpu'
        if use_gpu and torch.cuda.is_available():
            try:
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º CUDA
                test_tensor = torch.randn(1, 10).cuda()
                device = 'cuda'
                logger.info("‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
                device = 'cpu'
        else:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ")


        self.dqn_agent = DQNAgent(lottery_config, device=device)
        
        # –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –Ω–∞–≥—Ä–∞–¥
        # self.reward_calculator = ShapedRewardCalculator(lottery_config)
        
        # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        self.models_dir = f"backend/models/rl/{self.lottery_type}"
        os.makedirs(self.models_dir, exist_ok=True)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.generation_stats = {
            'total_generated': 0,
            'q_agent_used': 0,
            'dqn_agent_used': 0,
            'ensemble_used': 0,
            'average_confidence': 0
        }
        
        # –§–ª–∞–≥–∏ –æ–±—É—á–µ–Ω–Ω–æ—Å—Ç–∏
        self.q_trained = False
        self.dqn_trained = False
        
        logger.info(f"‚úÖ RLGenerator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è {self.lottery_type}")
    
    def train(self, 
              df_history: Optional[pd.DataFrame] = None,
              q_episodes: int = 1000,
              dqn_episodes: int = 500,
              window_size: int = 50,
              verbose: bool = True) -> Dict:
        """
        –û–±—É—á–µ–Ω–∏–µ –æ–±–æ–∏—Ö RL –∞–≥–µ–Ω—Ç–æ–≤
        
        Args:
            df_history: –ò—Å—Ç–æ—Ä–∏—è —Ç–∏—Ä–∞–∂–µ–π (–µ—Å–ª–∏ None, –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ –ë–î)
            q_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è Q-Learning
            dqn_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è DQN
            window_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            verbose: –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        """
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è RL –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è {self.lottery_type}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
        if df_history is None:
            df_history = data_manager.fetch_draws_from_db()
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_history)} —Ç–∏—Ä–∞–∂–µ–π –∏–∑ –ë–î")
        
        if len(df_history) < window_size + 10:
            logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(df_history)} < {window_size + 10}")
            return {'error': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö'}
        
        training_stats = {}
        
        # –û–±—É—á–µ–Ω–∏–µ Q-Learning –∞–≥–µ–Ω—Ç–∞
        logger.info(f"üìö –û–±—É—á–µ–Ω–∏–µ Q-Learning –∞–≥–µ–Ω—Ç–∞ ({q_episodes} —ç–ø–∏–∑–æ–¥–æ–≤)...")
        q_stats = self.q_agent.train(
            df_history=df_history,
            num_episodes=q_episodes,
            window_size=window_size,
            verbose=verbose
        )
        training_stats['q_learning'] = q_stats
        self.q_trained = True
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º Q-–∞–≥–µ–Ω—Ç–∞
        q_path = os.path.join(self.models_dir, "q_agent.pkl")
        self.q_agent.save(q_path)
        logger.info(f"üíæ Q-–∞–≥–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {q_path}")
        
        # –û–±—É—á–µ–Ω–∏–µ DQN –∞–≥–µ–Ω—Ç–∞
        logger.info(f"üß† –û–±—É—á–µ–Ω–∏–µ DQN –∞–≥–µ–Ω—Ç–∞ ({dqn_episodes} —ç–ø–∏–∑–æ–¥–æ–≤)...")
        dqn_stats = self.dqn_agent.train(
            df_history=df_history,
            num_episodes=dqn_episodes,
            window_size=window_size,
            verbose=verbose
        )
        training_stats['dqn'] = dqn_stats
        self.dqn_trained = True
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º DQN –∞–≥–µ–Ω—Ç–∞
        dqn_path = os.path.join(self.models_dir, "dqn_agent.pth")
        self.dqn_agent.save(dqn_path)
        logger.info(f"üíæ DQN –∞–≥–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {dqn_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—É—á–µ–Ω–∏—è
        stats_path = os.path.join(self.models_dir, "training_stats.json")
        with open(stats_path, 'w') as f:
            json.dump(training_stats, f, indent=2)
        
        logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ RL –∞–≥–µ–Ω—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        self._print_training_summary(training_stats)
        
        return training_stats
    
    def _print_training_summary(self, stats: Dict):
        """–í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("=" * 60)
        logger.info("üìä –ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø RL –ê–ì–ï–ù–¢–û–í")
        logger.info("=" * 60)
        
        if 'q_learning' in stats:
            q = stats['q_learning']
            logger.info(f"Q-Learning:")
            logger.info(f"  ‚Ä¢ –≠–ø–∏–∑–æ–¥–æ–≤: {q.get('total_episodes', 0)}")
            logger.info(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {q.get('average_reward', 0):.2f}")
            logger.info(f"  ‚Ä¢ Win rate: {q.get('win_rate', 0):.1f}%")
            logger.info(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä Q-—Ç–∞–±–ª–∏—Ü—ã: {q.get('q_table_size', 0)}")
        
        if 'dqn' in stats:
            dqn = stats['dqn']
            logger.info(f"DQN:")
            logger.info(f"  ‚Ä¢ –≠–ø–∏–∑–æ–¥–æ–≤: {dqn.get('total_episodes', 0)}")
            logger.info(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {dqn.get('average_reward', 0):.2f}")
            logger.info(f"  ‚Ä¢ Win rate: {dqn.get('win_rate', 0):.1f}%")
            logger.info(f"  ‚Ä¢ Final loss: {dqn.get('final_loss', 0):.4f}")
        
        logger.info("=" * 60)
    
    def generate_combinations(self,
                             count: int = 5,
                             df_history: Optional[pd.DataFrame] = None,
                             strategy: str = 'ensemble',
                             window_size: int = 50) -> List[Dict]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω—ã—Ö RL –∞–≥–µ–Ω—Ç–æ–≤
        
        Args:
            count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
            df_history: –ò—Å—Ç–æ—Ä–∏—è —Ç–∏—Ä–∞–∂–µ–π
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è ('q_learning', 'dqn', 'ensemble')
            window_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not self.q_trained and not self.dqn_trained:
            logger.warning("‚ö†Ô∏è RL –∞–≥–µ–Ω—Ç—ã –Ω–µ –æ–±—É—á–µ–Ω—ã! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ train() —Å–Ω–∞—á–∞–ª–∞.")
            return []
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
        if df_history is None:
            df_history = data_manager.fetch_draws_from_db()
        
        if len(df_history) < window_size:
            logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(df_history)} < {window_size}")
            return []
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        env = LotteryEnvironment(df_history, self.lottery_config, window_size)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤ –∏—Å—Ç–æ—Ä–∏–∏)
        current_state = env._compute_state(len(df_history) - 1)
        
        combinations = []
        
        for i in range(count):
            # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if strategy == 'q_learning' and self.q_trained:
                combo = self._generate_q_learning(current_state)
                self.generation_stats['q_agent_used'] += 1
            elif strategy == 'dqn' and self.dqn_trained:
                combo = self._generate_dqn(current_state)
                self.generation_stats['dqn_agent_used'] += 1
            elif strategy == 'ensemble':
                combo = self._generate_ensemble(current_state)
                self.generation_stats['ensemble_used'] += 1
            else:
                # Fallback –Ω–∞ —Å–ª—É—á–∞–π–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                combo = self._generate_random()
            
            self.generation_stats['total_generated'] += 1
            combinations.append(combo)
        
        logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(combinations)} –∫–æ–º–±–∏–Ω–∞—Ü–∏–π (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy})")
        
        return combinations
    
    def _generate_q_learning(self, state: LotteryState) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Q-Learning –∞–≥–µ–Ω—Ç–∞"""
        field1, field2 = self.q_agent.predict(state)
        
        # –ü–æ–ª—É—á–∞–µ–º Q-–∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        state_key = self.q_agent.get_state_key(state)
        action_key = self.q_agent.action_encoder.encode(field1, field2)
        
        q_value = 0
        if state_key in self.q_agent.q_table:
            q_value = self.q_agent.q_table[state_key].get(action_key, 0)
        
        return {
            'field1': sorted(field1),
            'field2': sorted(field2),
            'method': 'Q-Learning',
            'confidence': self._normalize_confidence(q_value, 'q'),
            'q_value': q_value,
            'state_features': state.to_dict()
        }
    
    def _generate_dqn(self, state: LotteryState) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é DQN –∞–≥–µ–Ω—Ç–∞"""
        field1, field2 = self.dqn_agent.predict(state)
        
        # –ü–æ–ª—É—á–∞–µ–º Q-–∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Å–µ—Ç–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        import torch
        with torch.no_grad():
            state_tensor = self.dqn_agent.state_to_tensor(state)
            q_values, _, _ = self.dqn_agent.q_network(state_tensor)
            max_q = q_values.max().item()
        
        return {
            'field1': sorted(field1),
            'field2': sorted(field2),
            'method': 'DQN',
            'confidence': self._normalize_confidence(max_q, 'dqn'),
            'q_value': max_q,
            'state_features': state.to_dict()
        }
    
    def _generate_ensemble(self, state: LotteryState) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–º –º–µ—Ç–æ–¥–æ–º"""
        combinations = []
        confidences = []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –æ–±–æ–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤
        if self.q_trained:
            q_combo = self._generate_q_learning(state)
            combinations.append((q_combo['field1'], q_combo['field2']))
            confidences.append(q_combo['confidence'])
        
        if self.dqn_trained:
            dqn_combo = self._generate_dqn(state)
            combinations.append((dqn_combo['field1'], dqn_combo['field2']))
            confidences.append(dqn_combo['confidence'])
        
        if not combinations:
            return self._generate_random()
        
        # –í—ã–±–∏—Ä–∞–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—é —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        best_idx = np.argmax(confidences)
        field1, field2 = combinations[best_idx]
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ (–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ)
        # –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á–∞—Å—Ç–æ—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –≤—ã–±–æ—Ä–∞ —á–∏—Å–µ–ª
        
        return {
            'field1': sorted(field1),
            'field2': sorted(field2),
            'method': 'Ensemble',
            'confidence': max(confidences),
            'avg_confidence': np.mean(confidences),
            'state_features': state.to_dict()
        }
    
    def _generate_random(self) -> Dict:
        """–°–ª—É—á–∞–π–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–∫ fallback"""
        import random
        field1 = sorted(random.sample(
            range(1, self.lottery_config['field1_max'] + 1),
            self.lottery_config['field1_size']
        ))
        field2 = sorted(random.sample(
            range(1, self.lottery_config['field2_max'] + 1),
            self.lottery_config['field2_size']
        ))
        
        return {
            'field1': field1,
            'field2': field2,
            'method': 'Random',
            'confidence': 0.0,
            'state_features': {}
        }
    
    def _normalize_confidence(self, value: float, agent_type: str) -> float:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
        
        Args:
            value: Q-–∑–Ω–∞—á–µ–Ω–∏–µ
            agent_type: –¢–∏–ø –∞–≥–µ–Ω—Ç–∞ ('q' –∏–ª–∏ 'dqn')
        
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        """
        # –ü—Ä–æ—Å—Ç–∞—è —Å–∏–≥–º–æ–∏–¥–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        # –ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã Q-–∑–Ω–∞—á–µ–Ω–∏–π
        if agent_type == 'q':
            # Q-Learning –æ–±—ã—á–Ω–æ –∏–º–µ–µ—Ç –º–µ–Ω—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            normalized = 1 / (1 + np.exp(-value / 10))
        else:
            # DQN –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –±–æ–ª—å—à–∏–π —Ä–∞–∑–±—Ä–æ—Å
            normalized = 1 / (1 + np.exp(-value / 100))
        
        return float(normalized)
    
    def load_models(self) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        
        Returns:
            True –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        """
        loaded = False
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ Q-–∞–≥–µ–Ω—Ç–∞
        q_path = os.path.join(self.models_dir, "q_agent.pkl")
        if os.path.exists(q_path):
            try:
                self.q_agent.load(q_path)
                self.q_trained = True
                loaded = True
                logger.info(f"‚úÖ Q-–∞–≥–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {q_path}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Q-–∞–≥–µ–Ω—Ç–∞: {e}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ DQN –∞–≥–µ–Ω—Ç–∞
        dqn_path = os.path.join(self.models_dir, "dqn_agent.pth")
        if os.path.exists(dqn_path):
            try:
                self.dqn_agent.load(dqn_path)
                self.dqn_trained = True
                loaded = True
                logger.info(f"‚úÖ DQN –∞–≥–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {dqn_path}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ DQN –∞–≥–µ–Ω—Ç–∞: {e}")
        
        return loaded

    def evaluate(self,
                 df_test: pd.DataFrame,
                 window_size: int = 50) -> Dict:
        """
        –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ RL –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        Args:
            df_test: –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            window_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞

        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        if not self.q_trained and not self.dqn_trained:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
            return {}

        logger.info(f"üìä –û—Ü–µ–Ω–∫–∞ RL –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ {len(df_test)} —Ç–∏—Ä–∞–∂–∞—Ö...")

        env = LotteryEnvironment(df_test, self.lottery_config, window_size)

        results = {
            'q_learning': {'rewards': [], 'matches': []},
            'dqn': {'rewards': [], 'matches': []},
            'ensemble': {'rewards': [], 'matches': []}
        }

        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
        for i in range(window_size, len(df_test) - 1):
            state = env._compute_state(i)
            actual_draw = df_test.iloc[i + 1]

            actual_f1 = eval(actual_draw['field1']) if isinstance(actual_draw['field1'], str) else actual_draw['field1']
            actual_f2 = eval(actual_draw['field2']) if isinstance(actual_draw['field2'], str) else actual_draw['field2']

            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
            if self.q_trained:
                pred_f1, pred_f2 = self.q_agent.predict(state)
                reward = env._calculate_reward(pred_f1, pred_f2, actual_f1, actual_f2)
                matches = len(set(pred_f1) & set(actual_f1))
                results['q_learning']['rewards'].append(reward)
                results['q_learning']['matches'].append(matches)

            if self.dqn_trained:
                pred_f1, pred_f2 = self.dqn_agent.predict(state)
                reward = env._calculate_reward(pred_f1, pred_f2, actual_f1, actual_f2)
                matches = len(set(pred_f1) & set(actual_f1))
                results['dqn']['rewards'].append(reward)
                results['dqn']['matches'].append(matches)

            if self.q_trained and self.dqn_trained:
                ensemble_combo = self._generate_ensemble(state)
                pred_f1, pred_f2 = ensemble_combo['field1'], ensemble_combo['field2']
                reward = env._calculate_reward(pred_f1, pred_f2, actual_f1, actual_f2)
                matches = len(set(pred_f1) & set(actual_f1))
                results['ensemble']['rewards'].append(reward)
                results['ensemble']['matches'].append(matches)

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = {}
        for agent_name, agent_results in results.items():
            if agent_results['rewards']:
                rewards = np.array(agent_results['rewards'])
                matches = np.array(agent_results['matches'])

                metrics[agent_name] = {
                    'total_reward': float(np.sum(rewards)),
                    'average_reward': float(np.mean(rewards)),
                    'win_rate': float(np.mean(rewards > 0) * 100),
                    'average_matches': float(np.mean(matches)),
                    'max_matches': int(np.max(matches)),
                    'roi': float((np.sum(rewards) / len(rewards)) * 100)
                }

        self._print_evaluation_results(metrics)

        return metrics
    
    def _print_evaluation_results(self, metrics: Dict):
        """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏"""
        logger.info("=" * 60)
        logger.info("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò RL –ê–ì–ï–ù–¢–û–í")
        logger.info("=" * 60)
        
        for agent_name, agent_metrics in metrics.items():
            logger.info(f"{agent_name}:")
            logger.info(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {agent_metrics['average_reward']:.2f}")
            logger.info(f"  ‚Ä¢ Win rate: {agent_metrics['win_rate']:.1f}%")
            logger.info(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {agent_metrics['average_matches']:.2f}")
            logger.info(f"  ‚Ä¢ –ú–∞–∫—Å. —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {agent_metrics['max_matches']}")
            logger.info(f"  ‚Ä¢ ROI: {agent_metrics['roi']:.2f}%")
        
        logger.info("=" * 60)
    
    def get_statistics(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞"""
        stats = self.generation_stats.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≥–µ–Ω—Ç–∞—Ö
        stats['agents'] = {
            'q_learning': {
                'trained': self.q_trained,
                'episodes': self.q_agent.total_episodes if self.q_trained else 0,
                'q_table_size': self.q_agent._get_q_table_size() if self.q_trained else 0
            },
            'dqn': {
                'trained': self.dqn_trained,
                'episodes': self.dqn_agent.total_episodes if self.dqn_trained else 0,
                'memory_size': len(self.dqn_agent.memory) if self.dqn_trained else 0
            }
        }
        
        return stats


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä RL –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤
class RLGeneratorManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è RL –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞–º–∏ —Ä–∞–∑–Ω—ã—Ö –ª–æ—Ç–µ—Ä–µ–π
    """
    
    def __init__(self):
        self.generators = {}
        logger.info("‚úÖ RLGeneratorManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def get_generator(self, lottery_type: str, lottery_config: Dict) -> RLGenerator:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ RL –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –¥–ª—è –ª–æ—Ç–µ—Ä–µ–∏
        
        Args:
            lottery_type: –¢–∏–ø –ª–æ—Ç–µ—Ä–µ–∏
            lottery_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ—Ç–µ—Ä–µ–∏
        
        Returns:
            RL –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
        """
        if lottery_type not in self.generators:
            self.generators[lottery_type] = RLGenerator(lottery_config)
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            if self.generators[lottery_type].load_models():
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ RL –º–æ–¥–µ–ª–∏ –¥–ª—è {lottery_type}")
        
        return self.generators[lottery_type]
    
    def train_all(self, verbose: bool = True) -> Dict:
        """
        –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö RL –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤
        
        Args:
            verbose: –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        """
        results = {}
        
        for lottery_type in data_manager.LOTTERY_CONFIGS.keys():
            logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ RL –¥–ª—è {lottery_type}...")
            
            config = data_manager.LOTTERY_CONFIGS[lottery_type]
            generator = self.get_generator(lottery_type, config)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            # with data_manager.LotteryContext(lottery_type):
            #     df = data_manager.fetch_draws_from_db()
            with data_manager.set_current_lottery(lottery_type):
                df = data_manager.fetch_draws_from_db()


            if len(df) >= 60:  # –ú–∏–Ω–∏–º—É–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                stats = generator.train(df, verbose=verbose)
                results[lottery_type] = stats
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {lottery_type}: {len(df)} —Ç–∏—Ä–∞–∂–µ–π")
                results[lottery_type] = {'error': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö'}
        
        return results


# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞
GLOBAL_RL_MANAGER = RLGeneratorManager()