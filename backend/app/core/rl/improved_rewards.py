"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≥—Ä–∞–¥ —Å Reward Shaping
backend/app/core/rl/improved_rewards.py
"""

import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import logging
from collections import Counter

logger = logging.getLogger(__name__)


class ImprovedRewardCalculator:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –Ω–∞–≥—Ä–∞–¥ —Å reward shaping
    """

    def __init__(self, lottery_config: Dict):
        self.lottery_config = lottery_config
        self.field1_size = lottery_config['field1_size']
        self.field2_size = lottery_config['field2_size']
        self.field1_max = lottery_config['field1_max']
        self.field2_max = lottery_config['field2_max']

        # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
        self.combination_history = []
        self.max_history = 1000

        # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è exploration bonus
        self.state_action_counts = {}

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–∏–∑–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –ª–æ—Ç–µ—Ä–µ–∏
        self.setup_prize_structure()

        logger.info(f"‚úÖ ImprovedRewardCalculator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è {self.field1_size}x{self.field1_max}")

    def setup_prize_structure(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–∏–∑–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ª–æ—Ç–µ—Ä–µ–∏"""
        self.ticket_cost = 100.0

        if self.field1_size == 4:  # 4x20 –ª–æ—Ç–µ—Ä–µ—è
            self.match_rewards = {
                (4, 4): 333333,  # –î–∂–µ–∫–ø–æ—Ç
                (4, 3): 2300,
                (4, 2): 650,
                (4, 1): 330,
                (4, 0): 140,
                (3, 3): 70,
                (3, 2): 7,
                (3, 1): 3,
                (3, 0): 6,
                (2, 2): 2,
                (2, 1): 1,
                (2, 0): 0
            }
        elif self.field1_size == 5:  # 5x36 –ª–æ—Ç–µ—Ä–µ—è
            self.match_rewards = {
                (5, 1): 290000,  # –°—É–ø–µ—Ä–¥–∂–µ–∫–ø–æ—Ç
                (5, 0): 14500,   # –î–∂–µ–∫–ø–æ—Ç
                (4, 1): 1000,
                (4, 0): 100,
                (3, 1): 50,
                (3, 0): 10,
                (2, 1): 5,
                (2, 0): 0
            }
        else:
            # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å—Ö–µ–º–∞
            self.match_rewards = {}
            for f1 in range(self.field1_size + 1):
                for f2 in range(self.field2_size + 1):
                    if f1 >= 2:
                        reward = (f1 ** 3) * (1 + f2) * 10
                        self.match_rewards[(f1, f2)] = reward
                    else:
                        self.match_rewards[(f1, f2)] = 0

    def calculate_reward(self,
                        predicted_f1: List[int],
                        predicted_f2: List[int],
                        actual_f1: List[int],
                        actual_f2: List[int],
                        state_features: Optional[Dict] = None) -> Tuple[float, Dict]:
        """
        –†–∞—Å—á–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã —Å reward shaping

        Returns:
            (reward, info_dict) - –Ω–∞–≥—Ä–∞–¥–∞ –∏ –¥–µ—Ç–∞–ª–∏ —Ä–∞—Å—á–µ—Ç–∞
        """
        info = {
            'base_reward': 0,
            'match_reward': 0,
            'shaping_reward': 0,
            'exploration_reward': 0,
            'total_reward': 0,
            'matches_f1': 0,
            'matches_f2': 0
        }

        # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—É—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if not actual_f1 or not actual_f2:
            info['total_reward'] = -self.ticket_cost
            return -self.ticket_cost, info

        # 1. –ë–∞–∑–æ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –±–∏–ª–µ—Ç–∞
        base_reward = -self.ticket_cost
        info['base_reward'] = base_reward

        # 2. –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        matches_f1 = len(set(predicted_f1) & set(actual_f1))
        matches_f2 = len(set(predicted_f2) & set(actual_f2))
        info['matches_f1'] = matches_f1
        info['matches_f2'] = matches_f2

        # –û—Å–Ω–æ–≤–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –ø—Ä–∏–∑–æ–≤
        match_key = (matches_f1, matches_f2)
        match_reward = self.match_rewards.get(match_key, 0)
        info['match_reward'] = match_reward

        # –õ–æ–≥–∏—Ä—É–µ–º –∫—Ä—É–ø–Ω—ã–µ –≤—ã–∏–≥—Ä—ã—à–∏
        if match_reward > 1000:
            logger.info(f"üéâ –ö—Ä—É–ø–Ω—ã–π –≤—ã–∏–≥—Ä—ã—à: {matches_f1}+{matches_f2} = {match_reward}")

        # 3. Reward Shaping - –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã
        shaping_reward = 0

        # 3.1 Near-miss bonus (–ø–æ—á—Ç–∏ —É–≥–∞–¥–∞–ª)
        if matches_f1 == self.field1_size - 1:  # –ù–∞ 1 –º–µ–Ω—å—à–µ –º–∞–∫—Å–∏–º—É–º–∞
            shaping_reward += 5.0  # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ –±–ª–∏–∑–æ—Å—Ç—å
            info['near_miss'] = True
        elif matches_f1 == self.field1_size - 2:  # –ù–∞ 2 –º–µ–Ω—å—à–µ
            shaping_reward += 1.0

        # 3.2 –ß–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–≥—Ä–∞–¥–∏–µ–Ω—Ç –Ω–∞–≥—Ä–∞–¥)
        partial_reward = matches_f1 * 0.5 + matches_f2 * 0.3
        shaping_reward += partial_reward

        # 3.3 –ë–æ–Ω—É—Å –∑–∞ –≥–æ—Ä—è—á–∏–µ —á–∏—Å–ª–∞
        if state_features and 'hot_numbers' in state_features:
            hot_numbers = state_features.get('hot_numbers', [])
            if hot_numbers and predicted_f1:
                hot_count = len(set(predicted_f1) & set(hot_numbers))
                hot_bonus = hot_count * 0.2
                shaping_reward += hot_bonus
                info['hot_numbers_used'] = hot_count

        # 3.4 –®—Ç—Ä–∞—Ñ –∑–∞ —Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞ (–Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π)
        if state_features and 'cold_numbers' in state_features:
            cold_numbers = state_features.get('cold_numbers', [])
            if cold_numbers and predicted_f1:
                cold_count = len(set(predicted_f1) & set(cold_numbers))
                if cold_count > self.field1_size // 2:
                    cold_penalty = cold_count * -0.1
                    shaping_reward += cold_penalty
                    info['cold_penalty'] = cold_penalty

        # 3.5 –ë–æ–Ω—É—Å –∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if self._has_interesting_pattern(predicted_f1):
            shaping_reward += 0.3
            info['pattern_bonus'] = True

        # 3.6 –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
        diversity = self._calculate_diversity(predicted_f1)
        if diversity > 0.6:
            shaping_reward += 0.2
            info['diversity_bonus'] = True

        info['shaping_reward'] = shaping_reward

        # 4. Exploration bonus (–ø–æ–æ—â—Ä—è–µ–º –Ω–æ–≤—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏)
        exploration_reward = self._calculate_exploration_bonus(
            predicted_f1, predicted_f2, state_features
        )
        info['exploration_reward'] = exploration_reward

        # 5. –ò—Ç–æ–≥–æ–≤–∞—è –Ω–∞–≥—Ä–∞–¥–∞
        total_reward = base_reward + match_reward + shaping_reward + exploration_reward
        info['total_reward'] = total_reward

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –≤ –∏—Å—Ç–æ—Ä–∏—é
        self._update_history(predicted_f1, predicted_f2)

        return total_reward, info

    def _has_interesting_pattern(self, numbers: List[int]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        if len(numbers) < 3:
            return False

        sorted_nums = sorted(numbers)

        # –ê—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏—è
        diffs = [sorted_nums[i+1] - sorted_nums[i] for i in range(len(sorted_nums)-1)]
        if len(set(diffs)) == 1 and diffs[0] > 0:
            return True

        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —á–∏—Å–ª–∞ (3+ –ø–æ–¥—Ä—è–¥)
        consecutive = 1
        for i in range(len(sorted_nums)-1):
            if sorted_nums[i+1] - sorted_nums[i] == 1:
                consecutive += 1
                if consecutive >= 3:
                    return True
            else:
                consecutive = 1

        # –°–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ä–µ–¥–Ω–µ–≥–æ)
        if len(sorted_nums) >= 4:
            mean = sum(sorted_nums) / len(sorted_nums)
            distances = [abs(n - mean) for n in sorted_nums]
            if len(set(distances)) <= 2:  # –ú–∞–∫—Å–∏–º—É–º 2 —Ä–∞–∑–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
                return True

        return False

    def _calculate_diversity(self, numbers: List[int]) -> float:
        """–†–∞—Å—á–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —á–∏—Å–µ–ª (0-1)"""
        if not numbers:
            return 0

        sorted_nums = sorted(numbers)

        # –†–∞–∑–±—Ä–æ—Å –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω—É
        range_coverage = (max(numbers) - min(numbers)) / max(self.field1_max, 1)

        # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        gaps = [sorted_nums[i+1] - sorted_nums[i] for i in range(len(sorted_nums)-1)]
        if gaps:
            gap_variance = np.std(gaps) / np.mean(gaps) if np.mean(gaps) > 0 else 1
            uniformity = 1 / (1 + gap_variance)
        else:
            uniformity = 0

        # –ß–µ—Ç–Ω–æ—Å—Ç—å/–Ω–µ—á–µ—Ç–Ω–æ—Å—Ç—å –±–∞–ª–∞–Ω—Å
        even_count = sum(1 for n in numbers if n % 2 == 0)
        parity_balance = 1 - abs(0.5 - even_count/len(numbers)) * 2

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞
        diversity = (range_coverage + uniformity + parity_balance) / 3

        return min(max(diversity, 0), 1)

    def _calculate_exploration_bonus(self,
                                    predicted_f1: List[int],
                                    predicted_f2: List[int],
                                    state_features: Optional[Dict]) -> float:
        """
        –ë–æ–Ω—É—Å –∑–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç count-based exploration
        """
        # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –¥–ª—è state-action –ø–∞—Ä—ã
        state_key = self._get_state_key(state_features) if state_features else "default"
        action_key = f"{sorted(predicted_f1)}_{sorted(predicted_f2)}"
        sa_key = f"{state_key}_{action_key}"

        # –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —ç—Ç–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å
        if sa_key not in self.state_action_counts:
            self.state_action_counts[sa_key] = 0

        count = self.state_action_counts[sa_key]
        self.state_action_counts[sa_key] += 1

        # Exploration bonus –æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω —á–∞—Å—Ç–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º UCB-like —Ñ–æ—Ä–º—É–ª—É
        exploration_bonus = 1.0 / np.sqrt(count + 1)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–æ–Ω—É—Å
        exploration_bonus = min(exploration_bonus, 2.0)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–æ–≤—É—é –∫–æ–º–±–∏–Ω–∞—Ü–∏—é
        if count == 0:
            exploration_bonus += 0.5

        return exploration_bonus

    def _get_state_key(self, state_features: Dict) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if not state_features:
            return "empty"

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª—é—á–∞
        key_features = []
        for feature in ['hot_numbers_count', 'parity_ratio', 'diversity_index']:
            if feature in state_features:
                # –î–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
                value = state_features[feature]
                if isinstance(value, (int, float)):
                    discretized = int(value * 10) / 10  # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ 0.1
                    key_features.append(f"{feature}:{discretized}")

        return "_".join(key_features) if key_features else "default"

    def _update_history(self, predicted_f1: List[int], predicted_f2: List[int]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π"""
        combo_key = f"{sorted(predicted_f1)}_{sorted(predicted_f2)}"
        self.combination_history.append(combo_key)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.combination_history) > self.max_history:
            self.combination_history.pop(0)

    def get_statistics(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        unique_combos = len(set(self.combination_history))
        total_combos = len(self.combination_history)

        exploration_rate = unique_combos / total_combos if total_combos > 0 else 0

        return {
            'total_combinations': total_combos,
            'unique_combinations': unique_combos,
            'exploration_rate': exploration_rate,
            'state_action_pairs': len(self.state_action_counts)
        }


class CuriosityDrivenBonus:
    """
    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è curiosity-driven exploration
    –ù–∞–≥—Ä–∞–∂–¥–∞–µ—Ç –∑–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    """

    def __init__(self, state_dim: int = 10):
        self.state_dim = state_dim
        self.seen_states = set()
        self.state_transitions = {}
        self.prediction_errors = []

    def calculate_curiosity_reward(self,
                                  state: np.ndarray,
                                  next_state: np.ndarray,
                                  action: Tuple) -> float:
        """
        –†–∞—Å—á–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã –∑–∞ –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ
        –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ prediction error (–∫–∞–∫ –≤ ICM - Intrinsic Curiosity Module)
        """
        state_key = self._discretize_state(state)
        next_state_key = self._discretize_state(next_state)
        action_key = str(action)

        # –ù–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ - –±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞
        if next_state_key not in self.seen_states:
            self.seen_states.add(next_state_key)
            curiosity_reward = 1.0
        else:
            curiosity_reward = 0.1

        # –ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥
        transition_key = f"{state_key}_{action_key}"
        if transition_key in self.state_transitions:
            expected_next = self.state_transitions[transition_key]
            if expected_next != next_state_key:
                # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –Ω–∞–≥—Ä–∞–¥–∞
                prediction_error = self._calculate_prediction_error(
                    expected_next, next_state_key
                )
                curiosity_reward += prediction_error * 0.5

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –ø–µ—Ä–µ—Ö–æ–¥–æ–≤
        self.state_transitions[transition_key] = next_state_key

        return curiosity_reward

    def _discretize_state(self, state: np.ndarray) -> str:
        """–î–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        discretized = np.round(state, decimals=1)
        return str(discretized.tolist())

    def _calculate_prediction_error(self, expected: str, actual: str) -> float:
        """–†–∞—Å—á–µ—Ç –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞–∑–ª–∏—á–∏—è
        if expected == actual:
            return 0.0
        return 1.0  # –ú–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –º–µ—Ç—Ä–∏–∫—É