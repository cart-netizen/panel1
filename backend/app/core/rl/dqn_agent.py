"""
Deep Q-Network (DQN) –∞–≥–µ–Ω—Ç –¥–ª—è –ª–æ—Ç–µ—Ä–µ–∏
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ Q-—Ñ—É–Ω–∫—Ü–∏–∏
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any
from collections import deque
import random
import logging
from datetime import datetime

from backend.app.core.rl.environment import LotteryEnvironment, LotteryState
from backend.app.core.rl.state_encoder import StateEncoder, ActionEncoder
from backend.app.core.rl.reward_calculator import RewardCalculator

logger = logging.getLogger(__name__)


class DQNNetwork(nn.Module):
    """
    –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ Q-—Ñ—É–Ω–∫—Ü–∏–∏
    """

    def __init__(self, state_size: int, action_embedding_size: int = 128, hidden_sizes: List[int] = None):
        """
        Args:
            state_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            action_embedding_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–µ–π—Å—Ç–≤–∏–π
            hidden_sizes: –†–∞–∑–º–µ—Ä—ã —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤
        """
        super(DQNNetwork, self).__init__()

        if hidden_sizes is None:
            hidden_sizes = [256, 512, 256]

        self.state_size = state_size
        self.action_embedding_size = action_embedding_size

        # –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.state_encoder = nn.Sequential(
            nn.Linear(state_size, hidden_sizes[0]),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_sizes[0]),
            nn.Dropout(0.2)
        )

        # –û—Å–Ω–æ–≤–Ω–∞—è —Å–µ—Ç—å
        layers = []
        for i in range(len(hidden_sizes) - 1):
            layers.extend([
                nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_sizes[i + 1]),
                nn.Dropout(0.2)
            ])
        self.hidden_layers = nn.Sequential(*layers)

        # –î—É—ç–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (Dueling DQN)
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_sizes[-1], 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_sizes[-1], 128),
            nn.ReLU(),
            nn.Linear(128, action_embedding_size)
        )

        # –î–µ–∫–æ–¥–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π
        self.action_decoder_f1 = nn.Sequential(
            nn.Linear(action_embedding_size, 256),
            nn.ReLU(),
            nn.Linear(256, 50),  # –ú–∞–∫—Å–∏–º—É–º 50 —á–∏—Å–µ–ª –¥–ª—è field1
            nn.Sigmoid()
        )

        self.action_decoder_f2 = nn.Sequential(
            nn.Linear(action_embedding_size, 128),
            nn.ReLU(),
            nn.Linear(128, 12),  # –ú–∞–∫—Å–∏–º—É–º 12 —á–∏—Å–µ–ª –¥–ª—è field2
            nn.Sigmoid()
        )

        # –ì–æ–ª–æ–≤—ã –¥–ª—è Q-–∑–Ω–∞—á–µ–Ω–∏–π –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        self.q_head = nn.Sequential(
            nn.Linear(hidden_sizes[-1], 256),
            nn.ReLU(),
            nn.Linear(256, action_embedding_size)
        )

        self.field1_head = nn.Sequential(
            nn.Linear(hidden_sizes[-1], 256),
            nn.ReLU(),
            nn.Linear(256, 50)  # –ú–∞–∫—Å–∏–º—É–º 50 —á–∏—Å–µ–ª –¥–ª—è field1
        )

        self.field2_head = nn.Sequential(
            nn.Linear(hidden_sizes[-1], 128),
            nn.ReLU(),
            nn.Linear(128, 12)  # –ú–∞–∫—Å–∏–º—É–º 12 —á–∏—Å–µ–ª –¥–ª—è field2
        )

    # def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    #     """
    #     –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —Å–µ—Ç–∏
    #
    #     Args:
    #         state: –¢–µ–Ω–∑–æ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è
    #
    #     Returns:
    #         (Q-–∑–Ω–∞—á–µ–Ω–∏—è, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ field1, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ field2)
    #     """
    #     # –ö–æ–¥–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    #     x = self.state_encoder(state)
    #     x = self.hidden_layers(x)
    #
    #     # –î—É—ç–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è Q-–∑–Ω–∞—á–µ–Ω–∏–π
    #     value = self.value_stream(x)
    #     advantage = self.advantage_stream(x)
    #
    #     # Q = V(s) + A(s,a) - mean(A(s,a))
    #     q_values = value + advantage - advantage.mean(dim=1, keepdim=True)
    #
    #     # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏–π
    #     action_embedding = advantage  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∫–∞–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥
    #     field1_probs = self.action_decoder_f1(action_embedding)
    #     field2_probs = self.action_decoder_f2(action_embedding)
    #
    #     return q_values, field1_probs, field2_probs

    def forward(self, state):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è BatchNorm
        if state.size(0) == 1:
            # –ü—Ä–∏ —Ä–∞–∑–º–µ—Ä–µ –±–∞—Ç—á–∞ = 1 –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ eval —Ä–µ–∂–∏–º –¥–ª—è BatchNorm
            self.state_encoder.eval()
            with torch.no_grad():
                x = self.state_encoder(state)
            self.state_encoder.train()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ train —Ä–µ–∂–∏–º
        else:
            x = self.state_encoder(state)

        # –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏
        x = self.hidden_layers(x)

        # Q-values –¥–ª—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
        q_values = self.q_head(x)

        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è
        field1_logits = self.field1_head(x)
        field2_logits = self.field2_head(x)

        field1_probs = F.softmax(field1_logits, dim=-1)
        field2_probs = F.softmax(field2_logits, dim=-1)

        return q_values, field1_probs, field2_probs


class ReplayBuffer:
    """
    –ë—É—Ñ–µ—Ä –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–ø—ã—Ç–∞
    """
    
    def __init__(self, capacity: int = 10000):
        """
        Args:
            capacity: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
        """
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state: np.ndarray, action: Tuple, reward: float, 
             next_state: Optional[np.ndarray], done: bool):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤ –±—É—Ñ–µ—Ä"""
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size: int) -> List[Tuple]:
        """–°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ –±—É—Ñ–µ—Ä–∞"""
        return random.sample(self.buffer, batch_size)
    
    def __len__(self) -> int:
        return len(self.buffer)


class DQNAgent:
    """
    Deep Q-Network –∞–≥–µ–Ω—Ç –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ª–æ—Ç–µ—Ä–µ–∏
    """
    
    def __init__(self,
                 lottery_config: Dict,
                 learning_rate: float = 0.001,
                 discount_factor: float = 0.99,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 batch_size: int = 32,
                 memory_size: int = 10000,
                 target_update_freq: int = 100,
                 device: str = None):
        """
        Args:
            lottery_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ—Ç–µ—Ä–µ–∏
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            discount_factor: –§–∞–∫—Ç–æ—Ä –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            epsilon: –ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ epsilon
            epsilon_decay: –ó–∞—Ç—É—Ö–∞–Ω–∏–µ epsilon
            epsilon_min: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ epsilon
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            memory_size: –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
            target_update_freq: –ß–∞—Å—Ç–æ—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π —Å–µ—Ç–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cuda/cpu)
        """
        self.lottery_config = lottery_config
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        
        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        # –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏
        feature_dims = {
            'universe_length': 100,
            'parity_ratio': 1,
            'mean_gap': 50,
            'mean_frequency': 10,
            'hot_numbers_count': 20,
            'cold_numbers_count': 20,
            'sum_trend': 100,
            'diversity_index': 1,
            'days_since_jackpot': 365,
            'draw_number': 1000
        }
        self.state_encoder = StateEncoder(feature_dims)
        self.action_encoder = ActionEncoder(lottery_config)
        
        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        self.state_size = 10  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏
        self.field1_size = lottery_config['field1_size']
        self.field2_size = lottery_config['field2_size']
        self.field1_max = lottery_config['field1_max']
        self.field2_max = lottery_config['field2_max']
        
        # –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏
        self.q_network = DQNNetwork(self.state_size).to(self.device)
        self.target_network = DQNNetwork(self.state_size).to(self.device)
        self.target_network.load_state_dict(self.q_network.state_dict())

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # –ë—É—Ñ–µ—Ä –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
        self.memory = ReplayBuffer(memory_size)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_steps = 0
        self.total_episodes = 0
        self.total_reward = 0
        self.wins = 0
        self.losses = []
        self.episode_rewards = []
        self.learning_history = []
        
        logger.info(f"‚úÖ DQN –∞–≥–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ {self.device}")
        logger.info(f"   –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {sum(p.numel() for p in self.q_network.parameters())} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    def state_to_tensor(self, state: LotteryState) -> torch.Tensor:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä"""
        state_vector = state.to_vector()
        return torch.FloatTensor(state_vector).unsqueeze(0).to(self.device)
    
    def choose_action(self, state: LotteryState, training: bool = True) -> Tuple[List[int], List[int]]:
        """
        –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è —Å –ø–æ–º–æ—â—å—é epsilon-greedy —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        
        Args:
            state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            training: –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
        
        Returns:
            –í—ã–±—Ä–∞–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (field1, field2)
        """
        # Epsilon-greedy
        if training and random.random() < self.epsilon:
            # –°–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            return self.action_encoder.sample_random_action()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç—å
        state_tensor = self.state_to_tensor(state)

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è BatchNorm
        if state_tensor.size(0) == 1:
            self.q_network.eval()

        with torch.no_grad():
            q_values, field1_probs, field2_probs = self.q_network(state_tensor)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ train —Ä–µ–∂–∏–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if training and state_tensor.size(0) == 1:
            self.q_network.train()
            
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –¥–µ–π—Å—Ç–≤–∏—è
        field1 = self._probs_to_numbers(field1_probs[0], self.field1_size, self.field1_max)
        field2 = self._probs_to_numbers(field2_probs[0], self.field2_size, self.field2_max)

        return field1, field2
    
    def _probs_to_numbers(self, probs: torch.Tensor, size: int, max_num: int) -> List[int]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –≤ —á–∏—Å–ª–∞
        
        Args:
            probs: –¢–µ–Ω–∑–æ—Ä –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∏—Å–µ–ª
            max_num: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ
        
        Returns:
            –°–ø–∏—Å–æ–∫ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —á–∏—Å–µ–ª
        """
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        probs = probs[:max_num].cpu().numpy()
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-K –∏–Ω–¥–µ–∫—Å–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
        top_indices = np.argsort(probs)[-size:]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤ —á–∏—Å–ª–∞ (–∏–Ω–¥–µ–∫—Å 0 = —á–∏—Å–ª–æ 1)
        numbers = [int(idx) + 1 for idx in top_indices]
        
        return sorted(numbers)
    
    def remember(self, state: LotteryState, action: Tuple[List[int], List[int]], 
                reward: float, next_state: Optional[LotteryState], done: bool):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ –≤ –±—É—Ñ–µ—Ä
        
        Args:
            state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            action: –í—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            reward: –ü–æ–ª—É—á–µ–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞
            next_state: –°–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            done: –ü—Ä–∏–∑–Ω–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        """
        state_vector = state.to_vector()
        next_state_vector = next_state.to_vector() if next_state else None
        
        self.memory.push(state_vector, action, reward, next_state_vector, done)

    def replay(self):
        """h
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –∏–∑ –±—É—Ñ–µ—Ä–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
        """
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è BatchNorm
        min_batch_size = max(2, self.batch_size // 4)  # –ú–∏–Ω–∏–º—É–º 2 –æ–±—Ä–∞–∑—Ü–∞

        if len(self.memory) < min_batch_size:
            return

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        actual_batch_size = min(self.batch_size, len(self.memory))
        if actual_batch_size < 2:
            actual_batch_size = 2  # –ú–∏–Ω–∏–º—É–º –¥–ª—è BatchNorm

        # –°—ç–º–ø–ª–∏—Ä—É–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        batch = self.memory.sample(actual_batch_size)

        states_array = np.array([e[0] for e in batch])
        states = torch.FloatTensor(states_array).to(self.device)
        actions = [e[1] for e in batch]
        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)
        next_states = [e[3] for e in batch]
        dones = torch.FloatTensor([e[4] for e in batch]).to(self.device)

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è BatchNorm
        if states.size(0) == 1:
            self.q_network.eval()
            self.target_network.eval()

        # –¢–µ–∫—É—â–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è
        current_q_values, _, _ = self.q_network(states)

        # –¶–µ–ª–µ–≤—ã–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è
        target_q_values = rewards.clone()

        # –î–ª—è –Ω–µ-—Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –¥–æ–±–∞–≤–ª—è–µ–º –±—É–¥—É—â—É—é –Ω–∞–≥—Ä–∞–¥—É
        non_terminal_mask = (dones == 0)
        if non_terminal_mask.any():
            non_terminal_list = [ns for ns, d in zip(next_states, dones) if d == 0 and ns is not None]
            if non_terminal_list:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º np.stack –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è numpy –º–∞—Å—Å–∏–≤–æ–≤
                non_terminal_array = np.stack(non_terminal_list)
                non_terminal_next_states = torch.FloatTensor(non_terminal_array).to(self.device)
            else:
                # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ç–µ–Ω–∑–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã
                non_terminal_next_states = torch.empty(0, 10).to(self.device)  # 10 = —Ä–∞–∑–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è

            if len(non_terminal_next_states) > 0:
                with torch.no_grad():
                    next_q_values, _, _ = self.target_network(non_terminal_next_states)
                    max_next_q = next_q_values.max(1)[0]

                    # –û–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –Ω–µ-—Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
                    j = 0
                    for i, is_non_terminal in enumerate(non_terminal_mask):
                        if is_non_terminal and next_states[i] is not None:
                            target_q_values[i] += self.discount_factor * max_next_q[j]
                            j += 1

        # –í—ã—á–∏—Å–ª—è–µ–º loss
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–µ–π—Å—Ç–≤–∏—è
        loss = F.mse_loss(current_q_values.mean(1), target_q_values)

        # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
        self.optimizer.zero_grad()
        loss.backward()

        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫–ª–∏–ø–ø–∏–Ω–≥
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)

        self.optimizer.step()

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ train —Ä–µ–∂–∏–º
        if states.size(0) == 1:
            self.q_network.train()
            self.target_network.train()

        self.losses.append(loss.item())
    
    def update_target_network(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Å–µ—Ç–∏"""
        self.target_network.load_state_dict(self.q_network.state_dict())
        logger.debug("üéØ –¶–µ–ª–µ–≤–∞—è —Å–µ—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
    
    def train_episode(self, env: LotteryEnvironment, max_steps: int = 100) -> Dict:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º —ç–ø–∏–∑–æ–¥–µ
        
        Args:
            env: –°—Ä–µ–¥–∞ –ª–æ—Ç–µ—Ä–µ–∏
            max_steps: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–∏–∑–æ–¥–∞
        """
        state = env.reset()
        episode_reward = 0
        steps = 0
        wins_episode = 0
        
        for step in range(max_steps):
            # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            action = self.choose_action(state, training=True)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            next_state, reward, done, info = env.step(action)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–ø—ã—Ç
            self.remember(state, action, reward, next_state, done)
            
            # –û–±—É—á–∞–µ–º—Å—è –Ω–∞ –æ–ø—ã—Ç–µ
            if len(self.memory) >= self.batch_size:
                self.replay()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            episode_reward += reward
            if reward > 0:
                wins_episode += 1
            
            steps += 1
            self.total_steps += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é —Å–µ—Ç—å
            if self.total_steps % self.target_update_freq == 0:
                self.update_target_network()
            
            state = next_state
            
            if done:
                break
        
        # –ó–∞—Ç—É—Ö–∞–Ω–∏–µ epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.total_episodes += 1
        self.total_reward += episode_reward
        # if wins_episode > 0:
        #     self.wins += 1
        self.wins += wins_episode

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        episode_stats = {
            'episode': self.total_episodes,
            'steps': steps,
            'reward': episode_reward,
            'wins': wins_episode,
            'epsilon': self.epsilon,
            'loss': np.mean(self.losses[-100:]) if self.losses else 0
        }
        
        self.episode_rewards.append(episode_reward)
        self.learning_history.append(episode_stats)
        
        return episode_stats
    
    def train(self,
              df_history: pd.DataFrame,
              num_episodes: int = 500,
              window_size: int = 50,
              verbose: bool = True) -> Dict:
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            df_history: –ò—Å—Ç–æ—Ä–∏—è —Ç–∏—Ä–∞–∂–µ–π
            num_episodes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤
            window_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            verbose: –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
        Returns:
            –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        """
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è DQN –∞–≥–µ–Ω—Ç–∞: {num_episodes} —ç–ø–∏–∑–æ–¥–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–¥—É
        env = LotteryEnvironment(df_history, self.lottery_config, window_size)
        
        best_episode_reward = -float('inf')
        best_episode = None
        
        for episode in range(num_episodes):
            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —ç–ø–∏–∑–æ–¥–µ
            stats = self.train_episode(env)
            
            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –ª—É—á—à–∏–π —ç–ø–∏–∑–æ–¥
            if stats['reward'] > best_episode_reward:
                best_episode_reward = stats['reward']
                best_episode = stats['episode']
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥
            if verbose and (episode + 1) % 50 == 0:
                avg_reward = np.mean(self.episode_rewards[-50:])
                avg_loss = np.mean(self.losses[-500:]) if self.losses else 0
                win_rate = (self.wins / max(self.total_steps, 1)) * 100
                # total_games = sum(len(episode) for episode in episode_history) if hasattr(self,
                #                                                                           'episode_history') else self.total_steps
                # win_rate = (self.wins / max(total_games, 1)) * 100 if total_games > 0 else 0


                logger.info(f"üìà –≠–ø–∏–∑–æ–¥ {episode + 1}/{num_episodes}: "
                          f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞={avg_reward:.2f}, "
                          f"Loss={avg_loss:.4f}, "
                          f"Win rate={win_rate:.1f}%, "
                          f"Œµ={self.epsilon:.3f}")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        final_stats = {
            'total_episodes': self.total_episodes,
            'total_steps': self.total_steps,
            'total_reward': self.total_reward,
            'average_reward': self.total_reward / self.total_episodes,
            'win_rate': (self.wins / max(self.total_steps, 1)) * 100,
            'best_episode': best_episode,
            'best_reward': best_episode_reward,
            'final_epsilon': self.epsilon,
            'final_loss': np.mean(self.losses[-100:]) if self.losses else 0,
            'memory_size': len(self.memory)
        }
        
        logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ DQN –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {final_stats['average_reward']:.2f}")
        logger.info(f"   Win rate: {final_stats['win_rate']:.1f}%")
        logger.info(f"   –õ—É—á—à–∏–π —ç–ø–∏–∑–æ–¥: {final_stats['best_episode']} (–Ω–∞–≥—Ä–∞–¥–∞={final_stats['best_reward']:.2f})")
        logger.info(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {final_stats['final_loss']:.4f}")
        
        return final_stats
    
    def predict(self, state: LotteryState) -> Tuple[List[int], List[int]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ª—É—á—à–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è (—Ä–µ–∂–∏–º —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏)
        
        Args:
            state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        
        Returns:
            –õ—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        """
        return self.choose_action(state, training=False)
    
    def save(self, filepath: str):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        """
        save_dict = {
            'q_network_state': self.q_network.state_dict(),
            'target_network_state': self.target_network.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'total_steps': self.total_steps,
            'total_episodes': self.total_episodes,
            'total_reward': self.total_reward,
            'wins': self.wins,
            'learning_history': self.learning_history,
            'config': {
                'learning_rate': self.learning_rate,
                'discount_factor': self.discount_factor,
                'epsilon_decay': self.epsilon_decay,
                'epsilon_min': self.epsilon_min,
                'batch_size': self.batch_size,
                'target_update_freq': self.target_update_freq
            }
        }
        
        torch.save(save_dict, filepath)
        logger.info(f"üíæ DQN –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filepath}")
    
    def load(self, filepath: str):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        """
        save_dict = torch.load(filepath, map_location=self.device, weights_only=False)
        
        self.q_network.load_state_dict(save_dict['q_network_state'])
        self.target_network.load_state_dict(save_dict['target_network_state'])
        self.optimizer.load_state_dict(save_dict['optimizer_state'])
        
        self.epsilon = save_dict['epsilon']
        self.total_steps = save_dict['total_steps']
        self.total_episodes = save_dict['total_episodes']
        self.total_reward = save_dict['total_reward']
        self.wins = save_dict['wins']
        self.learning_history = save_dict['learning_history']
        
        config = save_dict['config']
        self.learning_rate = config['learning_rate']
        self.discount_factor = config['discount_factor']
        self.epsilon_decay = config['epsilon_decay']
        self.epsilon_min = config['epsilon_min']
        self.batch_size = config['batch_size']
        self.target_update_freq = config['target_update_freq']
        
        logger.info(f"‚úÖ DQN –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {filepath}")
        logger.info(f"   –≠–ø–∏–∑–æ–¥–æ–≤: {self.total_episodes}, –®–∞–≥–æ–≤: {self.total_steps}")