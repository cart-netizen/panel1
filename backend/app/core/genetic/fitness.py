"""
–§–∏—Ç–Ω–µ—Å—Å-—Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import numpy as np
import pandas as pd
from typing import List, Tuple, Dict, Optional, Callable
from collections import Counter
import logging
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class FitnessEvaluator:
  """
  –û—Ü–µ–Ω—â–∏–∫ –ø—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–Ω–æ—Å—Ç–∏ —Ö—Ä–æ–º–æ—Å–æ–º
  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
  """

  def __init__(self, df_history: pd.DataFrame, lottery_config: Dict):
    """
    Args:
        df_history: –ò—Å—Ç–æ—Ä–∏—è —Ç–∏—Ä–∞–∂–µ–π
        lottery_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ—Ç–µ—Ä–µ–∏
    """
    self.df_history = df_history
    self.lottery_config = lottery_config

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ª–æ—Ç–µ—Ä–µ–∏
    self.field1_size = lottery_config['field1_size']
    self.field2_size = lottery_config['field2_size']
    self.field1_max = lottery_config['field1_max']
    self.field2_max = lottery_config['field2_max']

    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    self._precompute_statistics()

    # –í–µ—Å–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ñ–∏—Ç–Ω–µ—Å—Å-—Ñ—É–Ω–∫—Ü–∏–∏
    self.weights = {
      'historical_matches': 0.3,  # –°–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å –∏—Å—Ç–æ—Ä–∏–µ–π
      'frequency_alignment': 0.2,  # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–∞—Å—Ç–æ—Ç–∞–º
      'pattern_similarity': 0.15,  # –°—Ö–æ–∂–µ—Å—Ç—å —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
      'balance_score': 0.1,  # –ë–∞–ª–∞–Ω—Å —á–µ—Ç/–Ω–µ—á–µ—Ç, –º–∞–ª—ã–µ/–±–æ–ª—å—à–∏–µ
      'sum_range': 0.1,  # –ü–æ–ø–∞–¥–∞–Ω–∏–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω —Å—É–º–º
      'uniqueness': 0.05,  # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
      'trend_alignment': 0.1  # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–Ω–¥–∞–º
    }

    # –ö—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    self._fitness_cache = {}
    self._cache_hits = 0
    self._cache_misses = 0

    logger.info(f"‚úÖ FitnessEvaluator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å {len(df_history)} —Ç–∏—Ä–∞–∂–∞–º–∏")

  def _precompute_statistics(self):
    """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è"""
    if self.df_history.empty:
      self._init_empty_stats()
      return

    # –ß–∞—Å—Ç–æ—Ç—ã —á–∏—Å–µ–ª
    self.freq_field1 = Counter()
    self.freq_field2 = Counter()

    # –°—É–º–º—ã –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
    self.sums_field1 = []
    self.sums_field2 = []

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —á–µ—Ç/–Ω–µ—á–µ—Ç
    self.parity_patterns = []

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
    for _, row in self.df_history.iterrows():
      f1_list = row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', [])
      f2_list = row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', [])

      if isinstance(f1_list, list):
        self.freq_field1.update(f1_list)
        self.sums_field1.append(sum(f1_list))

      if isinstance(f2_list, list):
        self.freq_field2.update(f2_list)
        self.sums_field2.append(sum(f2_list))

      # –ü–∞—Ç—Ç–µ—Ä–Ω —á–µ—Ç–Ω–æ—Å—Ç–∏
      if isinstance(f1_list, list) and isinstance(f2_list, list):
        even_count = sum(1 for n in f1_list + f2_list if n % 2 == 0)
        self.parity_patterns.append(even_count)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—É–º–º
    if self.sums_field1:
      self.sum_stats_f1 = {
        'mean': np.mean(self.sums_field1),
        'std': np.std(self.sums_field1),
        'min': np.min(self.sums_field1),
        'max': np.max(self.sums_field1)
      }
    else:
      self.sum_stats_f1 = {'mean': 0, 'std': 1, 'min': 0, 'max': 100}

    if self.sums_field2:
      self.sum_stats_f2 = {
        'mean': np.mean(self.sums_field2),
        'std': np.std(self.sums_field2),
        'min': np.min(self.sums_field2),
        'max': np.max(self.sums_field2)
      }
    else:
      self.sum_stats_f2 = {'mean': 0, 'std': 1, 'min': 0, 'max': 100}

    # –ì–æ—Ä—è—á–∏–µ –∏ —Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞
    self._calculate_hot_cold_numbers()

    logger.info(f"üìä –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–æ: —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è {len(self.freq_field1)} —á–∏—Å–µ–ª –ø–æ–ª—è 1, "
                f"{len(self.freq_field2)} —á–∏—Å–µ–ª –ø–æ–ª—è 2")

  def _init_empty_stats(self):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Å—Ç–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    self.freq_field1 = Counter()
    self.freq_field2 = Counter()
    self.sums_field1 = []
    self.sums_field2 = []
    self.parity_patterns = []
    self.sum_stats_f1 = {'mean': 50, 'std': 10, 'min': 10, 'max': 90}
    self.sum_stats_f2 = {'mean': 50, 'std': 10, 'min': 10, 'max': 90}
    self.hot_numbers_f1 = []
    self.cold_numbers_f1 = []
    self.hot_numbers_f2 = []
    self.cold_numbers_f2 = []

  def _calculate_hot_cold_numbers(self, percentile: float = 0.2):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥–æ—Ä—è—á–∏—Ö –∏ —Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª"""
    # Field1
    if self.freq_field1:
      sorted_f1 = sorted(self.freq_field1.items(), key=lambda x: x[1], reverse=True)
      hot_count = max(1, int(len(sorted_f1) * percentile))
      self.hot_numbers_f1 = [num for num, _ in sorted_f1[:hot_count]]
      self.cold_numbers_f1 = [num for num, _ in sorted_f1[-hot_count:]]
    else:
      self.hot_numbers_f1 = []
      self.cold_numbers_f1 = []

    # Field2
    if self.freq_field2:
      sorted_f2 = sorted(self.freq_field2.items(), key=lambda x: x[1], reverse=True)
      hot_count = max(1, int(len(sorted_f2) * percentile))
      self.hot_numbers_f2 = [num for num, _ in sorted_f2[:hot_count]]
      self.cold_numbers_f2 = [num for num, _ in sorted_f2[-hot_count:]]
    else:
      self.hot_numbers_f2 = []
      self.cold_numbers_f2 = []

  def evaluate(self, field1: List[int], field2: List[int]) -> float:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–Ω–æ—Å—Ç–∏

    Args:
        field1: –ß–∏—Å–ª–∞ –ø–µ—Ä–≤–æ–≥–æ –ø–æ–ª—è
        field2: –ß–∏—Å–ª–∞ –≤—Ç–æ—Ä–æ–≥–æ –ø–æ–ª—è

    Returns:
        –ó–Ω–∞—á–µ–Ω–∏–µ fitness –æ—Ç 0 –¥–æ 100+
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cache_key = (tuple(sorted(field1)), tuple(sorted(field2)))
    if cache_key in self._fitness_cache:
      self._cache_hits += 1
      return self._fitness_cache[cache_key]

    self._cache_misses += 1

    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã fitness
    scores = {}

    # 1. –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    scores['historical_matches'] = self._evaluate_historical_matches(field1, field2)

    # 2. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–∞—Å—Ç–æ—Ç–∞–º
    scores['frequency_alignment'] = self._evaluate_frequency_alignment(field1, field2)

    # 3. –°—Ö–æ–∂–µ—Å—Ç—å —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
    scores['pattern_similarity'] = self._evaluate_pattern_similarity(field1, field2)

    # 4. –ë–∞–ª–∞–Ω—Å
    scores['balance_score'] = self._evaluate_balance(field1, field2)

    # 5. –î–∏–∞–ø–∞–∑–æ–Ω —Å—É–º–º
    scores['sum_range'] = self._evaluate_sum_range(field1, field2)

    # 6. –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
    scores['uniqueness'] = self._evaluate_uniqueness(field1, field2)

    # 7. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–Ω–¥–∞–º
    scores['trend_alignment'] = self._evaluate_trend_alignment(field1, field2)

    # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
    total_fitness = sum(scores[key] * self.weights[key] for key in scores)

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (0-100+)
    total_fitness = total_fitness * 100

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
    self._fitness_cache[cache_key] = total_fitness

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
    if len(self._fitness_cache) > 10000:
      # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
      keys_to_remove = list(self._fitness_cache.keys())[:5000]
      for key in keys_to_remove:
        del self._fitness_cache[key]

    return total_fitness

  def _evaluate_historical_matches(self, field1: List[int], field2: List[int]) -> float:
    """–û—Ü–µ–Ω–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ —Ç–∏—Ä–∞–∂–∞–º–∏"""
    if self.df_history.empty:
      return 0.5

    max_matches = 0
    total_matches = 0
    recent_weight = 1.5  # –ë–æ–ª—å—à–∏–π –≤–µ—Å –Ω–µ–¥–∞–≤–Ω–∏–º —Ç–∏—Ä–∞–∂–∞–º

    for idx, row in self.df_history.head(100).iterrows():  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 —Ç–∏—Ä–∞–∂–µ–π
      f1_hist = set(row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', []))
      f2_hist = set(row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', []))

      matches_f1 = len(set(field1) & f1_hist)
      matches_f2 = len(set(field2) & f2_hist)

      # –í–∑–≤–µ—à–∏–≤–∞–µ–º –ø–æ –¥–∞–≤–Ω–æ—Å—Ç–∏
      weight = recent_weight if idx < 20 else 1.0
      match_score = (matches_f1 / self.field1_size + matches_f2 / self.field2_size) / 2
      total_matches += match_score * weight

      max_matches = max(max_matches, matches_f1 + matches_f2)

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    avg_matches = total_matches / min(100, len(self.df_history))

    # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–≤–æ–∑–º–æ–∂–Ω–æ, –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —É–∂–µ –±—ã–ª–∞)
    if max_matches == self.field1_size + self.field2_size:
      avg_matches *= 0.5

    return min(1.0, avg_matches)

  def _evaluate_frequency_alignment(self, field1: List[int], field2: List[int]) -> float:
    """–û—Ü–µ–Ω–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —á–∞—Å—Ç–æ—Ç–Ω—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º"""
    if not self.freq_field1 and not self.freq_field2:
      return 0.5

    score = 0.0

    # Field1
    if self.freq_field1:
      hot_in_combo = len(set(field1) & set(self.hot_numbers_f1))
      cold_in_combo = len(set(field1) & set(self.cold_numbers_f1))

      # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: –±–æ–ª—å—à–µ –≥–æ—Ä—è—á–∏—Ö, –º–µ–Ω—å—à–µ —Ö–æ–ª–æ–¥–Ω—ã—Ö
      hot_ratio = hot_in_combo / max(1, len(self.hot_numbers_f1))
      cold_ratio = 1 - (cold_in_combo / max(1, len(self.cold_numbers_f1)))

      score += (hot_ratio * 0.6 + cold_ratio * 0.4) * 0.5

    # Field2
    if self.freq_field2:
      hot_in_combo = len(set(field2) & set(self.hot_numbers_f2))
      cold_in_combo = len(set(field2) & set(self.cold_numbers_f2))

      hot_ratio = hot_in_combo / max(1, len(self.hot_numbers_f2))
      cold_ratio = 1 - (cold_in_combo / max(1, len(self.cold_numbers_f2)))

      score += (hot_ratio * 0.6 + cold_ratio * 0.4) * 0.5

    return score

  def _evaluate_pattern_similarity(self, field1: List[int], field2: List[int]) -> float:
    """–û—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏"""
    if not self.parity_patterns:
      return 0.5

    # –ü–∞—Ç—Ç–µ—Ä–Ω —á–µ—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—É—â–µ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
    all_numbers = field1 + field2
    even_count = sum(1 for n in all_numbers if n % 2 == 0)

    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º
    if self.parity_patterns:
      avg_even = np.mean(self.parity_patterns)
      std_even = np.std(self.parity_patterns) if len(self.parity_patterns) > 1 else 1

      # Z-score
      if std_even > 0:
        z_score = abs(even_count - avg_even) / std_even
        # –ß–µ–º –±–ª–∏–∂–µ –∫ —Å—Ä–µ–¥–Ω–µ–º—É, —Ç–µ–º –ª—É—á—à–µ
        score = max(0, 1 - z_score / 3)  # 3 —Å–∏–≥–º—ã
      else:
        score = 0.5
    else:
      score = 0.5

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    sequence_score = self._check_sequences(all_numbers)

    return (score + sequence_score) / 2

  def _check_sequences(self, numbers: List[int]) -> float:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–∏—Å–µ–ª"""
    sorted_nums = sorted(numbers)
    max_seq = 1
    current_seq = 1

    for i in range(1, len(sorted_nums)):
      if sorted_nums[i] == sorted_nums[i - 1] + 1:
        current_seq += 1
        max_seq = max(max_seq, current_seq)
      else:
        current_seq = 1

    # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    if max_seq >= 4:
      return 0.2
    elif max_seq == 3:
      return 0.5
    elif max_seq == 2:
      return 0.8
    else:
      return 1.0

  def _evaluate_balance(self, field1: List[int], field2: List[int]) -> float:
    """–û—Ü–µ–Ω–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏"""
    all_numbers = field1 + field2

    # –ë–∞–ª–∞–Ω—Å —á–µ—Ç–Ω—ã—Ö/–Ω–µ—á–µ—Ç–Ω—ã—Ö
    even_count = sum(1 for n in all_numbers if n % 2 == 0)
    even_ratio = even_count / len(all_numbers)
    parity_score = 1 - abs(0.5 - even_ratio) * 2  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ 50/50

    # –ë–∞–ª–∞–Ω—Å –º–∞–ª—ã—Ö/–±–æ–ª—å—à–∏—Ö
    mid_point1 = self.field1_max // 2
    mid_point2 = self.field2_max // 2

    small_f1 = sum(1 for n in field1 if n <= mid_point1)
    small_f2 = sum(1 for n in field2 if n <= mid_point2)

    small_ratio = (small_f1 + small_f2) / (len(field1) + len(field2))
    size_score = 1 - abs(0.5 - small_ratio) * 2

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–µ–∫–∞–¥–∞–º (–¥–ª—è field1_max=20: 1-10, 11-20)
    decades_f1 = Counter((n - 1) // 10 for n in field1)
    decades_f2 = Counter((n - 1) // 10 for n in field2)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    decade_score = 1.0
    if len(decades_f1) == 1:  # –í—Å–µ —á–∏—Å–ª–∞ –∏–∑ –æ–¥–Ω–æ–π –¥–µ–∫–∞–¥—ã
      decade_score *= 0.5
    if len(decades_f2) == 1:
      decade_score *= 0.5

    return (parity_score + size_score + decade_score) / 3

  def _evaluate_sum_range(self, field1: List[int], field2: List[int]) -> float:
    """–û—Ü–µ–Ω–∫–∞ –ø–æ–ø–∞–¥–∞–Ω–∏—è —Å—É–º–º—ã –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω"""
    sum_f1 = sum(field1)
    sum_f2 = sum(field2)

    score = 0.0

    # Field1
    if self.sum_stats_f1['std'] > 0:
      z_score_f1 = abs(sum_f1 - self.sum_stats_f1['mean']) / self.sum_stats_f1['std']
      score_f1 = max(0, 1 - z_score_f1 / 2)  # –í –ø—Ä–µ–¥–µ–ª–∞—Ö 2 —Å–∏–≥–º
    else:
      score_f1 = 0.5

    # Field2
    if self.sum_stats_f2['std'] > 0:
      z_score_f2 = abs(sum_f2 - self.sum_stats_f2['mean']) / self.sum_stats_f2['std']
      score_f2 = max(0, 1 - z_score_f2 / 2)
    else:
      score_f2 = 0.5

    return (score_f1 + score_f2) / 2

  def _evaluate_uniqueness(self, field1: List[int], field2: List[int]) -> float:
    """–û—Ü–µ–Ω–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏"""
    if self.df_history.empty:
      return 1.0

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–ª–∞—Å—å –ª–∏ —Ç–æ—á–Ω–æ —Ç–∞–∫–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
    for _, row in self.df_history.head(200).iterrows():
      f1_hist = row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', [])
      f2_hist = row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', [])

      if (isinstance(f1_hist, list) and isinstance(f2_hist, list) and
          set(field1) == set(f1_hist) and set(field2) == set(f2_hist)):
        return 0.0  # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —É–∂–µ –±—ã–ª–∞

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å –Ω–µ–¥–∞–≤–Ω–∏–º–∏
    max_similarity = 0.0
    for _, row in self.df_history.head(50).iterrows():
      f1_hist = set(row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', []))
      f2_hist = set(row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', []))

      if f1_hist and f2_hist:
        similarity = (len(set(field1) & f1_hist) + len(set(field2) & f2_hist)) / \
                     (self.field1_size + self.field2_size)
        max_similarity = max(max_similarity, similarity)

    # –ß–µ–º –º–µ–Ω—å—à–µ —Å—Ö–æ–∂–µ—Å—Ç—å, —Ç–µ–º –ª—É—á—à–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
    return 1.0 - max_similarity * 0.5

  def _evaluate_trend_alignment(self, field1: List[int], field2: List[int]) -> float:
    """–û—Ü–µ–Ω–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–µ–∫—É—â–∏–º —Ç—Ä–µ–Ω–¥–∞–º"""
    try:
      # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤
      from backend.app.core.trend_analyzer import GLOBAL_TREND_ANALYZER

      trends = GLOBAL_TREND_ANALYZER.analyze_current_trends(self.df_history)

      score = 0.0
      count = 0

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≥–æ—Ä—è—á–∏–º —á–∏—Å–ª–∞–º —Å —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º
      if 'field1' in trends:
        hot_accel = set(trends['field1'].hot_acceleration)
        if hot_accel:
          overlap = len(set(field1) & hot_accel)
          score += overlap / len(hot_accel)
          count += 1

      if 'field2' in trends:
        hot_accel = set(trends['field2'].hot_acceleration)
        if hot_accel:
          overlap = len(set(field2) & hot_accel)
          score += overlap / len(hot_accel)
          count += 1

      return score / max(1, count) if count > 0 else 0.5

    except Exception as e:
      logger.warning(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
      return 0.5

  def batch_evaluate(self, combinations: List[Tuple[List[int], List[int]]]) -> List[float]:
    """
    –ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π

    Args:
        combinations: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (field1, field2)

    Returns:
        –°–ø–∏—Å–æ–∫ fitness –∑–Ω–∞—á–µ–Ω–∏–π
    """
    results = []
    for field1, field2 in combinations:
      results.append(self.evaluate(field1, field2))

    logger.info(f"üìä –ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ {len(combinations)} –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞. "
                f"Cache hits: {self._cache_hits}, misses: {self._cache_misses}")

    return results

  def update_weights(self, new_weights: Dict[str, float]):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ fitness"""
    for key in new_weights:
      if key in self.weights:
        self.weights[key] = new_weights[key]

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
    total = sum(self.weights.values())
    if total > 0:
      for key in self.weights:
        self.weights[key] /= total

    # –û—á–∏—â–∞–µ–º –∫—ç—à –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤
    self._fitness_cache.clear()
    logger.info(f"üìä –í–µ—Å–∞ fitness –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {self.weights}")

  def get_statistics(self) -> Dict:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ—Ü–µ–Ω—â–∏–∫–∞"""
    cache_hit_rate = self._cache_hits / max(1, self._cache_hits + self._cache_misses)

    return {
      'total_evaluations': self._cache_hits + self._cache_misses,
      'cache_hits': self._cache_hits,
      'cache_misses': self._cache_misses,
      'cache_hit_rate': cache_hit_rate,
      'cache_size': len(self._fitness_cache),
      'hot_numbers_f1': self.hot_numbers_f1[:5],
      'cold_numbers_f1': self.cold_numbers_f1[:5],
      'hot_numbers_f2': self.hot_numbers_f2[:5],
      'cold_numbers_f2': self.cold_numbers_f2[:5],
      'weights': self.weights.copy()
    }

  def clear_cache(self):
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
    self._fitness_cache.clear()
    self._cache_hits = 0
    self._cache_misses = 0
    logger.info("üßπ –ö—ç—à fitness –æ—á–∏—â–µ–Ω")


class MultiObjectiveFitness(FitnessEvaluator):
  """
  –ú–Ω–æ–≥–æ–∫—Ä–∏—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è fitness —Ñ—É–Ω–∫—Ü–∏—è —Å –ü–∞—Ä–µ—Ç–æ-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
  –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏—Ö —Ü–µ–ª–µ–π
  """

  def __init__(self, df_history: pd.DataFrame, lottery_config: Dict):
    super().__init__(df_history, lottery_config)

    # –û—Ç–¥–µ–ª—å–Ω—ã–µ —Ü–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    self.objectives = {
      'maximize_hot': self._objective_maximize_hot,
      'balance_distribution': self._objective_balance,
      'historical_similarity': self._objective_historical,
      'uniqueness': self._objective_uniqueness
    }

  def evaluate_multi_objective(self, field1: List[int], field2: List[int]) -> Dict[str, float]:
    """
    –û—Ü–µ–Ω–∫–∞ –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Ü–µ–ª—è–º

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –ø–æ –∫–∞–∂–¥–æ–π —Ü–µ–ª–∏
    """
    scores = {}
    for name, func in self.objectives.items():
      scores[name] = func(field1, field2)
    return scores

  def _objective_maximize_hot(self, field1: List[int], field2: List[int]) -> float:
    """–¶–µ–ª—å: –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–æ—Ä—è—á–∏–µ —á–∏—Å–ª–∞"""
    hot_count = 0
    hot_count += len(set(field1) & set(self.hot_numbers_f1))
    hot_count += len(set(field2) & set(self.hot_numbers_f2))

    max_possible = len(self.hot_numbers_f1) + len(self.hot_numbers_f2)
    return hot_count / max(1, max_possible)

  def _objective_balance(self, field1: List[int], field2: List[int]) -> float:
    """–¶–µ–ª—å: —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"""
    return self._evaluate_balance(field1, field2)

  def _objective_historical(self, field1: List[int], field2: List[int]) -> float:
    """–¶–µ–ª—å: —Å—Ö–æ–∂–µ—Å—Ç—å —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏"""
    return self._evaluate_historical_matches(field1, field2)

  def _objective_uniqueness(self, field1: List[int], field2: List[int]) -> float:
    """–¶–µ–ª—å: —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏"""
    return self._evaluate_uniqueness(field1, field2)

  def is_pareto_optimal(self, scores: Dict[str, float],
                        population_scores: List[Dict[str, float]]) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –ü–∞—Ä–µ—Ç–æ-–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏

    Args:
        scores: –û—Ü–µ–Ω–∫–∏ —Ç–µ–∫—É—â–µ–π –æ—Å–æ–±–∏
        population_scores: –û—Ü–µ–Ω–∫–∏ –≤—Å–µ–π –ø–æ–ø—É–ª—è—Ü–∏–∏

    Returns:
        True –µ—Å–ª–∏ –æ—Å–æ–±—å –ü–∞—Ä–µ—Ç–æ-–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞
    """
    for other_scores in population_scores:
      # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
      dominates = True
      for obj in scores:
        if scores[obj] > other_scores[obj]:
          dominates = False
          break

      # –ï—Å–ª–∏ –¥—Ä—É–≥–∞—è –æ—Å–æ–±—å –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç - —Ç–µ–∫—É—â–∞—è –Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞
      if dominates and any(other_scores[obj] > scores[obj] for obj in scores):
        return False

    return True