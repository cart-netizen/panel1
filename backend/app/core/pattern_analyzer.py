# core/pattern_analyzer.py
"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –ª–æ—Ç–µ—Ä–µ–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥—É–ª—è–º–∏ data_manager –∏ utils.
"""

import numpy as np
from collections import Counter
import itertools

from backend.app.core.utils import format_numbers


class AdvancedPatternAnalyzer:
  """–ö–ª–∞—Å—Å –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏ —Ç–∏—Ä–∞–∂–µ–π"""

  def __init__(self):
    self.analysis_cache = {}

  def analyze_hot_cold_numbers(self, df_history, window_sizes=[10, 20, 50], top_n=5):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–æ—Ä—è—á–∏–µ –∏ —Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞ –≤ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–Ω–∞—Ö.

    Args:
        df_history: DataFrame —Å –∏—Å—Ç–æ—Ä–∏–µ–π (—É–∂–µ –µ—Å—Ç—å –ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list, –ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list)
        window_sizes: –†–∞–∑–º–µ—Ä—ã –æ–∫–æ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø –≥–æ—Ä—è—á–∏—Ö/—Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª

    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞ –∏ –ø–æ–ª—è
    """
    if df_history.empty:
      return {}

    results = {}

    for window in window_sizes:
      window_df = df_history.head(window) if len(df_history) >= window else df_history

      # –ê–Ω–∞–ª–∏–∑ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è
      for field_num, field_col in enumerate(['–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'], 1):
        all_numbers = []
        for nums_list in window_df[field_col].dropna():
          if isinstance(nums_list, list):
            all_numbers.extend(nums_list)

        if not all_numbers:
          continue

        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç
        freq_counter = Counter(all_numbers)
        total_draws = len(window_df)

        # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ—è–≤–ª–µ–Ω–∏—è
        freq_percentages = {
          num: (count / total_draws) * 100
          for num, count in freq_counter.items()
        }

        # –û–∂–∏–¥–∞–µ–º–∞—è —á–∞—Å—Ç–æ—Ç–∞ (—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        expected_freq = (4 / 20) * 100  # 4 —á–∏—Å–ª–∞ –∏–∑ 20 = 20%

        # –ì–æ—Ä—è—á–∏–µ —á–∏—Å–ª–∞ (–≤—ã—à–µ –æ–∂–∏–¥–∞–µ–º–æ–π —á–∞—Å—Ç–æ—Ç—ã)
        hot_numbers = sorted(
          [(num, pct) for num, pct in freq_percentages.items() if pct > expected_freq],
          key=lambda x: x[1], reverse=True
        )[:top_n]

        # –•–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞ (–≤—Å–µ —á–∏—Å–ª–∞ –æ—Ç 1 –¥–æ 20)
        all_possible_numbers = set(range(1, 21))
        appeared_numbers = set(freq_counter.keys())
        never_appeared = all_possible_numbers - appeared_numbers

        cold_numbers = []
        # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º —á–∏—Å–ª–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –≤—ã–ø–∞–¥–∞–ª–∏
        for num in never_appeared:
          cold_numbers.append((num, 0.0))

        # –ó–∞—Ç–µ–º —á–∏—Å–ª–∞ —Å –Ω–∏–∑–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π
        low_freq = sorted(
          [(num, pct) for num, pct in freq_percentages.items() if pct < expected_freq],
          key=lambda x: x[1]
        )
        cold_numbers.extend(low_freq)
        cold_numbers = cold_numbers[:top_n]

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–∑–∏—Ü–∏—è–º –¥–ª—è –≥–æ—Ä—è—á–∏—Ö —á–∏—Å–µ–ª
        position_stats = self._analyze_positions(window_df, field_col, [n[0] for n in hot_numbers])

        results[f'field{field_num}_window{window}'] = {
          'window_size': window,
          'field': field_num,
          'hot_numbers': hot_numbers,
          'cold_numbers': cold_numbers,
          'expected_frequency': expected_freq,
          'position_preferences': position_stats,
          'total_draws_analyzed': total_draws
        }

    return results

  def _analyze_positions(self, df, field_col, numbers):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö —á–∏—Å–µ–ª"""
    position_stats = {}

    for num in numbers:
      positions = []
      for nums_list in df[field_col].dropna():
        if isinstance(nums_list, list) and num in nums_list:
          positions.append(nums_list.index(num))

      if positions:
        position_counter = Counter(positions)
        total = sum(position_counter.values())
        position_stats[num] = {
          pos: (count / total) * 100
          for pos, count in position_counter.items()
        }

    return position_stats

  def find_number_correlations(self, df_history, min_support=0.02):  # ‚Üê –°–ù–ò–ñ–ï–ù –° 0.1 –î–û 0.02 (2%)
    """
    –ù–∞—Ö–æ–¥–∏—Ç —á–∏—Å–ª–∞, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –≤—ã–ø–∞–¥–∞—é—Ç –≤–º–µ—Å—Ç–µ.

    Args:
        df_history: DataFrame —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        min_support: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ (–¥–æ–ª—è —Ç–∏—Ä–∞–∂–µ–π) - –°–ù–ò–ñ–ï–ù–û –¥–æ 2%

    Returns:
        dict: –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è
    """
    results = {}

    for field_num, field_col in enumerate(['–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'], 1):
      # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä
      pair_counter = Counter()
      total_draws = 0

      print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è –ø–æ–ª—è {field_num}...")

      for nums_list in df_history[field_col].dropna():
        if isinstance(nums_list, list) and len(nums_list) >= 2:
          total_draws += 1
          # –í—Å–µ –ø–∞—Ä—ã –∏–∑ —á–∏—Å–µ–ª
          for pair in itertools.combinations(sorted(nums_list), 2):
            pair_counter[pair] += 1

      print(f"üìä –ü–æ–ª–µ {field_num}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_draws} —Ç–∏—Ä–∞–∂–µ–π, –Ω–∞–π–¥–µ–Ω–æ {len(pair_counter)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä")

      if total_draws == 0:
        print(f"‚ö†Ô∏è –ü–æ–ª–µ {field_num}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        continue

      # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 1: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
      if total_draws < 20:
        # –î–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫ - –±–µ—Ä–µ–º –ø–∞—Ä—ã –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Ö–æ—Ç—è –±—ã 1 —Ä–∞–∑
        adaptive_min_support = 1 / total_draws
      elif total_draws < 50:
        # –î–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –≤—ã–±–æ—Ä–æ–∫ - 2% –∏–ª–∏ –º–∏–Ω–∏–º—É–º 1 —Ä–∞–∑
        adaptive_min_support = max(0.02, 1 / total_draws)
      else:
        # –î–ª—è –±–æ–ª—å—à–∏—Ö –≤—ã–±–æ—Ä–æ–∫ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–¥–∞–Ω–Ω—ã–π min_support, –Ω–æ –Ω–µ –º–µ–Ω–µ–µ 1%
        adaptive_min_support = max(min_support, 0.01)

      min_count = max(1, int(total_draws * adaptive_min_support))

      print(f"üéØ –ü–æ–ª–µ {field_num}: –ø–æ—Ä–æ–≥ = {adaptive_min_support:.3f} ({min_count} –≤—Å—Ç—Ä–µ—á)")

      # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 2: –ë–æ–ª–µ–µ —É–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
      frequent_pairs = []
      for pair, count in pair_counter.items():
        frequency_percent = (count / total_draws) * 100
        if count >= min_count:
          frequent_pairs.append((pair, count, frequency_percent))

      # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
      frequent_pairs.sort(key=lambda x: x[1], reverse=True)

      # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 3: –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –ø–∞—Ä, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º —á–∏—Å–ª–æ–º
      max_pairs = min(50, len(frequent_pairs))  # –ë—ã–ª–æ 20, —Å—Ç–∞–ª–æ 50
      frequent_pairs = frequent_pairs[:max_pairs]

      print(f"‚úÖ –ü–æ–ª–µ {field_num}: –Ω–∞–π–¥–µ–Ω–æ {len(frequent_pairs)} —á–∞—Å—Ç—ã—Ö –ø–∞—Ä")
      if frequent_pairs:
        top_pair = frequent_pairs[0]
        print(f"üî• –¢–æ–ø –ø–∞—Ä–∞ –ø–æ–ª—è {field_num}: {top_pair[0]} ({top_pair[2]:.1f}%, {top_pair[1]}x)")

      # –¢–∞–∫–∂–µ –Ω–∞—Ö–æ–¥–∏–º "–∞–Ω—Ç–∏–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏" - —á–∏—Å–ª–∞, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–¥–∫–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ
      all_pairs = set(itertools.combinations(range(1, 21), 2))
      appeared_pairs = set(pair_counter.keys())
      never_together = list(all_pairs - appeared_pairs)[:10]  # –¢–æ–ø-10

      results[f'field{field_num}'] = {
        'frequent_pairs': frequent_pairs,
        'never_together': never_together,
        'total_draws': total_draws,
        'total_unique_pairs': len(pair_counter),
        'min_count_used': min_count,
        'adaptive_threshold': adaptive_min_support
      }

    return results

  def analyze_draw_cycles(self, df_history, numbers_to_analyze=None):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ü–∏–∫–ª—ã –≤—ã–ø–∞–¥–µ–Ω–∏—è —á–∏—Å–µ–ª (—á–µ—Ä–µ–∑ —Å–∫–æ–ª—å–∫–æ —Ç–∏—Ä–∞–∂–µ–π –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è).

    Args:
        df_history: DataFrame —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        numbers_to_analyze: –°–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª –∏–ª–∏ None –¥–ª—è –≤—Å–µ—Ö

    Returns:
        dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–∏–∫–ª–æ–≤
    """
    if numbers_to_analyze is None:
      numbers_to_analyze = list(range(1, 21))

    cycles_data = {}

    for field_num, field_col in enumerate(['–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'], 1):
      field_cycles = {}

      for number in numbers_to_analyze:
        appearances = []

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø–æ–∑–∏—Ü–∏–∏ (–∏–Ω–¥–µ–∫—Å—ã), –≥–¥–µ –ø–æ—è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ
        for idx, nums_list in enumerate(df_history[field_col]):
          if isinstance(nums_list, list) and number in nums_list:
            appearances.append(idx)

        if len(appearances) >= 2:
          # –í—ã—á–∏—Å–ª—è–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –º–µ–∂–¥—É –ø–æ—è–≤–ª–µ–Ω–∏—è–º–∏
          intervals = []
          for i in range(1, len(appearances)):
            interval = appearances[i] - appearances[i - 1]
            intervals.append(interval)

          if intervals:
            field_cycles[number] = {
              'appearances': len(appearances),
              'avg_cycle': np.mean(intervals),
              'min_cycle': min(intervals),
              'max_cycle': max(intervals),
              'std_cycle': np.std(intervals) if len(intervals) > 1 else 0,
              'last_seen': appearances[0],  # –°–∫–æ–ª—å–∫–æ —Ç–∏—Ä–∞–∂–µ–π –Ω–∞–∑–∞–¥
              'overdue': appearances[0] > np.mean(intervals) if intervals else False
            }

      cycles_data[f'field{field_num}'] = field_cycles

    return cycles_data

  def detect_anomalies(self, df_history, zscore_threshold=2.5):
    """
    –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ —Ç–∏—Ä–∞–∂–∞—Ö.

    Args:
        df_history: DataFrame —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        zscore_threshold: –ü–æ—Ä–æ–≥ Z-score –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–∏

    Returns:
        dict: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–Ω–æ–º–∞–ª–∏—è—Ö
    """
    anomalies = {
      'unusual_sums': [],
      'unusual_spreads': [],
      'unusual_patterns': [],
      'consecutive_numbers': [],
      'all_even_odd': []
    }

    for idx, row in df_history.iterrows():
      draw_num = row.get('–¢–∏—Ä–∞–∂', idx)

      for field_num, field_col in enumerate(['–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'], 1):
        nums = row[field_col]
        if not isinstance(nums, list) or len(nums) != 4:
          continue

        # –ê–Ω–∞–ª–∏–∑ —Å—É–º–º—ã
        sum_nums = sum(nums)
        # –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã: –º–∏–Ω 10 (1+2+3+4), –º–∞–∫—Å 74 (17+18+19+20)
        # –°—Ä–µ–¥–Ω–µ–µ –æ–∂–∏–¥–∞–µ–º–æ–µ: 42
        if sum_nums < 20 or sum_nums > 64:  # –ü—Ä–∏–º–µ—Ä–Ω–æ 2.5 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
          anomalies['unusual_sums'].append({
            'draw': draw_num,
            'field': field_num,
            'sum': sum_nums,
            'numbers': format_numbers(nums)
          })

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–∞—Ö–∞
        spread = max(nums) - min(nums)
        if spread < 5 or spread > 18:  # –û—á–µ–Ω—å —É–∑–∫–∏–π –∏–ª–∏ —à–∏—Ä–æ–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω
          anomalies['unusual_spreads'].append({
            'draw': draw_num,
            'field': field_num,
            'spread': spread,
            'numbers': format_numbers(nums)
          })

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —á–∏—Å–ª–∞
        sorted_nums = sorted(nums)
        consecutive_count = 1
        for i in range(1, len(sorted_nums)):
          if sorted_nums[i] == sorted_nums[i - 1] + 1:
            consecutive_count += 1
          else:
            if consecutive_count >= 3:
              break
            consecutive_count = 1

        if consecutive_count >= 3:
          anomalies['consecutive_numbers'].append({
            'draw': draw_num,
            'field': field_num,
            'consecutive': consecutive_count,
            'numbers': format_numbers(nums)
          })

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Å–µ —á–µ—Ç–Ω—ã–µ –∏–ª–∏ –≤—Å–µ –Ω–µ—á–µ—Ç–Ω—ã–µ
        even_count = sum(1 for n in nums if n % 2 == 0)
        if even_count == 0 or even_count == 4:
          anomalies['all_even_odd'].append({
            'draw': draw_num,
            'field': field_num,
            'type': 'all_even' if even_count == 4 else 'all_odd',
            'numbers': format_numbers(nums)
          })

    # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –∞–Ω–æ–º–∞–ª–∏–π
    total_draws = len(df_history)
    anomaly_stats = {
      key: {
        'count': len(values),
        'percentage': (len(values) / total_draws) * 100 if total_draws > 0 else 0,
        'examples': values[:5]  # –ü–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤
      }
      for key, values in anomalies.items()
    }

    return anomaly_stats

  def get_smart_filters(self, df_history, risk_level='medium'):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–º–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è combination_generator –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞.

    Args:
        df_history: DataFrame —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        risk_level: 'conservative', 'medium', 'aggressive'

    Returns:
        dict: –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
    """
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 50 —Ç–∏—Ä–∞–∂–µ–π –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤
    recent_df = df_history.head(50)

    filters = {}

    # –ê–Ω–∞–ª–∏–∑ —Å—É–º–º
    for field_num, field_col in enumerate(['–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list'], 1):
      sums = []
      for nums in recent_df[field_col].dropna():
        if isinstance(nums, list):
          sums.append(sum(nums))

      if sums:
        mean_sum = np.mean(sums)
        std_sum = np.std(sums)

        if risk_level == 'conservative':
          # –£–∑–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω –æ–∫–æ–ª–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
          filters[f'sum_f{field_num}_min'] = int(mean_sum - std_sum)
          filters[f'sum_f{field_num}_max'] = int(mean_sum + std_sum)
        elif risk_level == 'medium':
          # –°—Ä–µ–¥–Ω–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω
          filters[f'sum_f{field_num}_min'] = int(mean_sum - 1.5 * std_sum)
          filters[f'sum_f{field_num}_max'] = int(mean_sum + 1.5 * std_sum)
        else:  # aggressive
          # –®–∏—Ä–æ–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω
          filters[f'sum_f{field_num}_min'] = max(10, int(mean_sum - 2 * std_sum))
          filters[f'sum_f{field_num}_max'] = min(74, int(mean_sum + 2 * std_sum))

    # –ü–æ–ª—É—á–∞–µ–º –≥–æ—Ä—è—á–∏–µ –∏ —Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞
    hot_cold = self.analyze_hot_cold_numbers(df_history, window_sizes=[20], top_n=3)

    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≥–æ—Ä—è—á–∏–º/—Ö–æ–ª–æ–¥–Ω—ã–º —á–∏—Å–ª–∞–º
    for field_data in hot_cold.values():
      field_num = field_data['field']
      if risk_level == 'conservative':
        # –í–∫–ª—é—á–∞–µ–º —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –≥–æ—Ä—è—á–µ–µ —á–∏—Å–ª–æ
        if field_data['hot_numbers']:
          filters[f'include_hot_f{field_num}'] = [n[0] for n in field_data['hot_numbers'][:1]]
      elif risk_level == 'aggressive':
        # –í–∫–ª—é—á–∞–µ–º —Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞ (–æ–Ω–∏ "–¥–æ–ª–∂–Ω—ã" –≤—ã–ø–∞—Å—Ç—å)
        if field_data['cold_numbers']:
          filters[f'include_cold_f{field_num}'] = [n[0] for n in field_data['cold_numbers'][:2]]

    return filters


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
GLOBAL_PATTERN_ANALYZER = AdvancedPatternAnalyzer()