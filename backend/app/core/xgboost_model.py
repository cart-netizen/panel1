"""
XGBoost –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ª–æ—Ç–µ—Ä–µ–π
–í–∫–ª—é—á–∞–µ—Ç async –æ–±—É—á–µ–Ω–∏–µ, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å SHAP
"""

import asyncio
import hashlib
import pickle
import threading
import time
from typing import List, Tuple, Optional, Dict, Any
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support
import shap
import logging

logger = logging.getLogger(__name__)


class XGBoostLotteryModel:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è XGBoost –º–æ–¥–µ–ª—å —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ—Ç–µ—Ä–µ–π
    –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Random Forest –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
    """

    def __init__(self, lottery_config: dict):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è XGBoost –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ª–æ—Ç–µ—Ä–µ–∏
        
        Args:
            lottery_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ—Ç–µ—Ä–µ–∏ –∏–∑ LOTTERY_CONFIGS
        """
        self.config = lottery_config
        self.field1_size = lottery_config['field1_size']
        self.field2_size = lottery_config['field2_size']
        self.field1_max = lottery_config['field1_max']
        self.field2_max = lottery_config['field2_max']
        
        # XGBoost –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è
        self.models_f1 = []  # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è field1
        self.models_f2 = []  # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è field2
        
        # SHAP –æ–±—ä—è—Å–Ω–∏—Ç–µ–ª–∏
        self.explainers_f1 = []
        self.explainers_f2 = []
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –ª–æ—Ç–µ—Ä–µ–π)
        self.xgb_params = {
            'objective': 'binary:logistic',
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 200,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'gamma': 0.1,
            'reg_alpha': 0.1,
            'reg_lambda': 1,
            'random_state': 42,
            'n_jobs': -1,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —è–¥—Ä–∞
            'tree_method': 'hist',  # –ë—ã—Å—Ç—Ä—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            'predictor': 'cpu_predictor',
            'enable_categorical': False
        }
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.metrics = {
            'accuracy': [],
            'roc_auc': [],
            'precision': [],
            'recall': [],
            'f1_score': [],
            'feature_importance': {},
            'training_time': 0,
            'prediction_count': 0
        }
        
        # –°—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è
        self.is_trained = False
        self._lock = threading.Lock()
        
        # –ö—ç—à –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        self._prediction_cache = {}
        self._cache_hits = 0
        self._cache_misses = 0
        
        logger.info(f"‚úÖ XGBoost –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –ª–æ—Ç–µ—Ä–µ–∏ {lottery_config.get('name', 'unknown')}")

    def _extract_features(self, df_history: pd.DataFrame) -> Tuple[np.ndarray, Dict[int, np.ndarray]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Ç–∏—Ä–∞–∂–µ–π
        
        Returns:
            X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            y: –°–ª–æ–≤–∞—Ä—å —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö {–Ω–æ–º–µ—Ä_—á–∏—Å–ª–∞: –±–∏–Ω–∞—Ä–Ω—ã–π_–≤–µ–∫—Ç–æ—Ä}
        """
        if df_history.empty:
            return np.array([]), {}
            
        features_list = []
        targets_f1 = {i: [] for i in range(1, self.field1_max + 1)}
        targets_f2 = {i: [] for i in range(1, self.field2_max + 1)}
        
        # –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        window_sizes = [3, 5, 10, 20]
        
        for idx in range(len(df_history) - max(window_sizes)):
            feature_vector = []
            
            # –¢–µ–∫—É—â–∏–π —Ç–∏—Ä–∞–∂ –¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            current_draw = df_history.iloc[idx]
            current_f1 = current_draw.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', [])
            current_f2 = current_draw.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', [])
            
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –æ–∫–æ–Ω
            for window in window_sizes:
                window_data = df_history.iloc[idx + 1:idx + 1 + window]
                
                if len(window_data) < window:
                    continue
                    
                # –ß–∞—Å—Ç–æ—Ç—ã —á–∏—Å–µ–ª –≤ –æ–∫–Ω–µ
                freq_f1 = np.zeros(self.field1_max)
                freq_f2 = np.zeros(self.field2_max)
                
                for _, row in window_data.iterrows():
                    for num in row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', []):
                        if 1 <= num <= self.field1_max:
                            freq_f1[num - 1] += 1
                    for num in row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', []):
                        if 1 <= num <= self.field2_max:
                            freq_f2[num - 1] += 1
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Å—Ç–æ—Ç
                freq_f1 = freq_f1 / window
                freq_f2 = freq_f2 / window
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                feature_vector.extend(freq_f1)
                feature_vector.extend(freq_f2)
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ–∫–Ω–∞
                all_f1_in_window = []
                all_f2_in_window = []
                
                for _, row in window_data.iterrows():
                    all_f1_in_window.extend(row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', []))
                    all_f2_in_window.extend(row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', []))
                
                if all_f1_in_window:
                    feature_vector.extend([
                        np.mean(all_f1_in_window),
                        np.std(all_f1_in_window),
                        np.median(all_f1_in_window),
                        len(set(all_f1_in_window)) / self.field1_max  # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
                    ])
                else:
                    feature_vector.extend([0, 0, 0, 0])
                    
                if all_f2_in_window:
                    feature_vector.extend([
                        np.mean(all_f2_in_window),
                        np.std(all_f2_in_window),
                        np.median(all_f2_in_window),
                        len(set(all_f2_in_window)) / self.field2_max  # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
                    ])
                else:
                    feature_vector.extend([0, 0, 0, 0])
            
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ "–≥–æ—Ä—è—á–∏—Ö" –∏ "—Ö–æ–ª–æ–¥–Ω—ã—Ö" —á–∏—Å–µ–ª
            last_appearance_f1 = np.full(self.field1_max, 100)  # –ë–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –Ω–µ–≤–∏–¥–∞–Ω–Ω—ã—Ö
            last_appearance_f2 = np.full(self.field2_max, 100)
            
            for look_back in range(1, min(50, len(df_history) - idx)):
                past_draw = df_history.iloc[idx + look_back]
                for num in past_draw.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', []):
                    if 1 <= num <= self.field1_max and last_appearance_f1[num - 1] == 100:
                        last_appearance_f1[num - 1] = look_back
                for num in past_draw.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', []):
                    if 1 <= num <= self.field2_max and last_appearance_f2[num - 1] == 100:
                        last_appearance_f2[num - 1] = look_back
            
            feature_vector.extend(last_appearance_f1)
            feature_vector.extend(last_appearance_f2)
            
            # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Å–∏–Ω—É—Å/–∫–æ—Å–∏–Ω—É—Å –Ω–æ–º–µ—Ä–∞ —Ç–∏—Ä–∞–∂–∞)
            draw_number = current_draw.get('–¢–∏—Ä–∞–∂', 0)
            feature_vector.extend([
                np.sin(2 * np.pi * draw_number / 365),  # –ì–æ–¥–æ–≤–æ–π —Ü–∏–∫–ª
                np.cos(2 * np.pi * draw_number / 365),
                np.sin(2 * np.pi * draw_number / 30),   # –ú–µ—Å—è—á–Ω—ã–π —Ü–∏–∫–ª
                np.cos(2 * np.pi * draw_number / 30),
                np.sin(2 * np.pi * draw_number / 7),    # –ù–µ–¥–µ–ª—å–Ω—ã–π —Ü–∏–∫–ª
                np.cos(2 * np.pi * draw_number / 7)
            ])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features_list.append(feature_vector)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            for num in range(1, self.field1_max + 1):
                targets_f1[num].append(1 if num in current_f1 else 0)
            for num in range(1, self.field2_max + 1):
                targets_f2[num].append(1 if num in current_f2 else 0)
        
        if not features_list:
            return np.array([]), {}
            
        X = np.array(features_list)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        all_targets = {}
        all_targets.update({f'f1_{num}': np.array(targets_f1[num]) for num in targets_f1})
        all_targets.update({f'f2_{num}': np.array(targets_f2[num]) for num in targets_f2})
        
        return X, all_targets

    def train(self, df_history: pd.DataFrame) -> bool:
        """
        –û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–µ–π —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        """
        if df_history.empty or len(df_history) < 50:
            logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è XGBoost: {len(df_history)} —Ç–∏—Ä–∞–∂–µ–π")
            return False
            
        with self._lock:
            try:
                start_time = time.time()
                logger.info(f"üéì –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è XGBoost –Ω–∞ {len(df_history)} —Ç–∏—Ä–∞–∂–∞—Ö...")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                X, y_dict = self._extract_features(df_history)
                
                if X.shape[0] == 0:
                    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏")
                    return False
                
                # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –º–æ–¥–µ–ª–∏
                self.models_f1 = []
                self.models_f2 = []
                self.explainers_f1 = []
                self.explainers_f2 = []
                
                # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∏—Å–ª–∞ field1
                logger.info(f"üìö –û–±—É—á–µ–Ω–∏–µ {self.field1_max} –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ª—è 1...")
                for num in range(1, self.field1_max + 1):
                    y = y_dict[f'f1_{num}']
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
                    if np.sum(y) < 5 or np.sum(y) > len(y) - 5:
                        logger.warning(f"–ü—Ä–æ–ø—É—Å–∫ —á–∏—Å–ª–∞ {num} –ø–æ–ª—è 1 - –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤")
                        self.models_f1.append(None)
                        self.explainers_f1.append(None)
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
                    model = xgb.XGBClassifier(**self.xgb_params)
                    
                    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
                    tscv = TimeSeriesSplit(n_splits=3)
                    scores = cross_val_score(model, X, y, cv=tscv, scoring='roc_auc')
                    
                    # –û–±—É—á–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
                    model.fit(X, y)
                    
                    # –°–æ–∑–¥–∞–µ–º SHAP explainer
                    explainer = shap.TreeExplainer(model)
                    
                    self.models_f1.append(model)
                    self.explainers_f1.append(explainer)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    self.metrics['roc_auc'].append(np.mean(scores))
                
                # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∏—Å–ª–∞ field2
                logger.info(f"üìö –û–±—É—á–µ–Ω–∏–µ {self.field2_max} –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ª—è 2...")
                for num in range(1, self.field2_max + 1):
                    y = y_dict[f'f2_{num}']
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
                    if np.sum(y) < 5 or np.sum(y) > len(y) - 5:
                        logger.warning(f"–ü—Ä–æ–ø—É—Å–∫ —á–∏—Å–ª–∞ {num} –ø–æ–ª—è 2 - –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤")
                        self.models_f2.append(None)
                        self.explainers_f2.append(None)
                        continue
                    
                    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
                    model = xgb.XGBClassifier(**self.xgb_params)
                    
                    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
                    tscv = TimeSeriesSplit(n_splits=3)
                    scores = cross_val_score(model, X, y, cv=tscv, scoring='roc_auc')
                    
                    # –û–±—É—á–∞–µ–º –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
                    model.fit(X, y)
                    
                    # –°–æ–∑–¥–∞–µ–º SHAP explainer
                    explainer = shap.TreeExplainer(model)
                    
                    self.models_f2.append(model)
                    self.explainers_f2.append(explainer)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    self.metrics['roc_auc'].append(np.mean(scores))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏
                if self.models_f1 and self.models_f1[0] is not None:
                    self.metrics['feature_importance'] = dict(zip(
                        [f'feature_{i}' for i in range(X.shape[1])],
                        self.models_f1[0].feature_importances_
                    ))
                
                self.metrics['training_time'] = time.time() - start_time
                self.is_trained = True
                
                avg_roc_auc = np.mean(self.metrics['roc_auc']) if self.metrics['roc_auc'] else 0
                logger.info(f"‚úÖ XGBoost –æ–±—É—á–µ–Ω –∑–∞ {self.metrics['training_time']:.2f}—Å, "
                          f"—Å—Ä–µ–¥–Ω–∏–π ROC-AUC: {avg_roc_auc:.3f}")
                
                return True
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è XGBoost: {e}")
                import traceback
                traceback.print_exc()
                self.is_trained = False
                return False

    async def train_async(self, df_history: pd.DataFrame) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.train, df_history)

    def predict_next_combination(self, last_f1: List[int], last_f2: List[int], 
                                 df_history: pd.DataFrame) -> Tuple[List[int], List[int]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–∏—Ä–∞–∂–∞
        
        Returns:
            Tuple[field1_numbers, field2_numbers]
        """
        if not self.is_trained:
            logger.warning("XGBoost –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return [], []
        
        # –°–æ–∑–¥–∞–µ–º –∫—ç—à-–∫–ª—é—á
        cache_key = f"{sorted(last_f1)}_{sorted(last_f2)}_{len(df_history)}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if cache_key in self._prediction_cache:
            self._cache_hits += 1
            return self._prediction_cache[cache_key]
        
        self._cache_misses += 1
        
        with self._lock:
            try:
                # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º —Ç–∏—Ä–∞–∂–æ–º
                test_df = pd.DataFrame([{
                    '–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list': last_f1,
                    '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list': last_f2,
                    '–¢–∏—Ä–∞–∂': len(df_history) + 1
                }])
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                test_df = pd.concat([test_df] + [df_history.head(50)], ignore_index=True)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                X_test, _ = self._extract_features(test_df)
                
                if X_test.shape[0] == 0:
                    return [], []
                
                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                X_pred = X_test[-1:] if len(X_test) > 0 else X_test
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è field1
                probs_f1 = []
                for i, model in enumerate(self.models_f1):
                    if model is not None:
                        prob = model.predict_proba(X_pred)[0, 1]
                        probs_f1.append((i + 1, prob))
                    else:
                        probs_f1.append((i + 1, 0.05))  # –ë–∞–∑–æ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è field2
                probs_f2 = []
                for i, model in enumerate(self.models_f2):
                    if model is not None:
                        prob = model.predict_proba(X_pred)[0, 1]
                        probs_f2.append((i + 1, prob))
                    else:
                        probs_f2.append((i + 1, 0.05))  # –ë–∞–∑–æ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø
                probs_f1.sort(key=lambda x: x[1], reverse=True)
                probs_f2.sort(key=lambda x: x[1], reverse=True)
                
                # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
                pred_f1 = [num for num, _ in probs_f1[:self.field1_size]]
                pred_f2 = [num for num, _ in probs_f2[:self.field2_size]]
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                result = (sorted(pred_f1), sorted(pred_f2))
                self._prediction_cache[cache_key] = result
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
                if len(self._prediction_cache) > 1000:
                    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
                    keys = list(self._prediction_cache.keys())
                    for key in keys[:500]:
                        del self._prediction_cache[key]
                
                self.metrics['prediction_count'] += 1
                
                return result
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è XGBoost: {e}")
                return [], []

    def score_combination(self, field1: List[int], field2: List[int], 
                         df_history: pd.DataFrame) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        Returns:
            –û—Ü–µ–Ω–∫–∞ –æ—Ç 0 –¥–æ 100
        """
        if not self.is_trained:
            return 50.0  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        
        with self._lock:
            try:
                # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
                test_df = pd.DataFrame([{
                    '–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list': field1,
                    '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list': field2,
                    '–¢–∏—Ä–∞–∂': len(df_history) + 1
                }])
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
                test_df = pd.concat([test_df] + [df_history.head(50)], ignore_index=True)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                X_test, _ = self._extract_features(test_df)
                
                if X_test.shape[0] == 0:
                    return 50.0
                
                X_pred = X_test[-1:] if len(X_test) > 0 else X_test
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —á–∏—Å–µ–ª
                score_f1 = []
                for num in field1:
                    if 1 <= num <= self.field1_max and self.models_f1[num - 1] is not None:
                        prob = self.models_f1[num - 1].predict_proba(X_pred)[0, 1]
                        score_f1.append(prob)
                
                score_f2 = []
                for num in field2:
                    if 1 <= num <= self.field2_max and self.models_f2[num - 1] is not None:
                        prob = self.models_f2[num - 1].predict_proba(X_pred)[0, 1]
                        score_f2.append(prob)
                
                # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                if score_f1 and score_f2:
                    avg_score = (np.mean(score_f1) + np.mean(score_f2)) / 2
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —à–∫–∞–ª—É 0-100
                    return min(max(avg_score * 100, 0), 100)
                else:
                    return 50.0
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ XGBoost: {e}")
                return 50.0

    def get_feature_importance(self, field_type: str = 'field1', number: int = 1) -> Dict[str, float]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∏—Å–ª–∞
        
        Args:
            field_type: 'field1' –∏–ª–∏ 'field2'
            number: –ù–æ–º–µ—Ä —á–∏—Å–ª–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–Ω–∞–∑–≤–∞–Ω–∏–µ_–ø—Ä–∏–∑–Ω–∞–∫–∞: –≤–∞–∂–Ω–æ—Å—Ç—å}
        """
        if not self.is_trained:
            return {}
        
        with self._lock:
            try:
                if field_type == 'field1' and 1 <= number <= self.field1_max:
                    model = self.models_f1[number - 1]
                elif field_type == 'field2' and 1 <= number <= self.field2_max:
                    model = self.models_f2[number - 1]
                else:
                    return {}
                
                if model is None:
                    return {}
                
                # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                importance = model.feature_importances_
                
                # –°–æ–∑–¥–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                feature_names = self._generate_feature_names()
                
                # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
                importance_dict = dict(zip(feature_names, importance))
                sorted_importance = dict(sorted(importance_dict.items(), 
                                              key=lambda x: x[1], reverse=True))
                
                return sorted_importance
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
                return {}

    def _generate_feature_names(self) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–∏—Ç–∞–µ–º—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        names = []
        window_sizes = [3, 5, 10, 20]
        
        for window in window_sizes:
            # –ß–∞—Å—Ç–æ—Ç—ã –¥–ª—è field1
            for i in range(1, self.field1_max + 1):
                names.append(f'freq_f1_{i}_win{window}')
            # –ß–∞—Å—Ç–æ—Ç—ã –¥–ª—è field2
            for i in range(1, self.field2_max + 1):
                names.append(f'freq_f2_{i}_win{window}')
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            names.extend([
                f'mean_f1_win{window}', f'std_f1_win{window}',
                f'median_f1_win{window}', f'diversity_f1_win{window}',
                f'mean_f2_win{window}', f'std_f2_win{window}',
                f'median_f2_win{window}', f'diversity_f2_win{window}'
            ])
        
        # –ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ
        for i in range(1, self.field1_max + 1):
            names.append(f'last_appear_f1_{i}')
        for i in range(1, self.field2_max + 1):
            names.append(f'last_appear_f2_{i}')
        
        # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        names.extend([
            'sin_year', 'cos_year', 
            'sin_month', 'cos_month',
            'sin_week', 'cos_week'
        ])
        
        return names

    def get_shap_explanation(self, field1: List[int], field2: List[int], 
                            df_history: pd.DataFrame) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ SHAP –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –¥–ª—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å SHAP –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º–∏
        """
        if not self.is_trained or not self.explainers_f1:
            return {'error': '–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞ –∏–ª–∏ SHAP –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω'}
        
        with self._lock:
            try:
                # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
                test_df = pd.DataFrame([{
                    '–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list': field1,
                    '–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list': field2,
                    '–¢–∏—Ä–∞–∂': len(df_history) + 1
                }])
                
                test_df = pd.concat([test_df] + [df_history.head(50)], ignore_index=True)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                X_test, _ = self._extract_features(test_df)
                
                if X_test.shape[0] == 0:
                    return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏'}
                
                X_pred = X_test[-1:] if len(X_test) > 0 else X_test
                
                # –ü–æ–ª—É—á–∞–µ–º SHAP –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —á–∏—Å–ª–∞ –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è
                shap_values_f1 = []
                shap_values_f2 = []
                
                # –î–ª—è field1
                for num in field1[:1]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ —á–∏—Å–ª–æ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                    if 1 <= num <= self.field1_max and self.explainers_f1[num - 1] is not None:
                        shap_vals = self.explainers_f1[num - 1].shap_values(X_pred)
                        shap_values_f1.append({
                            'number': num,
                            'shap_values': shap_vals[0].tolist() if len(shap_vals) > 0 else [],
                            'base_value': float(self.explainers_f1[num - 1].expected_value)
                        })
                
                # –î–ª—è field2
                for num in field2[:1]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ —á–∏—Å–ª–æ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
                    if 1 <= num <= self.field2_max and self.explainers_f2[num - 1] is not None:
                        shap_vals = self.explainers_f2[num - 1].shap_values(X_pred)
                        shap_values_f2.append({
                            'number': num,
                            'shap_values': shap_vals[0].tolist() if len(shap_vals) > 0 else [],
                            'base_value': float(self.explainers_f2[num - 1].expected_value)
                        })
                
                # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                feature_names = self._generate_feature_names()
                
                # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                top_features = []
                if shap_values_f1 and shap_values_f1[0]['shap_values']:
                    shap_abs = np.abs(shap_values_f1[0]['shap_values'])
                    top_indices = np.argsort(shap_abs)[-10:][::-1]
                    for idx in top_indices:
                        if idx < len(feature_names):
                            top_features.append({
                                'name': feature_names[idx],
                                'value': float(X_pred[0, idx]) if X_pred.shape[1] > idx else 0,
                                'shap_value': float(shap_values_f1[0]['shap_values'][idx])
                            })
                
                return {
                    'field1_explanations': shap_values_f1,
                    'field2_explanations': shap_values_f2,
                    'feature_names': feature_names[:20],  # –ü–µ—Ä–≤—ã–µ 20 –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏
                    'top_important_features': top_features,
                    'prediction_score': self.score_combination(field1, field2, df_history)
                }
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ SHAP –æ–±—ä—è—Å–Ω–µ–Ω–∏—è: {e}")
                import traceback
                traceback.print_exc()
                return {'error': str(e)}

    def get_metrics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        metrics = self.metrics.copy()
        metrics['cache_hit_rate'] = (self._cache_hits / max(1, self._cache_hits + self._cache_misses)) * 100
        metrics['total_predictions'] = self._cache_hits + self._cache_misses
        return metrics

    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        with self._lock:
            self._prediction_cache.clear()
            self._cache_hits = 0
            self._cache_misses = 0
            logger.info("–ö—ç—à XGBoost –æ—á–∏—â–µ–Ω")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä XGBoost –º–æ–¥–µ–ª–µ–π (—Å–∏–Ω–≥–ª—Ç–æ–Ω)
class XGBoostModelManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ XGBoost –º–æ–¥–µ–ª—è–º–∏ —Ä–∞–∑–Ω—ã—Ö –ª–æ—Ç–µ—Ä–µ–π"""
    
    def __init__(self):
        self._models = {}
        self._lock = threading.Lock()
        
    def get_model(self, lottery_type: str, lottery_config: dict) -> XGBoostLotteryModel:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ª–æ—Ç–µ—Ä–µ–∏"""
        with self._lock:
            if lottery_type not in self._models:
                self._models[lottery_type] = XGBoostLotteryModel(lottery_config)
                logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è XGBoost –º–æ–¥–µ–ª—å –¥–ª—è {lottery_type}")
            return self._models[lottery_type]
    
    def train_all_models(self, lottery_configs: dict, data_fetcher_func):
        """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        for lottery_type, config in lottery_configs.items():
            try:
                logger.info(f"–û–±—É—á–µ–Ω–∏–µ XGBoost –¥–ª—è {lottery_type}")
                from backend.app.core.lottery_context import LotteryContext
                
                with LotteryContext(lottery_type):
                    df = data_fetcher_func()
                    
                if not df.empty:
                    model = self.get_model(lottery_type, config)
                    model.train(df)
                    logger.info(f"XGBoost –¥–ª—è {lottery_type} –æ–±—É—á–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                else:
                    logger.warning(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è XGBoost {lottery_type}")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è XGBoost –¥–ª—è {lottery_type}: {e}")
    
    def get_all_metrics(self) -> Dict[str, Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        with self._lock:
            return {
                lottery_type: model.get_metrics() 
                for lottery_type, model in self._models.items()
            }
    
    def clear_all_caches(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        with self._lock:
            for model in self._models.values():
                model.clear_cache()


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞
GLOBAL_XGBOOST_MANAGER = XGBoostModelManager()
