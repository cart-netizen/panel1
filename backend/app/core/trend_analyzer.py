"""
–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –±–æ—Ç–∞
–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
"""
import time
import threading
from datetime import datetime, timedelta
from collections import Counter, deque
from typing import Dict, List, Tuple, Optional
import numpy as np
import pandas as pd
from dataclasses import dataclass


@dataclass
class TrendMetrics:
  """–ú–µ—Ç—Ä–∏–∫–∏ —Ç–µ–∫—É—â–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤"""
  hot_acceleration: List[int]  # –ß–∏—Å–ª–∞ —Å –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–π —á–∞—Å—Ç–æ—Ç–æ–π
  cold_reversal: List[int]  # –•–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞ –≥–æ—Ç–æ–≤—ã–µ –∫ –≤—ã—Ö–æ–¥—É
  momentum_numbers: List[int]  # –ß–∏—Å–ª–∞ —Å –∏–º–ø—É–ª—å—Å–æ–º
  pattern_shift: str  # –°–¥–≤–∏–≥ –ø–∞—Ç—Ç–µ—Ä–Ω–∞: 'stable', 'ascending', 'descending'
  confidence_score: float  # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ç—Ä–µ–Ω–¥–µ (0-1)
  trend_strength: float  # –°–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞ (0-1)


@dataclass
class CombinationMetrics:
  """–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏"""
  trend_alignment: float  # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–Ω–¥—É (0-1)
  momentum_score: float  # –ò–º–ø—É–ª—å—Å –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ (0-1)
  pattern_resonance: float  # –†–µ–∑–æ–Ω–∞–Ω—Å —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º (0-1)
  risk_assessment: str  # 'low', 'medium', 'high'
  expected_performance: float  # –û–∂–∏–¥–∞–µ–º–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (0-1)


class DynamicTrendAnalyzer:
  """
  –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π –∏–≥—Ä—ã
  –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥—ã –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
  """

  def __init__(self, max_cache_size=500):
    self.trend_cache = {}
    self.pattern_memory = deque(maxlen=100)  # –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 100 –∞–Ω–∞–ª–∏–∑–æ–≤
    self.lock = threading.Lock()

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞
    self.micro_trend_window = 10  # –û–∫–Ω–æ –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤
    self.macro_trend_window = 50  # –û–∫–Ω–æ –º–∞–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤
    self.momentum_threshold = 0.6  # –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–º–ø—É–ª—å—Å–∞

    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    self.adaptive_weights = {
      'recent_bias': 0.7,  # –í–µ—Å –Ω–µ–¥–∞–≤–Ω–∏—Ö —Å–æ–±—ã—Ç–∏–π
      'pattern_stability': 0.3,  # –í–µ—Å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
      'momentum_factor': 0.8,  # –§–∞–∫—Ç–æ—Ä –∏–º–ø—É–ª—å—Å–∞
      'reversal_sensitivity': 0.5  # –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ —Ä–∞–∑–≤–æ—Ä–æ—Ç–∞–º
    }

  def analyze_current_trends(self, df_history: pd.DataFrame) -> Dict[str, TrendMetrics]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—É—â–∏–µ —Ç—Ä–µ–Ω–¥—ã –¥–ª—è –æ–±–æ–∏—Ö –ø–æ–ª–µ–π

    Returns:
        Dict —Å –∫–ª—é—á–∞–º–∏ 'field1', 'field2', —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ TrendMetrics
    """
    if df_history.empty:
      return self._get_default_trends()

    with self.lock:
      trends = {}

      for field_num in [1, 2]:
        field_col = f'–ß–∏—Å–ª–∞_–ü–æ–ª–µ{field_num}_list'
        if field_col not in df_history.columns:
          continue

        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç—Ä–µ–Ω–¥–∞ –¥–ª—è –ø–æ–ª—è
        trend_metrics = self._analyze_field_trends(
          df_history, field_col, field_num
        )
        trends[f'field{field_num}'] = trend_metrics

      # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
      self.pattern_memory.append({
        'timestamp': datetime.now(),
        'trends': trends,
        'data_signature': self._calculate_data_signature(df_history)
      })

      return trends

  def _analyze_field_trends(self, df_history: pd.DataFrame,
                            field_col: str, field_num: int) -> TrendMetrics:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–¥—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—è"""

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ç–∏—Ä–∞–∂–∏
    recent_draws = df_history.head(self.micro_trend_window)
    macro_draws = df_history.head(self.macro_trend_window)

    # –ê–Ω–∞–ª–∏–∑ –≥–æ—Ä—è—á–∏—Ö —á–∏—Å–µ–ª —Å —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º
    hot_acceleration = self._find_accelerating_numbers(
      recent_draws, macro_draws, field_col, field_num
    )

    # –ê–Ω–∞–ª–∏–∑ —Ö–æ–ª–æ–¥–Ω—ã—Ö —á–∏—Å–µ–ª –≥–æ—Ç–æ–≤—ã—Ö –∫ —Ä–∞–∑–≤–æ—Ä–æ—Ç—É
    cold_reversal = self._find_reversal_candidates(
      recent_draws, macro_draws, field_col, field_num
    )

    # –ß–∏—Å–ª–∞ —Å –∏–º–ø—É–ª—å—Å–æ–º
    momentum_numbers = self._calculate_momentum_numbers(
      recent_draws, field_col, field_num
    )

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–¥–≤–∏–≥–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
    pattern_shift = self._detect_pattern_shift(
      recent_draws, macro_draws, field_col
    )

    # –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–∏–ª—ã —Ç—Ä–µ–Ω–¥–∞
    confidence_score = self._calculate_confidence(
      recent_draws, macro_draws, field_col
    )

    trend_strength = self._calculate_trend_strength(
      hot_acceleration, cold_reversal, momentum_numbers
    )

    return TrendMetrics(
      hot_acceleration=hot_acceleration[:5],  # –¢–æ–ø-5
      cold_reversal=cold_reversal[:3],  # –¢–æ–ø-3
      momentum_numbers=momentum_numbers[:4],  # –¢–æ–ø-4
      pattern_shift=pattern_shift,
      confidence_score=confidence_score,
      trend_strength=trend_strength
    )

  def _find_accelerating_numbers(self, recent_draws: pd.DataFrame,
                                 macro_draws: pd.DataFrame, field_col: str,
                                 field_num: int) -> List[int]:
    """–ù–∞—Ö–æ–¥–∏—Ç —á–∏—Å–ª–∞ —Å –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–π —á–∞—Å—Ç–æ—Ç–æ–π (—É—Å–∫–æ—Ä–µ–Ω–∏–µ)"""

    # –ß–∞—Å—Ç–æ—Ç–∞ –≤ –Ω–µ–¥–∞–≤–Ω–∏—Ö —Ç–∏—Ä–∞–∂–∞—Ö
    recent_freq = self._calculate_frequencies(recent_draws, field_col)

    # –ß–∞—Å—Ç–æ—Ç–∞ –≤ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω–æ–º –ø–µ—Ä–∏–æ–¥–µ
    macro_freq = self._calculate_frequencies(macro_draws, field_col)

    # –ù–∞—Ö–æ–¥–∏–º —á–∏—Å–ª–∞ —Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º
    accelerating = []
    for num in range(1, self._get_field_max(field_num) + 1):
      recent_rate = recent_freq.get(num, 0) / len(recent_draws) if len(recent_draws) > 0 else 0
      macro_rate = macro_freq.get(num, 0) / len(macro_draws) if len(macro_draws) > 0 else 0

      # –£—Å–∫–æ—Ä–µ–Ω–∏–µ = –Ω–µ–¥–∞–≤–Ω—è—è —á–∞—Å—Ç–æ—Ç–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤—ã—à–µ –æ–±—â–µ–π
      if recent_rate > macro_rate * 1.5 and recent_rate > 0.2:
        acceleration_factor = recent_rate / (macro_rate + 0.01)
        accelerating.append((num, acceleration_factor))

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∞–∫—Ç–æ—Ä—É —É—Å–∫–æ—Ä–µ–Ω–∏—è
    accelerating.sort(key=lambda x: x[1], reverse=True)
    return [num for num, _ in accelerating]

  def _find_reversal_candidates(self, recent_draws: pd.DataFrame,
                                macro_draws: pd.DataFrame, field_col: str,
                                field_num: int) -> List[int]:
    """–ù–∞—Ö–æ–¥–∏—Ç —Ö–æ–ª–æ–¥–Ω—ã–µ —á–∏—Å–ª–∞ –≥–æ—Ç–æ–≤—ã–µ –∫ —Ä–∞–∑–≤–æ—Ä–æ—Ç—É"""

    # –ê–Ω–∞–ª–∏–∑ —Ü–∏–∫–ª–æ–≤ –≤—ã–ø–∞–¥–µ–Ω–∏—è
    reversal_candidates = []
    field_max = self._get_field_max(field_num)

    for num in range(1, field_max + 1):
      # –°–∫–æ–ª—å–∫–æ —Ç–∏—Ä–∞–∂–µ–π –Ω–∞–∑–∞–¥ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ –≤—ã–ø–∞–¥–∞–ª–æ
      last_appearance = self._find_last_appearance(recent_draws, field_col, num)

      # –°—Ä–µ–¥–Ω–∏–π —Ü–∏–∫–ª –≤ –º–∞–∫—Ä–æ-–ø–µ—Ä–∏–æ–¥–µ
      avg_cycle = self._calculate_average_cycle(macro_draws, field_col, num)

      # –ï—Å–ª–∏ —á–∏—Å–ª–æ "–ø—Ä–æ—Å—Ä–æ—á–µ–Ω–æ" –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–≤–æ–µ–≥–æ —Ü–∏–∫–ª–∞
      if last_appearance > avg_cycle * 1.3 and avg_cycle > 0:
        overdue_factor = last_appearance / avg_cycle
        reversal_candidates.append((num, overdue_factor))

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∞–∫—Ç–æ—Ä—É "–ø—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω–æ—Å—Ç–∏"
    reversal_candidates.sort(key=lambda x: x[1], reverse=True)
    return [num for num, _ in reversal_candidates]

  def _calculate_momentum_numbers(self, recent_draws: pd.DataFrame,
                                  field_col: str, field_num: int) -> List[int]:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —á–∏—Å–ª–∞ —Å —Ç–µ–∫—É—â–∏–º –∏–º–ø—É–ª—å—Å–æ–º"""

    momentum_scores = {}
    field_max = self._get_field_max(field_num)

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Ç–∏—Ä–∞–∂–µ–π —Å –≤–µ—Å–∞–º–∏
    weights = [0.4, 0.3, 0.2, 0.08, 0.02]  # –ë–æ–ª—å—à–∏–π –≤–µ—Å –ø–æ—Å–ª–µ–¥–Ω–∏–º —Ç–∏—Ä–∞–∂–∞–º

    for i, (_, row) in enumerate(recent_draws.head(5).iterrows()):
      numbers = row.get(field_col, [])
      if not isinstance(numbers, list):
        continue

      weight = weights[i] if i < len(weights) else 0.01

      for num in numbers:
        if 1 <= num <= field_max:
          momentum_scores[num] = momentum_scores.get(num, 0) + weight

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–º–ø—É–ª—å—Å—É
    sorted_momentum = sorted(momentum_scores.items(), key=lambda x: x[1], reverse=True)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞ —Å –∑–Ω–∞—á–∏–º—ã–º –∏–º–ø—É–ª—å—Å–æ–º
    return [num for num, score in sorted_momentum if score >= self.momentum_threshold]

  def _detect_pattern_shift(self, recent_draws: pd.DataFrame,
                            macro_draws: pd.DataFrame, field_col: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–¥–≤–∏–≥ –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""

    if len(recent_draws) < 3 or len(macro_draws) < 10:
      return 'stable'

    # –ê–Ω–∞–ª–∏–∑ —Å—É–º–º –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ç–∏—Ä–∞–∂–µ–π
    recent_sums = self._calculate_sums(recent_draws, field_col)
    macro_sums = self._calculate_sums(macro_draws, field_col)

    recent_avg = np.mean(recent_sums) if recent_sums else 0
    macro_avg = np.mean(macro_sums) if macro_sums else 0

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–¥–≤–∏–≥–∞
    if recent_avg > macro_avg * 1.1:
      return 'ascending'
    elif recent_avg < macro_avg * 0.9:
      return 'descending'
    else:
      return 'stable'

  def _calculate_confidence(self, recent_draws: pd.DataFrame,
                            macro_draws: pd.DataFrame, field_col: str) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∞–Ω–∞–ª–∏–∑–µ"""

    # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    data_confidence = min(len(recent_draws) / self.micro_trend_window, 1.0)

    # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω–∞
    pattern_variance = self._calculate_pattern_variance(recent_draws, field_col)
    stability_confidence = max(0, 1 - pattern_variance)

    # –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    confidence = (data_confidence * 0.6 + stability_confidence * 0.4)

    return min(max(confidence, 0.1), 0.95)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–µ–∂–¥—É 0.1 –∏ 0.95

  def _calculate_trend_strength(self, hot_acceleration: List[int],
                                cold_reversal: List[int],
                                momentum_numbers: List[int]) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∏–ª—É —Ç—Ä–µ–Ω–¥–∞"""

    # –°–∏–ª–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∑–Ω–∞—á–∏–º—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    indicators = len(hot_acceleration) + len(cold_reversal) + len(momentum_numbers)
    max_possible = 12  # 5 + 3 + 4

    strength = min(indicators / max_possible, 1.0)

    # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    if len(hot_acceleration) > 2 and len(momentum_numbers) > 2:
      strength *= 1.2

    return min(strength, 1.0)

  def evaluate_combination(self, field1: List[int], field2: List[int],
                           trends: Dict[str, TrendMetrics]) -> CombinationMetrics:
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–∏—Ö —Ç—Ä–µ–Ω–¥–æ–≤

    Returns:
        CombinationMetrics —Å –æ—Ü–µ–Ω–∫–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
    """

    if not trends:
      return self._get_default_combination_metrics()

    # –û—Ü–µ–Ω–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è
    field1_metrics = self._evaluate_field_combination(
      field1, trends.get('field1'), 1
    )
    field2_metrics = self._evaluate_field_combination(
      field2, trends.get('field2'), 2
    )

    # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    trend_alignment = (field1_metrics['alignment'] + field2_metrics['alignment']) / 2
    momentum_score = (field1_metrics['momentum'] + field2_metrics['momentum']) / 2
    pattern_resonance = (field1_metrics['resonance'] + field2_metrics['resonance']) / 2

    # –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–∞
    risk_assessment = self._assess_combination_risk(
      trend_alignment, momentum_score, pattern_resonance
    )

    # –û–∂–∏–¥–∞–µ–º–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    expected_performance = self._calculate_expected_performance(
      trend_alignment, momentum_score, pattern_resonance, trends
    )

    return CombinationMetrics(
      trend_alignment=trend_alignment,
      momentum_score=momentum_score,
      pattern_resonance=pattern_resonance,
      risk_assessment=risk_assessment,
      expected_performance=expected_performance
    )

  def _evaluate_field_combination(self, numbers: List[int],
                                  trend_metrics: Optional[TrendMetrics],
                                  field_num: int) -> Dict[str, float]:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —á–∏—Å–ª–∞ –ø–æ–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç—Ä–µ–Ω–¥–æ–≤"""

    if not trend_metrics or not numbers:
      return {'alignment': 0.5, 'momentum': 0.5, 'resonance': 0.5}

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–Ω–¥–∞–º
    hot_matches = len(set(numbers) & set(trend_metrics.hot_acceleration))
    cold_matches = len(set(numbers) & set(trend_metrics.cold_reversal))
    momentum_matches = len(set(numbers) & set(trend_metrics.momentum_numbers))

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–æ–∫
    total_numbers = len(numbers)
    alignment = (hot_matches + cold_matches) / total_numbers if total_numbers > 0 else 0
    momentum = momentum_matches / total_numbers if total_numbers > 0 else 0

    # –†–µ–∑–æ–Ω–∞–Ω—Å —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º
    resonance = self._calculate_pattern_resonance(numbers, trend_metrics)

    return {
      'alignment': min(alignment, 1.0),
      'momentum': min(momentum, 1.0),
      'resonance': min(resonance, 1.0)
    }

  def _calculate_pattern_resonance(self, numbers: List[int],
                                   trend_metrics: TrendMetrics) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑–æ–Ω–∞–Ω—Å —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º"""

    # –ë–∞–∑–æ–≤—ã–π —Ä–µ–∑–æ–Ω–∞–Ω—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏–ª—ã —Ç—Ä–µ–Ω–¥–∞
    base_resonance = trend_metrics.trend_strength

    # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ø–∞—Ç—Ç–µ—Ä–Ω–∞
    pattern_bonus = 0
    if trend_metrics.pattern_shift == 'ascending':
      # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —á–∏—Å–ª–∞ –∏–∑ –≤–µ—Ä—Ö–Ω–µ–π –ø–æ–ª–æ–≤–∏–Ω—ã –¥–∏–∞–ø–∞–∑–æ–Ω–∞
      high_numbers = [n for n in numbers if n > 10]  # –î–ª—è 1-20
      pattern_bonus = len(high_numbers) / len(numbers) * 0.3
    elif trend_metrics.pattern_shift == 'descending':
      # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —á–∏—Å–ª–∞ –∏–∑ –Ω–∏–∂–Ω–µ–π –ø–æ–ª–æ–≤–∏–Ω—ã
      low_numbers = [n for n in numbers if n <= 10]
      pattern_bonus = len(low_numbers) / len(numbers) * 0.3

    return min(base_resonance + pattern_bonus, 1.0)

  def _assess_combination_risk(self, alignment: float, momentum: float,
                               resonance: float) -> str:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∏—Å–∫ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏"""

    overall_score = (alignment + momentum + resonance) / 3

    if overall_score >= 0.7:
      return 'low'
    elif overall_score >= 0.4:
      return 'medium'
    else:
      return 'high'

  def _calculate_expected_performance(self, alignment: float, momentum: float,
                                      resonance: float, trends: Dict) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–∂–∏–¥–∞–µ–º—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"""

    # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    base_performance = (alignment + momentum + resonance) / 3

    # –ë–æ–Ω—É—Å –∑–∞ –æ–±—â—É—é —Å–∏–ª—É —Ç—Ä–µ–Ω–¥–æ–≤
    avg_trend_strength = np.mean([
      trends[key].trend_strength for key in trends.keys()
      if hasattr(trends[key], 'trend_strength')
    ])

    # –ë–æ–Ω—É—Å –∑–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    avg_confidence = np.mean([
      trends[key].confidence_score for key in trends.keys()
      if hasattr(trends[key], 'confidence_score')
    ])

    # –ò—Ç–æ–≥–æ–≤–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    performance = base_performance * (1 + avg_trend_strength * 0.2 + avg_confidence * 0.1)

    return min(performance, 1.0)

  # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
  def _calculate_frequencies(self, df: pd.DataFrame, field_col: str) -> Dict[int, int]:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —á–∏—Å–µ–ª"""
    freq = Counter()
    for _, row in df.iterrows():
      numbers = row.get(field_col, [])
      if isinstance(numbers, list):
        freq.update(numbers)
    return dict(freq)

  def _calculate_sums(self, df: pd.DataFrame, field_col: str) -> List[int]:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—É–º–º—ã –∫–æ–º–±–∏–Ω–∞—Ü–∏–π"""
    sums = []
    for _, row in df.iterrows():
      numbers = row.get(field_col, [])
      if isinstance(numbers, list) and numbers:
        sums.append(sum(numbers))
    return sums

  def _find_last_appearance(self, df: pd.DataFrame, field_col: str, number: int) -> int:
    """–ù–∞—Ö–æ–¥–∏—Ç, —Å–∫–æ–ª—å–∫–æ —Ç–∏—Ä–∞–∂–µ–π –Ω–∞–∑–∞–¥ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ –≤—ã–ø–∞–¥–∞–ª–æ —á–∏—Å–ª–æ"""
    for i, (_, row) in enumerate(df.iterrows()):
      numbers = row.get(field_col, [])
      if isinstance(numbers, list) and number in numbers:
        return i
    return len(df)  # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ

  def _calculate_average_cycle(self, df: pd.DataFrame, field_col: str, number: int) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–∏–π —Ü–∏–∫–ª –≤—ã–ø–∞–¥–µ–Ω–∏—è —á–∏—Å–ª–∞"""
    appearances = []
    for i, (_, row) in enumerate(df.iterrows()):
      numbers = row.get(field_col, [])
      if isinstance(numbers, list) and number in numbers:
        appearances.append(i)

    if len(appearances) < 2:
      return 10.0  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    cycles = []
    for i in range(1, len(appearances)):
      cycles.append(appearances[i] - appearances[i - 1])

    return np.mean(cycles) if cycles else 10.0

  def _calculate_pattern_variance(self, df: pd.DataFrame, field_col: str) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
    sums = self._calculate_sums(df, field_col)
    return np.var(sums) / np.mean(sums) if sums and np.mean(sums) > 0 else 1.0

  def _get_field_max(self, field_num: int) -> int:
    """–ü–æ–ª—É—á–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –¥–ª—è –ø–æ–ª—è"""
    from backend.app.core.data_manager import get_current_config
    config = get_current_config()
    return config.get(f'field{field_num}_max', 20)

  def _calculate_data_signature(self, df: pd.DataFrame) -> str:
    """–°–æ–∑–¥–∞–µ—Ç –ø–æ–¥–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    if df.empty:
      return "empty"
    latest_draw = df.iloc[0].get('–¢–∏—Ä–∞–∂', 0)
    data_size = len(df)
    return f"{latest_draw}_{data_size}"

  def _get_default_trends(self) -> Dict[str, TrendMetrics]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã"""
    default_trend = TrendMetrics(
      hot_acceleration=[],
      cold_reversal=[],
      momentum_numbers=[],
      pattern_shift='stable',
      confidence_score=0.3,
      trend_strength=0.1
    )
    return {'field1': default_trend, 'field2': default_trend}

  def _get_default_combination_metrics(self) -> CombinationMetrics:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏"""
    return CombinationMetrics(
      trend_alignment=0.5,
      momentum_score=0.5,
      pattern_resonance=0.5,
      risk_assessment='medium',
      expected_performance=0.5
    )

  def get_trend_summary(self, trends: Dict[str, TrendMetrics]) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤"""
    if not trends:
      return "–¢—Ä–µ–Ω–¥—ã –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã"

    summary_parts = []

    for field_name, trend in trends.items():
      field_num = field_name[-1]
      parts = [f"–ü–æ–ª–µ {field_num}:"]

      if trend.hot_acceleration:
        parts.append(f"üî• –ì–æ—Ä—è—á–∏–µ: {trend.hot_acceleration[:3]}")

      if trend.cold_reversal:
        parts.append(f"‚ùÑÔ∏è –ì–æ—Ç–æ–≤—ã: {trend.cold_reversal[:2]}")

      if trend.momentum_numbers:
        parts.append(f"‚ö° –ò–º–ø—É–ª—å—Å: {trend.momentum_numbers[:2]}")

      parts.append(f"üìä –°–∏–ª–∞: {trend.trend_strength:.1f}")
      parts.append(f"üéØ –ü–∞—Ç—Ç–µ—Ä–Ω: {trend.pattern_shift}")

      summary_parts.append(" ".join(parts))

    return " | ".join(summary_parts)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç—Ä–µ–Ω–¥–æ–≤
GLOBAL_TREND_ANALYZER = DynamicTrendAnalyzer()


def analyze_combination_with_trends(field1: List[int], field2: List[int],
                                    df_history: pd.DataFrame) -> Tuple[float, str]:
  """
  –ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º —Ç—Ä–µ–Ω–¥–æ–≤

  Returns:
      Tuple[float, str]: (–æ—Ü–µ–Ω–∫–∞_—Ç—Ä–µ–Ω–¥–∞, –æ–ø–∏—Å–∞–Ω–∏–µ)
  """
  try:
    trends = GLOBAL_TREND_ANALYZER.analyze_current_trends(df_history)
    metrics = GLOBAL_TREND_ANALYZER.evaluate_combination(field1, field2, trends)

    description = f"–¢—Ä–µ–Ω–¥: {metrics.expected_performance:.2f}, –†–∏—Å–∫: {metrics.risk_assessment}"

    return metrics.expected_performance, description

  except Exception as e:
    return 0.5, f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)[:50]}"