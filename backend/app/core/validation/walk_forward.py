"""
Walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö
–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ lookahead bias
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Callable
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, mean_squared_error, mean_absolute_error
)
import json

from backend.app.core import data_manager

logger = logging.getLogger(__name__)


@dataclass
class ValidationWindow:
    """–û–∫–Ω–æ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    train_start: int
    train_end: int
    test_start: int
    test_end: int
    window_id: int
    
    @property
    def train_size(self) -> int:
        return self.train_end - self.train_start + 1
    
    @property
    def test_size(self) -> int:
        return self.test_end - self.test_start + 1


@dataclass
class ValidationMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –æ–∫–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    window_id: int
    accuracy: float
    precision: float
    recall: float
    f1: float
    roc_auc: Optional[float]
    mse: float
    mae: float
    matches_distribution: Dict[int, int]  # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    custom_metrics: Dict[str, float]
    
    def to_dict(self) -> Dict:
        return {
            'window_id': self.window_id,
            'accuracy': self.accuracy,
            'precision': self.precision,
            'recall': self.recall,
            'f1': self.f1,
            'roc_auc': self.roc_auc,
            'mse': self.mse,
            'mae': self.mae,
            'matches_distribution': self.matches_distribution,
            'custom_metrics': self.custom_metrics
        }


@dataclass
class ValidationResults:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç—ã walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    model_name: str
    lottery_type: str
    total_windows: int
    window_metrics: List[ValidationMetrics]
    average_metrics: Dict[str, float]
    std_metrics: Dict[str, float]
    best_window: int
    worst_window: int
    training_times: List[float]
    prediction_times: List[float]
    total_time: float
    parameters: Dict[str, Any]
    
    def get_summary(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        return {
            'model': self.model_name,
            'lottery': self.lottery_type,
            'windows': self.total_windows,
            'avg_accuracy': self.average_metrics.get('accuracy', 0),
            'avg_f1': self.average_metrics.get('f1', 0),
            'avg_roc_auc': self.average_metrics.get('roc_auc', 0),
            'std_accuracy': self.std_metrics.get('accuracy', 0),
            'best_window_id': self.best_window,
            'worst_window_id': self.worst_window,
            'total_time_seconds': self.total_time
        }
    
    def to_json(self) -> str:
        """–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤ JSON"""
        data = {
            'model_name': self.model_name,
            'lottery_type': self.lottery_type,
            'total_windows': self.total_windows,
            'window_metrics': [m.to_dict() for m in self.window_metrics],
            'average_metrics': self.average_metrics,
            'std_metrics': self.std_metrics,
            'best_window': self.best_window,
            'worst_window': self.worst_window,
            'training_times': self.training_times,
            'prediction_times': self.prediction_times,
            'total_time': self.total_time,
            'parameters': self.parameters,
            'summary': self.get_summary()
        }
        return json.dumps(data, indent=2)


class WalkForwardValidator:
    """
    Walk-forward –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –ª–æ—Ç–µ—Ä–µ–π
    –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç lookahead bias –∏ –¥–∞—ë—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –æ—Ü–µ–Ω–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self, 
                 initial_train_size: int = 500,
                 test_size: int = 50,
                 step_size: int = 50,
                 expanding_window: bool = True):
        """
        Args:
            initial_train_size: –ù–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –æ–∫–Ω–∞
            test_size: –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –æ–∫–Ω–∞
            step_size: –®–∞–≥ —Å–¥–≤–∏–≥–∞ –æ–∫–Ω–∞
            expanding_window: True = —Ä–∞—Å—à–∏—Ä—è—é—â–µ–µ—Å—è –æ–∫–Ω–æ, False = —Å–∫–æ–ª—å–∑—è—â–µ–µ
        """
        self.initial_train_size = initial_train_size
        self.test_size = test_size
        self.step_size = step_size
        self.expanding_window = expanding_window
        
        logger.info(f"‚úÖ Walk-forward –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: "
                   f"train={initial_train_size}, test={test_size}, "
                   f"step={step_size}, expanding={expanding_window}")
    
    def create_windows(self, data_size: int) -> List[ValidationWindow]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        
        Args:
            data_size: –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–∫–æ–Ω –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        windows = []
        window_id = 0
        
        # –ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è
        train_start = 0
        train_end = self.initial_train_size - 1
        
        while train_end + self.test_size < data_size:
            test_start = train_end + 1
            test_end = min(test_start + self.test_size - 1, data_size - 1)
            
            windows.append(ValidationWindow(
                train_start=train_start,
                train_end=train_end,
                test_start=test_start,
                test_end=test_end,
                window_id=window_id
            ))
            
            # –°–¥–≤–∏–≥–∞–µ–º –æ–∫–Ω–æ
            if self.expanding_window:
                # –†–∞—Å—à–∏—Ä—è—é—â–µ–µ—Å—è –æ–∫–Ω–æ - –Ω–∞—á–∞–ª–æ –æ—Å—Ç–∞–µ—Ç—Å—è, –∫–æ–Ω–µ—Ü —Å–¥–≤–∏–≥–∞–µ—Ç—Å—è
                train_end += self.step_size
            else:
                # –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ - —Å–¥–≤–∏–≥–∞—é—Ç—Å—è –æ–±–∞ –∫–æ–Ω—Ü–∞
                train_start += self.step_size
                train_end += self.step_size
            
            window_id += 1
        
        logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(windows)} –æ–∫–æ–Ω –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
        return windows
    
    def validate_model(self,
                       model_class: Any,
                       model_params: Dict,
                       df_history: pd.DataFrame,
                       lottery_config: Dict,
                       custom_evaluator: Optional[Callable] = None) -> ValidationResults:
        """
        –ü–æ–ª–Ω–∞—è walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        
        Args:
            model_class: –ö–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ (XGBoostLotteryModel, LotteryRFOps –∏ —Ç.–¥.)
            model_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
            df_history: –ò—Å—Ç–æ—Ä–∏—è —Ç–∏—Ä–∞–∂–µ–π
            lottery_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ—Ç–µ—Ä–µ–∏
            custom_evaluator: –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        import time
        start_time = time.time()
        
        if df_history.empty or len(df_history) < self.initial_train_size + self.test_size:
            logger.error(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {len(df_history)} —Ç–∏—Ä–∞–∂–µ–π")
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
        
        # –°–æ–∑–¥–∞–µ–º –æ–∫–Ω–∞
        windows = self.create_windows(len(df_history))
        
        if not windows:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –æ–∫–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
        
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ {model_class.__name__}")
        
        window_metrics = []
        training_times = []
        prediction_times = []
        
        for window in windows:
            logger.info(f"üìç –û–∫–Ω–æ {window.window_id}: "
                       f"train[{window.train_start}:{window.train_end}], "
                       f"test[{window.test_start}:{window.test_end}]")
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            train_data = df_history.iloc[window.train_start:window.train_end + 1].copy()
            test_data = df_history.iloc[window.test_start:window.test_end + 1].copy()

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞
            model = model_class(lottery_config, **model_params)
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            train_start = time.time()
            try:
                success = model.train(train_data)
                if not success:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –æ–∫–Ω–∞ {window.window_id}")
                    continue
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–∫–Ω–∞ {window.window_id}: {e}")
                continue
            
            training_time = time.time() - train_start
            training_times.append(training_time)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
            pred_start = time.time()
            metrics = self._evaluate_window(
                model, train_data, test_data, 
                window, lottery_config, custom_evaluator
            )
            prediction_time = time.time() - pred_start
            prediction_times.append(prediction_time)
            
            window_metrics.append(metrics)
            
            logger.info(f"‚úÖ –û–∫–Ω–æ {window.window_id}: "
                       f"accuracy={metrics.accuracy:.3f}, "
                       f"f1={metrics.f1:.3f}")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        avg_metrics, std_metrics = self._calculate_aggregate_metrics(window_metrics)
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–µ–µ –∏ —Ö—É–¥—à–µ–µ –æ–∫–Ω–æ
        best_window = max(window_metrics, key=lambda m: m.f1).window_id
        worst_window = min(window_metrics, key=lambda m: m.f1).window_id
        
        total_time = time.time() - start_time
        
        results = ValidationResults(
            model_name=model_class.__name__,
            lottery_type=lottery_config.get('name', 'unknown'),
            total_windows=len(windows),
            window_metrics=window_metrics,
            average_metrics=avg_metrics,
            std_metrics=std_metrics,
            best_window=best_window,
            worst_window=worst_window,
            training_times=training_times,
            prediction_times=prediction_times,
            total_time=total_time,
            parameters={
                'initial_train_size': self.initial_train_size,
                'test_size': self.test_size,
                'step_size': self.step_size,
                'expanding_window': self.expanding_window,
                'model_params': model_params
            }
        )
        
        logger.info(f"üéØ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.2f}—Å")
        logger.info(f"üìä –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏: accuracy={avg_metrics['accuracy']:.3f}, "
                   f"f1={avg_metrics['f1']:.3f}, "
                   f"roc_auc={avg_metrics.get('roc_auc', 0):.3f}")
        
        return results
    
    def _evaluate_window(self,
                        model: Any,
                        train_data: pd.DataFrame,
                        test_data: pd.DataFrame,
                        window: ValidationWindow,
                        lottery_config: Dict,
                        custom_evaluator: Optional[Callable]) -> ValidationMetrics:
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–¥–Ω–æ–º –æ–∫–Ω–µ
        """
        predictions = []
        actuals = []
        matches_distribution = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}

        df_history = data_manager.fetch_draws_from_db()

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏—Ä–∞–∂–∞ –≤ —Ç–µ—Å—Ç–µ
        for idx, test_row in test_data.iterrows():
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç–∏—Ä–∞–∂ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if idx > 0:
                prev_idx = idx - 1
                prev_row = df_history.iloc[prev_idx] if prev_idx >= 0 else train_data.iloc[-1]
                
                prev_f1 = prev_row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', [])
                prev_f2 = prev_row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', [])
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
                try:
                    if hasattr(model, 'predict_next_combination'):
                        pred_f1, pred_f2 = model.predict_next_combination(
                            prev_f1, prev_f2, train_data
                        )
                    else:
                        # Fallback –¥–ª—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞
                        pred_f1 = list(range(1, lottery_config['field1_size'] + 1))
                        pred_f2 = list(range(1, lottery_config['field2_size'] + 1))
                    
                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏
                    actual_f1 = test_row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ1_list', [])
                    actual_f2 = test_row.get('–ß–∏—Å–ª–∞_–ü–æ–ª–µ2_list', [])
                    
                    # –°—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    matches_f1 = len(set(pred_f1) & set(actual_f1))
                    matches_f2 = len(set(pred_f2) & set(actual_f2))
                    total_matches = matches_f1 + matches_f2
                    
                    matches_distribution[total_matches] = matches_distribution.get(total_matches, 0) + 1
                    
                    # –ë–∏–Ω–∞—Ä–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∏—Å–ª–∞
                    for num in range(1, lottery_config['field1_max'] + 1):
                        predictions.append(1 if num in pred_f1 else 0)
                        actuals.append(1 if num in actual_f1 else 0)
                    
                    for num in range(1, lottery_config['field2_max'] + 1):
                        predictions.append(1 if num in pred_f2 else 0)
                        actuals.append(1 if num in actual_f2 else 0)
                        
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
                    continue
        
        if not predictions or not actuals:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            return ValidationMetrics(
                window_id=window.window_id,
                accuracy=0, precision=0, recall=0, f1=0, roc_auc=None,
                mse=1.0, mae=1.0,
                matches_distribution=matches_distribution,
                custom_metrics={}
            )
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        predictions = np.array(predictions)
        actuals = np.array(actuals)
        
        accuracy = accuracy_score(actuals, predictions)
        precision = precision_score(actuals, predictions, average='micro', zero_division=0)
        recall = recall_score(actuals, predictions, average='micro', zero_division=0)
        f1 = f1_score(actuals, predictions, average='micro', zero_division=0)
        
        # ROC-AUC –µ—Å–ª–∏ –µ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        roc_auc = None
        if hasattr(model, 'score_combination'):
            # –ú–æ–∂–µ–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–ª—É—á–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            try:
                # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - –∏—Å–ø–æ–ª—å–∑—É–µ–º score –∫–∞–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                roc_auc = 0.5  # Placeholder, —Ç–∞–∫ –∫–∞–∫ –Ω—É–∂–Ω—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            except:
                pass
        
        mse = mean_squared_error(actuals, predictions)
        mae = mean_absolute_error(actuals, predictions)
        
        # –ö–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        custom_metrics = {}
        if custom_evaluator:
            try:
                custom_metrics = custom_evaluator(model, train_data, test_data)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ evaluator: {e}")
        
        return ValidationMetrics(
            window_id=window.window_id,
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1=f1,
            roc_auc=roc_auc,
            mse=mse,
            mae=mae,
            matches_distribution=matches_distribution,
            custom_metrics=custom_metrics
        )
    
    def _calculate_aggregate_metrics(self, 
                                    window_metrics: List[ValidationMetrics]) -> Tuple[Dict, Dict]:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏—Ö –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –º–µ—Ç—Ä–∏–∫
        """
        if not window_metrics:
            return {}, {}
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –º–∞—Å—Å–∏–≤—ã
        metrics_arrays = {
            'accuracy': [m.accuracy for m in window_metrics],
            'precision': [m.precision for m in window_metrics],
            'recall': [m.recall for m in window_metrics],
            'f1': [m.f1 for m in window_metrics],
            'mse': [m.mse for m in window_metrics],
            'mae': [m.mae for m in window_metrics]
        }
        
        # ROC-AUC –º–æ–∂–µ—Ç –±—ã—Ç—å None
        roc_aucs = [m.roc_auc for m in window_metrics if m.roc_auc is not None]
        if roc_aucs:
            metrics_arrays['roc_auc'] = roc_aucs
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ
        avg_metrics = {
            name: np.mean(values) for name, values in metrics_arrays.items()
        }
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        std_metrics = {
            name: np.std(values) for name, values in metrics_arrays.items()
        }
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        total_matches_dist = {}
        for m in window_metrics:
            for matches, count in m.matches_distribution.items():
                total_matches_dist[matches] = total_matches_dist.get(matches, 0) + count
        
        avg_metrics['avg_matches_distribution'] = total_matches_dist
        
        return avg_metrics, std_metrics
    
    def compare_models(self,
                       models: List[Tuple[Any, Dict]],
                       df_history: pd.DataFrame,
                       lottery_config: Dict) -> pd.DataFrame:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
        
        Args:
            models: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (model_class, model_params)
            df_history: –ò—Å—Ç–æ—Ä–∏—è —Ç–∏—Ä–∞–∂–µ–π
            lottery_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ—Ç–µ—Ä–µ–∏
            
        Returns:
            DataFrame —Å–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π
        """
        results = []
        
        for model_class, model_params in models:
            logger.info(f"üîÑ –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {model_class.__name__}")
            
            try:
                validation_results = self.validate_model(
                    model_class, model_params, 
                    df_history, lottery_config
                )
                
                summary = validation_results.get_summary()
                summary['status'] = 'success'
                results.append(summary)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ {model_class.__name__}: {e}")
                results.append({
                    'model': model_class.__name__,
                    'status': 'error',
                    'error': str(e)
                })
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_df = pd.DataFrame(results)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ F1 score
        if 'avg_f1' in comparison_df.columns:
            comparison_df = comparison_df.sort_values('avg_f1', ascending=False)
        
        return comparison_df
    
    def plot_validation_results(self, results: ValidationResults):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—Ç—Ä–µ–±—É–µ—Ç matplotlib)
        """
        try:
            import matplotlib.pyplot as plt
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –æ–∫–Ω–∞–º
            windows = [m.window_id for m in results.window_metrics]
            accuracies = [m.accuracy for m in results.window_metrics]
            f1_scores = [m.f1 for m in results.window_metrics]
            
            fig, axes = plt.subplots(2, 2, figsize=(12, 8))
            fig.suptitle(f'Walk-Forward Validation: {results.model_name}', fontsize=16)
            
            # –ì—Ä–∞—Ñ–∏–∫ accuracy
            axes[0, 0].plot(windows, accuracies, 'b-', marker='o')
            axes[0, 0].axhline(y=results.average_metrics['accuracy'], color='r', linestyle='--', label='Average')
            axes[0, 0].set_xlabel('Window ID')
            axes[0, 0].set_ylabel('Accuracy')
            axes[0, 0].set_title('Accuracy –ø–æ –æ–∫–Ω–∞–º')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # –ì—Ä–∞—Ñ–∏–∫ F1
            axes[0, 1].plot(windows, f1_scores, 'g-', marker='s')
            axes[0, 1].axhline(y=results.average_metrics['f1'], color='r', linestyle='--', label='Average')
            axes[0, 1].set_xlabel('Window ID')
            axes[0, 1].set_ylabel('F1 Score')
            axes[0, 1].set_title('F1 Score –ø–æ –æ–∫–Ω–∞–º')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            total_matches = {}
            for m in results.window_metrics:
                for matches, count in m.matches_distribution.items():
                    total_matches[matches] = total_matches.get(matches, 0) + count
            
            if total_matches:
                matches_nums = list(total_matches.keys())
                matches_counts = list(total_matches.values())
                axes[1, 0].bar(matches_nums, matches_counts, color='purple', alpha=0.7)
                axes[1, 0].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π')
                axes[1, 0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
                axes[1, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π')
                axes[1, 0].grid(True, alpha=0.3)
            
            # –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
            axes[1, 1].plot(range(len(results.training_times)), results.training_times, 'r-', marker='^')
            axes[1, 1].set_xlabel('Window ID')
            axes[1, 1].set_ylabel('–í—Ä–µ–º—è (—Å–µ–∫)')
            axes[1, 1].set_title('–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –ø–æ –æ–∫–Ω–∞–º')
            axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            return fig
            
        except ImportError:
            logger.warning("matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return None


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –≤–∞–ª–∏–¥–∞—Ç–æ—Ä —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
DEFAULT_VALIDATOR = WalkForwardValidator(
    initial_train_size=300,
    test_size=30,
    step_size=30,
    expanding_window=True
)
